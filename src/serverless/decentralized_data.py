"""
Decentralized Data Nervous System - AUDITORIA360
Sistema de dados distribu√≠do usando DuckDB + R2 para consultas anal√≠ticas instant√¢neas.
"""

import os
import io
import time
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import duckdb
import pandas as pd
import boto3
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import json


class DecentralizedDataNervousSystem:
    """
    Sistema Nervoso de Dados Descentralizado - arquitetura revolucion√°ria onde os dados
    n√£o vivem em um local √∫nico, mas existem como ficheiros Parquet otimizados distribu√≠dos
    no R2, permitindo consultas SQL instant√¢neas sem lat√™ncia de base de dados tradicional.
    """
    
    def __init__(self, r2_config: Optional[Dict] = None):
        """Inicializa o sistema nervoso de dados descentralizado."""
        self.r2_config = r2_config or self._load_r2_config()
        self.s3_client = self._setup_r2_client()
        self.metadata_store = {}  # Em produ√ß√£o, usaria Cloudflare KV
        self._connection_pool = {}
        
    def _load_r2_config(self) -> Dict:
        """Carrega configura√ß√£o do R2 do ambiente."""
        return {
            'endpoint_url': os.getenv('R2_ENDPOINT_URL', 'https://r2.cloudflarestorage.com'),
            'aws_access_key_id': os.getenv('R2_ACCESS_KEY_ID', 'demo_key'),
            'aws_secret_access_key': os.getenv('R2_SECRET_ACCESS_KEY', 'demo_secret'),
            'bucket_name': os.getenv('R2_BUCKET_NAME', 'auditoria360-data'),
            'region_name': os.getenv('R2_REGION', 'auto')
        }
    
    def _setup_r2_client(self):
        """Configura cliente S3 para Cloudflare R2."""
        try:
            return boto3.client(
                's3',
                endpoint_url=self.r2_config['endpoint_url'],
                aws_access_key_id=self.r2_config['aws_access_key_id'],
                aws_secret_access_key=self.r2_config['aws_secret_access_key'],
                region_name=self.r2_config['region_name']
            )
        except Exception:
            # Fallback para demo - cria cliente mock
            return MockS3Client()
    
    def create_optimized_parquet_dataset(self, 
                                       dataset_name: str, 
                                       size_gb: float = 10.0) -> Dict[str, Any]:
        """
        Cria dataset Parquet otimizado de 10GB+ para testes de escala.
        
        Returns:
            Dict com informa√ß√µes do dataset criado
        """
        print(f"üöÄ Criando dataset Parquet otimizado: {dataset_name} ({size_gb}GB)")
        
        # Calcular n√∫mero de registros para atingir o tamanho desejado
        target_size_bytes = size_gb * 1024 * 1024 * 1024
        # Estimativa: ~1KB por registro ap√≥s compress√£o Parquet
        estimated_records = int(target_size_bytes / 1024)
        
        start_time = time.time()
        dataset_info = {
            'name': dataset_name,
            'created_at': datetime.now().isoformat(),
            'size_gb': size_gb,
            'estimated_records': estimated_records,
            'partitions': [],
            'version': 1
        }
        
        # Criar parti√ß√µes distribu√≠das para otimizar consultas paralelas
        partitions_count = max(10, int(size_gb))  # 1 parti√ß√£o por GB m√≠nimo
        records_per_partition = estimated_records // partitions_count
        
        for partition_id in range(partitions_count):
            partition_data = self._generate_partition_data(
                partition_id, records_per_partition, dataset_name
            )
            
            # Converter para Parquet otimizado
            parquet_buffer = self._optimize_parquet(partition_data)
            
            # Upload para R2 com metadados
            partition_key = f"datasets/{dataset_name}/partition_{partition_id:04d}.parquet"
            
            try:
                self.s3_client.put_object(
                    Bucket=self.r2_config['bucket_name'],
                    Key=partition_key,
                    Body=parquet_buffer,
                    Metadata={
                        'dataset': dataset_name,
                        'partition_id': str(partition_id),
                        'records_count': str(len(partition_data)),
                        'created_at': datetime.now().isoformat()
                    }
                )
                
                partition_info = {
                    'partition_id': partition_id,
                    'key': partition_key,
                    'records_count': len(partition_data),
                    'size_bytes': len(parquet_buffer)
                }
                dataset_info['partitions'].append(partition_info)
                
                print(f"‚úÖ Parti√ß√£o {partition_id + 1}/{partitions_count} criada")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Fallback para armazenamento local: {e}")
                # Salvar localmente para demo
                local_path = f"/tmp/{partition_key.replace('/', '_')}"
                Path(local_path).parent.mkdir(parents=True, exist_ok=True)
                with open(local_path, 'wb') as f:
                    f.write(parquet_buffer)
                    
                partition_info = {
                    'partition_id': partition_id,
                    'key': partition_key,
                    'local_path': local_path,
                    'records_count': len(partition_data),
                    'size_bytes': len(parquet_buffer)
                }
                dataset_info['partitions'].append(partition_info)
        
        # Salvar metadados do dataset
        self.metadata_store[dataset_name] = dataset_info
        
        creation_time = time.time() - start_time
        dataset_info['creation_time_seconds'] = creation_time
        
        print(f"üéØ Dataset {dataset_name} criado com sucesso!")
        print(f"üìä Estat√≠sticas:")
        print(f"   ‚Ä¢ Parti√ß√µes: {len(dataset_info['partitions'])}")
        print(f"   ‚Ä¢ Registros totais: {sum(p['records_count'] for p in dataset_info['partitions']):,}")
        print(f"   ‚Ä¢ Tamanho total: {sum(p['size_bytes'] for p in dataset_info['partitions']) / 1024 / 1024:.1f} MB")
        print(f"   ‚Ä¢ Tempo de cria√ß√£o: {creation_time:.2f}s")
        
        return dataset_info
    
    def _generate_partition_data(self, partition_id: int, records_count: int, dataset_name: str) -> pd.DataFrame:
        """Gera dados sint√©ticos para uma parti√ß√£o espec√≠fica."""
        import numpy as np
        
        # Gerar dados real√≠sticos de auditoria/folha de pagamento
        base_data = {
            'partition_id': [partition_id] * records_count,
            'employee_id': range(partition_id * records_count, (partition_id + 1) * records_count),
            'audit_date': pd.date_range(
                start='2020-01-01', 
                end='2024-12-31', 
                periods=records_count
            ),
            'department': np.random.choice(
                ['TI', 'RH', 'Financeiro', 'Vendas', 'Marketing', 'Opera√ß√µes'], 
                records_count
            ),
            'salary_base': np.random.normal(5000, 2000, records_count).round(2),
            'overtime_hours': np.random.exponential(5, records_count).round(1),
            'compliance_score': np.random.uniform(0.7, 1.0, records_count).round(3),
            'audit_status': np.random.choice(
                ['approved', 'pending', 'rejected', 'review'], 
                records_count,
                p=[0.7, 0.15, 0.05, 0.1]
            ),
            'risk_level': np.random.choice(
                ['low', 'medium', 'high', 'critical'], 
                records_count,
                p=[0.5, 0.3, 0.15, 0.05]
            ),
            # Adicionar campos calculados para consultas complexas
            'total_compensation': 0.0,
            'compliance_flag': '',
            'audit_category': '',
        }
        
        df = pd.DataFrame(base_data)
        
        # Calcular campos derivados
        df['total_compensation'] = (
            df['salary_base'] + 
            (df['overtime_hours'] * df['salary_base'] / 220 * 1.5)
        ).round(2)
        
        df['compliance_flag'] = df['compliance_score'].apply(
            lambda x: 'excellent' if x >= 0.95 else 'good' if x >= 0.85 else 'warning'
        )
        
        df['audit_category'] = df.apply(
            lambda row: f"{row['department']}_audit_{row['risk_level']}", axis=1
        )
        
        return df
    
    def _optimize_parquet(self, df: pd.DataFrame) -> bytes:
        """Otimiza DataFrame para formato Parquet com m√°xima compress√£o."""
        buffer = io.BytesIO()
        
        # Configura√ß√µes de otimiza√ß√£o para Parquet
        df.to_parquet(
            buffer,
            engine='pyarrow',
            compression='snappy',  # Bom equil√≠brio entre compress√£o e velocidade
            index=False,
            # Otimiza√ß√µes espec√≠ficas para analytics
            use_dictionary=True,  # Compress√£o de strings repetitivas
            write_statistics=True,  # Metadados para query optimization
        )
        
        return buffer.getvalue()
    
    async def execute_distributed_query(self, 
                                       query: str, 
                                       dataset_name: str,
                                       function_id: str = None) -> Dict[str, Any]:
        """
        Executa consulta distribu√≠da atrav√©s de fun√ß√£o serverless ef√™mera.
        
        Esta fun√ß√£o simula o "Fogo de Artif√≠cio" - ela brilha por milissegundos
        para executar a consulta e depois desaparece sem deixar rastro.
        """
        function_id = function_id or f"func_{int(time.time() * 1000)}"
        start_time = time.time()
        
        print(f"‚ö° Fun√ß√£o {function_id} iniciada - consultando {dataset_name}")
        
        try:
            # Simular "cold start" baseado no tamanho do dataset
            dataset_info = self.metadata_store.get(dataset_name)
            if dataset_info:
                cold_start_time = self._simulate_cold_start(dataset_info)
                await asyncio.sleep(cold_start_time)
            
            # Obter conex√£o DuckDB ef√™mera
            con = self._get_ephemeral_connection(function_id)
            
            # Registrar parti√ß√µes do dataset no DuckDB
            await self._register_distributed_dataset(con, dataset_name)
            
            # Executar consulta otimizada
            result = con.execute(query).fetchdf()
            
            execution_time = time.time() - start_time
            
            # "Fun√ß√£o desaparece" - limpeza imediata
            self._cleanup_ephemeral_connection(function_id)
            
            print(f"‚ú® Fun√ß√£o {function_id} executada em {execution_time*1000:.1f}ms - {len(result)} registros")
            
            return {
                'function_id': function_id,
                'execution_time_ms': execution_time * 1000,
                'records_returned': len(result),
                'result': result.to_dict('records') if len(result) <= 100 else 'truncated',
                'query': query,
                'status': 'completed',
                'cold_start_ms': cold_start_time * 1000 if dataset_info else 0
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            self._cleanup_ephemeral_connection(function_id)
            
            print(f"üí• Fun√ß√£o {function_id} falhou em {execution_time*1000:.1f}ms: {e}")
            
            return {
                'function_id': function_id,
                'execution_time_ms': execution_time * 1000,
                'status': 'failed',
                'error': str(e),
                'query': query
            }
    
    def _simulate_cold_start(self, dataset_info: Dict) -> float:
        """Simula tempo de cold start baseado no tamanho do dataset."""
        # Cold start aumenta com o n√∫mero de parti√ß√µes (mais metadados para carregar)
        partitions_count = len(dataset_info.get('partitions', []))
        base_cold_start = 0.05  # 50ms base
        size_penalty = partitions_count * 0.01  # 10ms por parti√ß√£o
        
        return min(base_cold_start + size_penalty, 0.3)  # M√°ximo 300ms
    
    def _get_ephemeral_connection(self, function_id: str) -> duckdb.DuckDBPyConnection:
        """Obt√©m conex√£o DuckDB ef√™mera para a fun√ß√£o."""
        con = duckdb.connect(':memory:')
        
        # Otimiza√ß√µes espec√≠ficas para analytics em larga escala
        con.execute("SET threads=4")
        con.execute("SET memory_limit='2GB'")
        con.execute("SET enable_progress_bar=false")
        
        self._connection_pool[function_id] = con
        return con
    
    async def _register_distributed_dataset(self, con: duckdb.DuckDBPyConnection, dataset_name: str):
        """Registra dataset distribu√≠do no DuckDB para consultas."""
        dataset_info = self.metadata_store.get(dataset_name)
        if not dataset_info:
            raise ValueError(f"Dataset {dataset_name} n√£o encontrado")
        
        # Criar views das parti√ß√µes para consulta unificada
        partition_views = []
        
        for partition in dataset_info['partitions']:
            partition_id = partition['partition_id']
            
            if 'local_path' in partition:
                # Modo demo - usar arquivos locais
                local_path = partition['local_path']
                if os.path.exists(local_path):
                    con.execute(f"""
                        CREATE VIEW partition_{partition_id} AS 
                        SELECT * FROM read_parquet('{local_path}')
                    """)
                    partition_views.append(f"partition_{partition_id}")
            else:
                # Modo produ√ß√£o - usar R2 URLs (simulado)
                print(f"‚ö†Ô∏è Parti√ß√£o R2 {partition['key']} seria carregada via httpfs em produ√ß√£o")
        
        # Criar view unificada do dataset
        if partition_views:
            union_query = " UNION ALL ".join([f"SELECT * FROM {view}" for view in partition_views])
            con.execute(f"CREATE VIEW {dataset_name} AS {union_query}")
    
    def _cleanup_ephemeral_connection(self, function_id: str):
        """Limpa conex√£o ef√™mera ap√≥s execu√ß√£o da fun√ß√£o."""
        if function_id in self._connection_pool:
            try:
                self._connection_pool[function_id].close()
                del self._connection_pool[function_id]
            except:
                pass
    
    async def massive_parallel_query_test(self, 
                                        dataset_name: str,
                                        concurrent_functions: int = 1000) -> Dict[str, Any]:
        """
        Teste de Consulta Descentralizada Massiva:
        Simula 1000 fun√ß√µes serverless executando consultas anal√≠ticas simultaneamente.
        """
        print(f"üöÄ TESTE MASSIVO: {concurrent_functions} fun√ß√µes simult√¢neas em {dataset_name}")
        
        # Consultas anal√≠ticas complexas variadas
        analytical_queries = [
            f"SELECT department, COUNT(*) as employees, AVG(salary_base) as avg_salary FROM {dataset_name} GROUP BY department",
            f"SELECT audit_status, risk_level, COUNT(*) as count FROM {dataset_name} GROUP BY audit_status, risk_level",
            f"SELECT DATE_PART('year', audit_date) as year, SUM(total_compensation) as total FROM {dataset_name} GROUP BY year ORDER BY year",
            f"SELECT compliance_flag, AVG(compliance_score) as avg_score FROM {dataset_name} GROUP BY compliance_flag",
            f"SELECT department, risk_level, MAX(total_compensation) as max_comp FROM {dataset_name} GROUP BY department, risk_level",
        ]
        
        # Criar tarefas ass√≠ncronas para execu√ß√£o paralela
        tasks = []
        start_time = time.time()
        
        for i in range(concurrent_functions):
            query = analytical_queries[i % len(analytical_queries)]
            function_id = f"massive_test_func_{i:04d}"
            
            task = self.execute_distributed_query(query, dataset_name, function_id)
            tasks.append(task)
        
        print(f"‚ö° Executando {len(tasks)} consultas em paralelo...")
        
        # Executar com controle de concorr√™ncia para n√£o sobrecarregar
        batch_size = 50  # Processar em lotes para estabilidade
        results = []
        
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            results.extend(batch_results)
            
            print(f"‚úÖ Lote {i//batch_size + 1}/{(len(tasks)-1)//batch_size + 1} completado")
        
        total_time = time.time() - start_time
        
        # Analisar resultados
        successful_queries = [r for r in results if isinstance(r, dict) and r.get('status') == 'completed']
        failed_queries = [r for r in results if isinstance(r, dict) and r.get('status') == 'failed']
        exceptions = [r for r in results if not isinstance(r, dict)]
        
        execution_times = [r['execution_time_ms'] for r in successful_queries]
        p99_time = sorted(execution_times)[int(len(execution_times) * 0.99)] if execution_times else 0
        
        test_results = {
            'test_type': 'massive_parallel_query',
            'concurrent_functions': concurrent_functions,
            'total_execution_time_seconds': total_time,
            'successful_queries': len(successful_queries),
            'failed_queries': len(failed_queries),
            'exceptions': len(exceptions),
            'performance_metrics': {
                'p99_execution_time_ms': p99_time,
                'avg_execution_time_ms': sum(execution_times) / len(execution_times) if execution_times else 0,
                'min_execution_time_ms': min(execution_times) if execution_times else 0,
                'max_execution_time_ms': max(execution_times) if execution_times else 0,
            },
            'success_rate_percentage': (len(successful_queries) / concurrent_functions) * 100,
            'target_p99_ms': 500,
            'test_passed': p99_time < 500
        }
        
        print(f"\nüìä RESULTADOS DO TESTE MASSIVO:")
        print(f"   ‚Ä¢ Fun√ß√µes executadas: {len(successful_queries)}/{concurrent_functions}")
        print(f"   ‚Ä¢ Taxa de sucesso: {test_results['success_rate_percentage']:.1f}%")
        print(f"   ‚Ä¢ Tempo P99: {p99_time:.1f}ms (meta: <500ms)")
        print(f"   ‚Ä¢ Tempo m√©dio: {test_results['performance_metrics']['avg_execution_time_ms']:.1f}ms")
        print(f"   ‚Ä¢ ‚úÖ TESTE APROVADO!" if test_results['test_passed'] else "   ‚Ä¢ ‚ùå TESTE REPROVADO!")
        
        return test_results
    
    def create_immutable_dataset_version(self, 
                                       dataset_name: str, 
                                       modifications: Dict[str, Any]) -> Dict[str, Any]:
        """
        Teste de Imutabilidade e Versionamento de Dados:
        Cria nova vers√£o do dataset sem modificar o original.
        """
        print(f"üîÑ Criando vers√£o imut√°vel de {dataset_name}")
        
        original_dataset = self.metadata_store.get(dataset_name)
        if not original_dataset:
            raise ValueError(f"Dataset original {dataset_name} n√£o encontrado")
        
        # Criar nova vers√£o com timestamp
        new_version = original_dataset['version'] + 1
        version_timestamp = datetime.now().isoformat()
        new_dataset_name = f"{dataset_name}_v{new_version}_{int(time.time())}"
        
        # Metadados da nova vers√£o
        new_dataset_info = {
            **original_dataset,
            'name': new_dataset_name,
            'version': new_version,
            'parent_dataset': dataset_name,
            'created_at': version_timestamp,
            'modifications': modifications,
            'immutable': True,
            'partitions': []
        }
        
        print(f"üìù Aplicando modifica√ß√µes e criando {new_dataset_name}")
        
        # Processar cada parti√ß√£o aplicando modifica√ß√µes
        for original_partition in original_dataset['partitions']:
            if 'local_path' in original_partition:
                # Carregar parti√ß√£o original
                original_df = pd.read_parquet(original_partition['local_path'])
                
                # Aplicar modifica√ß√µes (exemplo: filtros, agrega√ß√µes, transforma√ß√µes)
                modified_df = self._apply_modifications(original_df, modifications)
                
                # Gerar nova parti√ß√£o com dados modificados
                new_partition_buffer = self._optimize_parquet(modified_df)
                
                # Salvar nova parti√ß√£o
                new_partition_key = f"datasets/{new_dataset_name}/partition_{original_partition['partition_id']:04d}.parquet"
                new_local_path = f"/tmp/{new_partition_key.replace('/', '_')}"
                
                Path(new_local_path).parent.mkdir(parents=True, exist_ok=True)
                with open(new_local_path, 'wb') as f:
                    f.write(new_partition_buffer)
                
                new_partition_info = {
                    'partition_id': original_partition['partition_id'],
                    'key': new_partition_key,
                    'local_path': new_local_path,
                    'records_count': len(modified_df),
                    'size_bytes': len(new_partition_buffer),
                    'parent_partition': original_partition['key']
                }
                new_dataset_info['partitions'].append(new_partition_info)
        
        # Registrar nova vers√£o nos metadados
        self.metadata_store[new_dataset_name] = new_dataset_info
        
        # Criar ponteiro de metadados para versionamento
        version_pointer = {
            'dataset_base_name': dataset_name,
            'current_version': new_version,
            'current_dataset_name': new_dataset_name,
            'version_history': [
                {'version': 1, 'dataset_name': dataset_name, 'created_at': original_dataset['created_at']},
                {'version': new_version, 'dataset_name': new_dataset_name, 'created_at': version_timestamp}
            ],
            'last_updated': version_timestamp
        }
        
        self.metadata_store[f"{dataset_name}_version_pointer"] = version_pointer
        
        print(f"‚úÖ Nova vers√£o {new_dataset_name} criada com sucesso!")
        print(f"üìä Estat√≠sticas da vers√£o:")
        print(f"   ‚Ä¢ Parti√ß√µes: {len(new_dataset_info['partitions'])}")
        print(f"   ‚Ä¢ Registros totais: {sum(p['records_count'] for p in new_dataset_info['partitions']):,}")
        print(f"   ‚Ä¢ Dataset original preservado: {dataset_name}")
        
        return {
            'original_dataset': dataset_name,
            'new_dataset': new_dataset_name,
            'version': new_version,
            'modifications_applied': modifications,
            'version_pointer': version_pointer,
            'immutability_verified': True
        }
    
    def _apply_modifications(self, df: pd.DataFrame, modifications: Dict[str, Any]) -> pd.DataFrame:
        """Aplica modifica√ß√µes aos dados seguindo padr√£o 'data as code'."""
        modified_df = df.copy()
        
        # Aplicar filtros
        if 'filters' in modifications:
            for filter_expr in modifications['filters']:
                modified_df = modified_df.query(filter_expr)
        
        # Aplicar transforma√ß√µes
        if 'transformations' in modifications:
            for transform in modifications['transformations']:
                if transform['type'] == 'salary_adjustment':
                    factor = transform['factor']
                    modified_df['salary_base'] = modified_df['salary_base'] * factor
                    modified_df['total_compensation'] = modified_df['total_compensation'] * factor
                elif transform['type'] == 'compliance_recalc':
                    modified_df['compliance_score'] = modified_df['compliance_score'] * transform.get('factor', 1.0)
                    modified_df['compliance_flag'] = modified_df['compliance_score'].apply(
                        lambda x: 'excellent' if x >= 0.95 else 'good' if x >= 0.85 else 'warning'
                    )
        
        # Aplicar agrega√ß√µes
        if 'aggregations' in modifications:
            for agg in modifications['aggregations']:
                if agg['type'] == 'department_summary':
                    modified_df = modified_df.groupby('department').agg({
                        'salary_base': 'mean',
                        'compliance_score': 'mean',
                        'total_compensation': 'sum',
                        'employee_id': 'count'
                    }).reset_index()
                    modified_df.rename(columns={'employee_id': 'employee_count'}, inplace=True)
        
        return modified_df
    
    def rollback_to_version(self, dataset_base_name: str, target_version: int) -> Dict[str, Any]:
        """
        Reverte para vers√£o anterior do dataset alterando apenas o ponteiro de metadados.
        Demonstra o padr√£o 'dados como c√≥digo' com versionamento imut√°vel.
        """
        version_pointer_key = f"{dataset_base_name}_version_pointer"
        version_pointer = self.metadata_store.get(version_pointer_key)
        
        if not version_pointer:
            raise ValueError(f"Ponteiro de vers√£o n√£o encontrado para {dataset_base_name}")
        
        # Encontrar dataset da vers√£o solicitada
        target_dataset_name = None
        for version_info in version_pointer['version_history']:
            if version_info['version'] == target_version:
                target_dataset_name = version_info['dataset_name']
                break
        
        if not target_dataset_name:
            raise ValueError(f"Vers√£o {target_version} n√£o encontrada para {dataset_base_name}")
        
        # Verificar se dataset da vers√£o alvo existe
        if target_dataset_name not in self.metadata_store:
            raise ValueError(f"Dataset {target_dataset_name} n√£o existe nos metadados")
        
        # Atualizar ponteiro para vers√£o alvo
        old_current = version_pointer['current_dataset_name']
        version_pointer['current_version'] = target_version
        version_pointer['current_dataset_name'] = target_dataset_name
        version_pointer['last_updated'] = datetime.now().isoformat()
        version_pointer['rollback_from'] = old_current
        
        self.metadata_store[version_pointer_key] = version_pointer
        
        print(f"üîÑ Rollback executado com sucesso!")
        print(f"   ‚Ä¢ Dataset ativo alterado de {old_current} para {target_dataset_name}")
        print(f"   ‚Ä¢ Vers√£o atual: {target_version}")
        print(f"   ‚Ä¢ Dados originais preservados e inalterados")
        
        return {
            'rollback_successful': True,
            'previous_active_dataset': old_current,
            'new_active_dataset': target_dataset_name,
            'target_version': target_version,
            'pointer_updated': version_pointer_key
        }


class MockS3Client:
    """Cliente mock para demonstra√ß√£o quando R2 n√£o est√° dispon√≠vel."""
    
    def put_object(self, **kwargs):
        print(f"üì¶ Mock S3: PUT {kwargs.get('Key', 'unknown')} ({len(kwargs.get('Body', b''))//1024}KB)")
        return {'ETag': 'mock-etag'}
    
    def get_object(self, **kwargs):
        print(f"üì¶ Mock S3: GET {kwargs.get('Key', 'unknown')}")
        return {'Body': io.BytesIO(b'mock data')}


async def demo_decentralized_nervous_system():
    """Demonstra√ß√£o completa do Sistema Nervoso de Dados Descentralizado."""
    print("üß† === DEMO: SISTEMA NERVOSO DE DADOS DESCENTRALIZADO ===")
    
    # Inicializar sistema
    nervous_system = DecentralizedDataNervousSystem()
    
    # 1. Criar dataset Parquet otimizado de 10GB
    print("\nüéØ FASE 1: Cria√ß√£o de Dataset Massivo")
    dataset_info = nervous_system.create_optimized_parquet_dataset(
        "audit_dataset_massive", size_gb=1.0  # Reduzido para demo
    )
    
    # 2. Teste de consulta descentralizada massiva
    print("\nüéØ FASE 2: Teste de Consultas Paralelas Massivas")
    parallel_test_results = await nervous_system.massive_parallel_query_test(
        "audit_dataset_massive", concurrent_functions=100  # Reduzido para demo
    )
    
    # 3. Teste de imutabilidade e versionamento
    print("\nüéØ FASE 3: Teste de Imutabilidade e Versionamento")
    version_result = nervous_system.create_immutable_dataset_version(
        "audit_dataset_massive",
        modifications={
            'filters': ['salary_base > 4000'],
            'transformations': [
                {'type': 'salary_adjustment', 'factor': 1.05},
                {'type': 'compliance_recalc', 'factor': 1.02}
            ]
        }
    )
    
    # 4. Teste de rollback
    print("\nüéØ FASE 4: Teste de Rollback")
    rollback_result = nervous_system.rollback_to_version("audit_dataset_massive", 1)
    
    # Sum√°rio final
    print("\nüéâ === RESUMO DOS TESTES ===")
    print(f"‚úÖ Dataset massivo criado: {dataset_info['name']}")
    print(f"‚úÖ Consultas paralelas: {parallel_test_results['successful_queries']}/{parallel_test_results['concurrent_functions']}")
    print(f"‚úÖ P99 tempo consulta: {parallel_test_results['performance_metrics']['p99_execution_time_ms']:.1f}ms")
    print(f"‚úÖ Versionamento imut√°vel: {version_result['new_dataset']}")
    print(f"‚úÖ Rollback funcional: vers√£o {rollback_result['target_version']}")
    
    return {
        'dataset_creation': dataset_info,
        'parallel_queries': parallel_test_results,
        'immutable_versioning': version_result,
        'rollback_test': rollback_result
    }


if __name__ == "__main__":
    # Executar demonstra√ß√£o
    asyncio.run(demo_decentralized_nervous_system())