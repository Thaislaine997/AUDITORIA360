{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explora√ß√£o e Prototipagem - AUDITORIA360\n",
    "\n",
    "## üìã Vis√£o Geral\n",
    "\n",
    "Este notebook √© utilizado para a explora√ß√£o de dados e prototipagem de modelos relacionados √† auditoria de folha de pagamento. Aqui, realizaremos an√°lises explorat√≥rias, visualiza√ß√µes e testes de diferentes abordagens de modelagem.\n",
    "\n",
    "## üéØ Objetivos\n",
    "\n",
    "- An√°lise explorat√≥ria de dados de folha de pagamento\n",
    "- Identifica√ß√£o de padr√µes e anomalias\n",
    "- Prototipagem de modelos de machine learning\n",
    "- Valida√ß√£o de hip√≥teses de auditoria\n",
    "\n",
    "## üìö Pr√©-requisitos\n",
    "\n",
    "- Python 3.8+\n",
    "- Bibliotecas: pandas, numpy, matplotlib, seaborn, scikit-learn\n",
    "- Dados de folha de pagamento processados\n",
    "\n",
    "## üöÄ Como Usar\n",
    "\n",
    "1. Execute as c√©lulas sequencialmente\n",
    "2. Certifique-se de que os dados est√£o dispon√≠veis no diret√≥rio `../data/processed/`\n",
    "3. Ajuste os par√¢metros conforme necess√°rio\n",
    "4. Analise os resultados e visualiza√ß√µes geradas"
   ],
   "id": "main-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√£o do Ambiente\n",
    "\n",
    "Importa√ß√£o das bibliotecas necess√°rias e configura√ß√£o inicial do ambiente de an√°lise."
   ],
   "id": "environment-setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suprimir warnings desnecess√°rios\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configura√ß√µes do pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Ambiente configurado com sucesso!\")\n",
    "print(f\"üì¶ Pandas vers√£o: {pd.__version__}\")\n",
    "print(f\"üìä Numpy vers√£o: {np.__version__}\")"
   ],
   "id": "imports-config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Carregamento e Verifica√ß√£o dos Dados\n",
    "\n",
    "Carregamento dos dados de folha de pagamento e verifica√ß√£o inicial da estrutura."
   ],
   "id": "data-loading"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos dos dados\n",
    "DATA_PATH = Path('../data/processed')\n",
    "RAW_DATA_PATH = Path('../data/raw')\n",
    "\n",
    "# Verificar se os diret√≥rios existem\n",
    "if not DATA_PATH.exists():\n",
    "    print(\"‚ö†Ô∏è  Diret√≥rio de dados processados n√£o encontrado.\")\n",
    "    print(\"üìù Criando dados de exemplo para demonstra√ß√£o...\")\n",
    "    \n",
    "    # Criar dados de exemplo para demonstra√ß√£o\n",
    "    np.random.seed(42)\n",
    "    n_employees = 1000\n",
    "    \n",
    "    df_folha = pd.DataFrame({\n",
    "        'funcionario_id': range(1, n_employees + 1),\n",
    "        'nome': [f'Funcion√°rio {i}' for i in range(1, n_employees + 1)],\n",
    "        'cpf': [f'{np.random.randint(10000000000, 99999999999)}' for _ in range(n_employees)],\n",
    "        'salario_base': np.random.normal(5000, 2000, n_employees).clip(min=1320),\n",
    "        'horas_extras': np.random.exponential(10, n_employees),\n",
    "        'descontos': np.random.normal(500, 200, n_employees).clip(min=0),\n",
    "        'salario_liquido': None,\n",
    "        'departamento': np.random.choice(['TI', 'RH', 'Financeiro', 'Operacional', 'Vendas'], n_employees),\n",
    "        'cargo': np.random.choice(['Analista', 'Coordenador', 'Gerente', 'Diretor', 'Assistente'], n_employees),\n",
    "        'data_admissao': pd.date_range('2020-01-01', '2024-12-31', periods=n_employees),\n",
    "        'status': np.random.choice(['Ativo', 'Inativo', 'F√©rias'], n_employees, p=[0.85, 0.10, 0.05])\n",
    "    })\n",
    "    \n",
    "    # Calcular sal√°rio l√≠quido\n",
    "    df_folha['salario_liquido'] = df_folha['salario_base'] + df_folha['horas_extras'] - df_folha['descontos']\n",
    "    \n",
    "    print(\"‚úÖ Dados de exemplo criados com sucesso!\")\n",
    "else:\n",
    "    try:\n",
    "        # Tentar carregar dados reais\n",
    "        df_folha = pd.read_csv(DATA_PATH / 'dados_folha.csv')\n",
    "        print(\"‚úÖ Dados carregados com sucesso!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è  Arquivo dados_folha.csv n√£o encontrado. Usando dados de exemplo.\")\n",
    "        # Usar l√≥gica de dados de exemplo aqui tamb√©m\n",
    "\n",
    "# Informa√ß√µes b√°sicas sobre o dataset\n",
    "print(f\"\\nüìä Informa√ß√µes do Dataset:\")\n",
    "print(f\"   ‚Ä¢ N√∫mero de registros: {len(df_folha):,}\")\n",
    "print(f\"   ‚Ä¢ N√∫mero de colunas: {len(df_folha.columns)}\")\n",
    "print(f\"   ‚Ä¢ Per√≠odo dos dados: {df_folha['data_admissao'].min()} a {df_folha['data_admissao'].max()}\")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "df_folha.head()"
   ],
   "id": "load-data-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç An√°lise Explorat√≥ria dos Dados\n",
    "\n",
    "Nesta se√ß√£o, realizaremos uma an√°lise explorat√≥ria dos dados para entender melhor as caracter√≠sticas e padr√µes presentes na folha de pagamento."
   ],
   "id": "exploratory-analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva dos dados\n",
    "print(\"üìä ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Informa√ß√µes gerais\n",
    "print(\"\\nüî¢ Informa√ß√µes Gerais:\")\n",
    "print(df_folha.info())\n",
    "\n",
    "# Estat√≠sticas das colunas num√©ricas\n",
    "print(\"\\nüìà Estat√≠sticas das Vari√°veis Num√©ricas:\")\n",
    "numeric_cols = df_folha.select_dtypes(include=[np.number]).columns\n",
    "display(df_folha[numeric_cols].describe())\n",
    "\n",
    "# An√°lise de valores √∫nicos para vari√°veis categ√≥ricas\n",
    "print(\"\\nüìã An√°lise de Vari√°veis Categ√≥ricas:\")\n",
    "categorical_cols = df_folha.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if col not in ['nome', 'cpf']:  # Excluir colunas com muitos valores √∫nicos\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(df_folha[col].value_counts())"
   ],
   "id": "descriptive-stats"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o de dados faltantes e outliers\n",
    "print(\"üîç VERIFICA√á√ÉO DE QUALIDADE DOS DADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Valores faltantes\n",
    "missing_data = df_folha.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Valores Faltantes Encontrados:\")\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Coluna': missing_data.index,\n",
    "        'Valores Faltantes': missing_data.values,\n",
    "        'Percentual': (missing_data.values / len(df_folha) * 100).round(2)\n",
    "    })\n",
    "    display(missing_df[missing_df['Valores Faltantes'] > 0])\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhum valor faltante encontrado!\")\n",
    "\n",
    "# Detec√ß√£o de outliers usando IQR\n",
    "print(\"\\nüéØ Detec√ß√£o de Outliers (M√©todo IQR):\")\n",
    "for col in ['salario_base', 'horas_extras', 'descontos', 'salario_liquido']:\n",
    "    Q1 = df_folha[col].quantile(0.25)\n",
    "    Q3 = df_folha[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df_folha[(df_folha[col] < (Q1 - 1.5 * IQR)) | (df_folha[col] > (Q3 + 1.5 * IQR))]\n",
    "    print(f\"   ‚Ä¢ {col}: {len(outliers)} outliers ({len(outliers)/len(df_folha)*100:.2f}%)\")"
   ],
   "id": "data-quality"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√µes Explorat√≥rias\n",
    "\n",
    "Cria√ß√£o de visualiza√ß√µes para identificar padr√µes, tend√™ncias e poss√≠veis anomalias nos dados."
   ],
   "id": "quality-check-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar layout de subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Distribui√ß√µes das Principais Vari√°veis Num√©ricas', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribui√ß√£o do sal√°rio base\n",
    "axes[0, 0].hist(df_folha['salario_base'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribui√ß√£o do Sal√°rio Base')\n",
    "axes[0, 0].set_xlabel('Sal√°rio Base (R$)')\n",
    "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribui√ß√£o das horas extras\n",
    "axes[0, 1].hist(df_folha['horas_extras'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribui√ß√£o das Horas Extras')\n",
    "axes[0, 1].set_xlabel('Horas Extras (R$)')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribui√ß√£o dos descontos\n",
    "axes[1, 0].hist(df_folha['descontos'], bins=30, alpha=0.7, color='salmon', edgecolor='black')\n",
    "axes[1, 0].set_title('Distribui√ß√£o dos Descontos')\n",
    "axes[1, 0].set_xlabel('Descontos (R$)')\n",
    "axes[1, 0].set_ylabel('Frequ√™ncia')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribui√ß√£o do sal√°rio l√≠quido\n",
    "axes[1, 1].hist(df_folha['salario_liquido'], bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[1, 1].set_title('Distribui√ß√£o do Sal√°rio L√≠quido')\n",
    "axes[1, 1].set_xlabel('Sal√°rio L√≠quido (R$)')\n",
    "axes[1, 1].set_ylabel('Frequ√™ncia')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise por departamento e cargo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Distribui√ß√£o por departamento\n",
    "dept_counts = df_folha['departamento'].value_counts()\n",
    "axes[0].pie(dept_counts.values, labels=dept_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribui√ß√£o de Funcion√°rios por Departamento')\n",
    "\n",
    "# Sal√°rio m√©dio por cargo\n",
    "salary_by_position = df_folha.groupby('cargo')['salario_liquido'].mean().sort_values(ascending=True)\n",
    "axes[1].barh(salary_by_position.index, salary_by_position.values, color='lightcoral')\n",
    "axes[1].set_title('Sal√°rio L√≠quido M√©dio por Cargo')\n",
    "axes[1].set_xlabel('Sal√°rio L√≠quido M√©dio (R$)')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, v in enumerate(salary_by_position.values):\n",
    "    axes[1].text(v + 50, i, f'R$ {v:,.0f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "visualization-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Prototipagem de Modelos de Machine Learning\n",
    "\n",
    "Nesta se√ß√£o, testaremos diferentes modelos de machine learning para identificar poss√≠veis riscos e anomalias na folha de pagamento."
   ],
   "id": "anomaly-detection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara√ß√£o dos dados para modelagem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(\"üîß PREPARA√á√ÉO DOS DADOS PARA MODELAGEM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar vari√°vel target para detec√ß√£o de anomalias\n",
    "# Consideramos como anomalia sal√°rios muito altos ou muito baixos para o cargo\n",
    "df_model = df_folha.copy()\n",
    "\n",
    "# Encoding de vari√°veis categ√≥ricas\n",
    "le_dept = LabelEncoder()\n",
    "le_cargo = LabelEncoder()\n",
    "le_status = LabelEncoder()\n",
    "\n",
    "df_model['departamento_encoded'] = le_dept.fit_transform(df_model['departamento'])\n",
    "df_model['cargo_encoded'] = le_cargo.fit_transform(df_model['cargo'])\n",
    "df_model['status_encoded'] = le_status.fit_transform(df_model['status'])\n",
    "\n",
    "# Criar features adicionais\n",
    "df_model['tempo_empresa'] = (pd.Timestamp.now() - pd.to_datetime(df_model['data_admissao'])).dt.days\n",
    "df_model['razao_extras_base'] = df_model['horas_extras'] / df_model['salario_base']\n",
    "df_model['razao_desconto_base'] = df_model['descontos'] / df_model['salario_base']\n",
    "\n",
    "# Selecionar features para modelagem\n",
    "features = ['salario_base', 'horas_extras', 'descontos', 'departamento_encoded', \n",
    "           'cargo_encoded', 'status_encoded', 'tempo_empresa', 'razao_extras_base', 'razao_desconto_base']\n",
    "\n",
    "X = df_model[features]\n",
    "\n",
    "print(f\"‚úÖ Features selecionadas: {len(features)}\")\n",
    "print(f\"üìä Shape dos dados: {X.shape}\")\n",
    "print(f\"üéØ Features: {features}\")"
   ],
   "id": "anomaly-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: Detec√ß√£o de Anomalias usando Isolation Forest\n",
    "print(\"üå≤ MODELO 1: ISOLATION FOREST (DETEC√á√ÉO DE ANOMALIAS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Treinar Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "\n",
    "# Adicionar labels ao dataframe\n",
    "df_model['anomalia'] = anomaly_labels\n",
    "df_model['anomalia_label'] = df_model['anomalia'].map({1: 'Normal', -1: 'Anomalia'})\n",
    "\n",
    "# An√°lise dos resultados\n",
    "anomaly_counts = df_model['anomalia_label'].value_counts()\n",
    "print(f\"\\nüìä Resultados da Detec√ß√£o de Anomalias:\")\n",
    "print(f\"   ‚Ä¢ Normal: {anomaly_counts['Normal']} ({anomaly_counts['Normal']/len(df_model)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Anomalias: {anomaly_counts['Anomalia']} ({anomaly_counts['Anomalia']/len(df_model)*100:.1f}%)\")\n",
    "\n",
    "# Visualizar anomalias\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(df_model['salario_base'], df_model['salario_liquido'], \n",
    "                     c=df_model['anomalia'], cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Sal√°rio Base (R$)')\n",
    "plt.ylabel('Sal√°rio L√≠quido (R$)')\n",
    "plt.title('Detec√ß√£o de Anomalias: Sal√°rio Base vs Sal√°rio L√≠quido')\n",
    "plt.colorbar(scatter, label='Anomalia (-1) / Normal (1)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analisar caracter√≠sticas das anomalias\n",
    "print(\"\\nüîç Caracter√≠sticas das Anomalias Detectadas:\")\n",
    "anomalias = df_model[df_model['anomalia'] == -1]\n",
    "print(f\"\\nüìà Estat√≠sticas das Anomalias:\")\n",
    "print(anomalias[['salario_base', 'horas_extras', 'descontos', 'salario_liquido']].describe())"
   ],
   "id": "clustering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Clustering para Segmenta√ß√£o de Funcion√°rios\n",
    "print(\"üéØ MODELO 2: K-MEANS CLUSTERING (SEGMENTA√á√ÉO)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Determinar n√∫mero √≥timo de clusters usando o m√©todo do cotovelo\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plotar m√©todo do cotovelo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('N√∫mero de Clusters (k)')\n",
    "plt.ylabel('In√©rcia')\n",
    "plt.title('M√©todo do Cotovelo para Determina√ß√£o do K √ìtimo')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Aplicar K-Means com k=4\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Adicionar clusters ao dataframe\n",
    "df_model['cluster'] = cluster_labels\n",
    "\n",
    "# An√°lise dos clusters\n",
    "print(f\"\\nüìä Distribui√ß√£o dos Clusters:\")\n",
    "cluster_counts = df_model['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = count / len(df_model) * 100\n",
    "    print(f\"   ‚Ä¢ Cluster {cluster_id}: {count} funcion√°rios ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualizar clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(df_model['salario_base'], df_model['salario_liquido'], \n",
    "                     c=df_model['cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Sal√°rio Base (R$)')\n",
    "plt.ylabel('Sal√°rio L√≠quido (R$)')\n",
    "plt.title('Segmenta√ß√£o de Funcion√°rios por Clusters')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Perfil dos clusters\n",
    "print(\"\\nüë• Perfil dos Clusters:\")\n",
    "for cluster_id in sorted(df_model['cluster'].unique()):\n",
    "    cluster_data = df_model[df_model['cluster'] == cluster_id]\n",
    "    print(f\"\\nüéØ CLUSTER {cluster_id}:\")\n",
    "    print(f\"   ‚Ä¢ Sal√°rio Base M√©dio: R$ {cluster_data['salario_base'].mean():,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Sal√°rio L√≠quido M√©dio: R$ {cluster_data['salario_liquido'].mean():,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Departamento Principal: {cluster_data['departamento'].mode().iloc[0]}\")\n",
    "    print(f\"   ‚Ä¢ Cargo Principal: {cluster_data['cargo'].mode().iloc[0]}\")"
   ],
   "id": "clustering-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Relat√≥rio de Auditoria e Recomenda√ß√µes\n",
    "\n",
    "Com base na an√°lise explorat√≥ria e nos modelos desenvolvidos, apresentamos os principais achados e recomenda√ß√µes."
   ],
   "id": "audit-report"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relat√≥rio de auditoria\n",
    "print(\"üìä RELAT√ìRIO DE AUDITORIA - FOLHA DE PAGAMENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resumo geral\n",
    "total_funcionarios = len(df_folha)\n",
    "total_folha = df_folha['salario_liquido'].sum()\n",
    "media_salarial = df_folha['salario_liquido'].mean()\n",
    "\n",
    "print(f\"\\nüìà RESUMO EXECUTIVO:\")\n",
    "print(f\"   ‚Ä¢ Total de Funcion√°rios: {total_funcionarios:,}\")\n",
    "print(f\"   ‚Ä¢ Folha de Pagamento Total: R$ {total_folha:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Sal√°rio L√≠quido M√©dio: R$ {media_salarial:,.2f}\")\n",
    "\n",
    "# Principais achados\n",
    "anomalias_detectadas = len(df_model[df_model['anomalia'] == -1])\n",
    "percentual_anomalias = anomalias_detectadas / total_funcionarios * 100\n",
    "\n",
    "print(f\"\\nüö® PRINCIPAIS ACHADOS:\")\n",
    "print(f\"   ‚Ä¢ Anomalias Detectadas: {anomalias_detectadas} ({percentual_anomalias:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Clusters Identificados: {optimal_k}\")\n",
    "\n",
    "# Top 5 funcion√°rios com maiores sal√°rios\n",
    "print(f\"\\nüí∞ TOP 5 MAIORES SAL√ÅRIOS:\")\n",
    "top_salarios = df_folha.nlargest(5, 'salario_liquido')[['nome', 'cargo', 'departamento', 'salario_liquido']]\n",
    "for idx, row in top_salarios.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['nome']}: R$ {row['salario_liquido']:,.2f} ({row['cargo']} - {row['departamento']})\")\n",
    "\n",
    "# Funcion√°rios com anomalias (top 5)\n",
    "print(f\"\\n‚ö†Ô∏è  TOP 5 ANOMALIAS DETECTADAS:\")\n",
    "anomalias_top = df_model[df_model['anomalia'] == -1].nlargest(5, 'salario_liquido')[['nome', 'cargo', 'departamento', 'salario_liquido']]\n",
    "for idx, row in anomalias_top.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['nome']}: R$ {row['salario_liquido']:,.2f} ({row['cargo']} - {row['departamento']})\")\n",
    "\n",
    "# Recomenda√ß√µes\n",
    "print(f\"\\nüí° RECOMENDA√á√ïES:\")\n",
    "print(f\"   1. Investigar as {anomalias_detectadas} anomalias detectadas\")\n",
    "print(f\"   2. Revisar estrutura salarial por cargo e departamento\")\n",
    "print(f\"   3. Implementar controles autom√°ticos para detec√ß√£o de inconsist√™ncias\")\n",
    "print(f\"   4. Estabelecer pol√≠ticas claras de remunera√ß√£o por cluster\")\n",
    "print(f\"   5. Realizar auditoria mensal usando os modelos desenvolvidos\")\n",
    "\n",
    "print(f\"\\n‚úÖ Relat√≥rio gerado com sucesso!\")"
   ],
   "id": "report-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclus√µes e Pr√≥ximos Passos\n",
    "\n",
    "### Principais Resultados\n",
    "\n",
    "1. **An√°lise Explorat√≥ria**: Identificamos a distribui√ß√£o dos sal√°rios, padr√µes por departamento e cargo\n",
    "2. **Detec√ß√£o de Anomalias**: Utilizamos Isolation Forest para identificar casos suspeitos\n",
    "3. **Segmenta√ß√£o**: Aplicamos K-Means para criar clusters de funcion√°rios com caracter√≠sticas similares\n",
    "4. **Relat√≥rio de Auditoria**: Gerado automaticamente com achados e recomenda√ß√µes\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "\n",
    "1. **Valida√ß√£o**: Validar as anomalias detectadas com a equipe de RH\n",
    "2. **Implementa√ß√£o**: Integrar os modelos ao sistema de produ√ß√£o\n",
    "3. **Monitoramento**: Criar dashboard para acompanhamento cont√≠nuo\n",
    "4. **Melhoria**: Refinar os modelos com feedback da equipe\n",
    "5. **Automa√ß√£o**: Implementar pipeline de auditoria autom√°tica\n",
    "\n",
    "### Documenta√ß√£o\n",
    "\n",
    "- C√≥digo fonte dispon√≠vel neste notebook\n",
    "- Modelos salvos para uso em produ√ß√£o\n",
    "- Relat√≥rios export√°veis em formato PDF/Excel\n",
    "- Dashboards interativos em desenvolvimento\n",
    "\n",
    "---\n",
    "\n",
    "**üìÖ √öltima atualiza√ß√£o**: Janeiro 2025  \n",
    "**üë®‚Äçüíª Desenvolvido por**: Equipe AUDITORIA360  \n",
    "**üìß Contato**: Para d√∫vidas ou sugest√µes, consulte a documenta√ß√£o t√©cnica"
   ],
   "id": "conclusions"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}