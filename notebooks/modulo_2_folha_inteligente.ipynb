{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "main-header",
   "metadata": {},
   "source": [
    "# Módulo 2: Controle Mensal da Folha Inteligente - AUDITORIA360\n",
    "\n",
    "## 📋 Visão Geral\n",
    "\n",
    "Este notebook documenta e demonstra as funcionalidades do Módulo 2, incluindo:\n",
    "- Importação de extratos de folha de pagamento em formato PDF\n",
    "- Processamento assíncrono usando OCR (substituto do Document AI)\n",
    "- Validação e mapeamento de dados\n",
    "- Armazenamento estruturado no banco de dados\n",
    "\n",
    "## 🎯 Objetivos\n",
    "\n",
    "- Automatizar a extração de dados de PDFs de folha de pagamento\n",
    "- Implementar processamento assíncrono para arquivos grandes\n",
    "- Validar e mapear dados extraídos para estrutura interna\n",
    "- Monitorar e reportar status de processamento\n",
    "\n",
    "## 📚 Pré-requisitos\n",
    "\n",
    "- Python 3.8+\n",
    "- Bibliotecas: requests, pandas, streamlit, paddleocr\n",
    "- API AUDITORIA360 em execução\n",
    "- Arquivos PDF de folha de pagamento\n",
    "\n",
    "## 🚀 Como Usar\n",
    "\n",
    "1. Configure as variáveis de ambiente\n",
    "2. Execute as células sequencialmente\n",
    "3. Faça upload do arquivo PDF\n",
    "4. Monitore o processamento\n",
    "5. Visualize os resultados"
=======
   "id": "8434983e",
   "metadata": {},
   "source": [
    "# Módulo 2: Controle Mensal da Folha Inteligente\n",
    "\n",
    "Este notebook documenta e prototipa as funcionalidades do Módulo 2 do sistema AUDITORIA360, focando na importação e extração de dados de extratos de folha de pagamento. O fluxo principal inclui:\n",
    "\n",
    "1. **Upload de arquivos PDF** via interface Streamlit\n",
    "2. **Processamento assíncrono** usando Document AI\n",
    "3. **Validação e mapeamento** dos dados extraídos\n",
    "4. **Armazenamento** estruturado no BigQuery\n",
    "5. **Monitoramento** de status e tratamento de erros\n",
    "\n",
    "## Arquitetura do Sistema\n",
    "- **Frontend**: Streamlit para interface de usuário\n",
    "- **Backend**: API FastAPI para processamento assíncrono\n",
    "- **OCR**: Document AI para extração de dados de PDFs\n",
    "- **Storage**: Google Cloud Storage para arquivos\n",
    "- **Database**: BigQuery para dados estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd6fee",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Configuração inicial das bibliotecas, autenticação com Google Cloud e inicialização de variáveis globais necessárias para o funcionamento do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62802b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Google Cloud libraries\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "# Configuração da autenticação com Google Cloud\n",
    "# Nota: Em produção, use variáveis de ambiente ou service account keys\n",
    "SERVICE_ACCOUNT_PATH = 'path/to/your-service-account-key.json'\n",
    "if os.path.exists(SERVICE_ACCOUNT_PATH):\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_PATH)\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bigquery_client = bigquery.Client(credentials=credentials)\n",
    "    docai_client = documentai.DocumentProcessorServiceClient(credentials=credentials)\n",
    "else:\n",
    "    # Usar credenciais padrão do ambiente\n",
    "    storage_client = storage.Client()\n",
    "    bigquery_client = bigquery.Client()\n",
    "    docai_client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "# Variáveis de configuração globais\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'seu-projeto-id')\n",
    "BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'seu-bucket-folhas-clientes')\n",
    "PROCESSOR_ID = os.getenv('DOCAI_PROCESSOR_ID', 'seu-processor-id')\n",
    "LOCATION = os.getenv('DOCAI_LOCATION', 'us')\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "\n",
    "print(f\"Configuração carregada:\")\n",
    "print(f\"- Projeto: {PROJECT_ID}\")\n",
    "print(f\"- Bucket: {BUCKET_NAME}\")\n",
    "print(f\"- Processor ID: {PROCESSOR_ID}\")\n",
    "print(f\"- Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8822d2",
   "metadata": {},
   "source": [
    "## 2. Upload de Arquivo PDF\n",
    "\n",
    "Interface para upload de arquivos PDF usando Streamlit. Os arquivos são validados e salvos no Google Cloud Storage para processamento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf_interface():\n",
    "    \"\"\"\n",
    "    Interface Streamlit para upload de arquivos PDF.\n",
    "    Valida o arquivo e realiza o upload para o Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    st.title(\"📄 Upload de Extrato de Folha de Pagamento\")\n",
    "    st.markdown(\"Selecione o arquivo PDF do extrato para processamento automático.\")\n",
    "    \n",
    "    # Widget de upload de arquivo\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Selecione o arquivo PDF:\", \n",
    "        type=[\"pdf\"],\n",
    "        help=\"Apenas arquivos PDF são aceitos. Tamanho máximo: 10MB\"\n",
    "    )\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Validar tamanho do arquivo (máximo 10MB)\n",
    "        file_size = len(uploaded_file.getvalue())\n",
    "        if file_size > 10 * 1024 * 1024:  # 10MB\n",
    "            st.error(\"❌ Arquivo muito grande. Tamanho máximo permitido: 10MB\")\n",
    "            return None\n",
    "            \n",
    "        st.success(f\"✅ Arquivo '{uploaded_file.name}' carregado com sucesso!\")\n",
    "        st.info(f\"📊 Tamanho: {file_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Botão para processar o arquivo\n",
    "        if st.button(\"🚀 Processar Arquivo\", type=\"primary\"):\n",
    "            with st.spinner(\"Salvando arquivo no Google Cloud Storage...\"):\n",
    "                try:\n",
    "                    # Upload para GCS\n",
    "                    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "                    blob_name = f\"uploads/{uploaded_file.name}\"\n",
    "                    blob = bucket.blob(blob_name)\n",
    "                    \n",
    "                    # Reset file pointer and upload\n",
    "                    uploaded_file.seek(0)\n",
    "                    blob.upload_from_file(uploaded_file, content_type='application/pdf')\n",
    "                    \n",
    "                    gcs_uri = f\"gs://{BUCKET_NAME}/{blob_name}\"\n",
    "                    st.success(f\"✅ Arquivo salvo no GCS: {gcs_uri}\")\n",
    "                    return gcs_uri\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"❌ Erro ao salvar arquivo: {str(e)}\")\n",
    "                    return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Exemplo de uso da interface\n",
    "# gcs_uri = upload_pdf_interface()"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "config-section",
   "metadata": {},
   "source": [
    "## 🔧 Configuração do Ambiente\n",
    "\n",
    "Configuração inicial das bibliotecas, variáveis de ambiente e autenticação."
=======
   "id": "ffb271a1",
   "metadata": {},
   "source": [
    "## 3. Processamento Assíncrono com Document AI\n",
    "\n",
    "Processamento de documentos PDF utilizando o Google Document AI para extração de dados estruturados. O processo é assíncrono para melhor performance."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suprimir warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações da API\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "CLIENT_ID = os.getenv('CLIENT_ID', '12345')\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Ambiente configurado com sucesso!\")\n",
    "print(f\"🌐 API Base URL: {API_BASE_URL}\")\n",
    "print(f\"🆔 Client ID: {CLIENT_ID}\")"
=======
   "id": "4b9b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_docai(gcs_uri, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Processa um PDF armazenado no GCS usando Document AI.\n",
    "    \n",
    "    Args:\n",
    "        gcs_uri (str): URI do arquivo no Google Cloud Storage\n",
    "        client_id (str): ID do cliente para processamento\n",
    "    \n",
    "    Returns:\n",
    "        str: Job ID para acompanhamento do processamento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iniciar processamento assíncrono via API\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async\",\n",
    "            json={\"gcs_uri\": gcs_uri}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            job_data = response.json()\n",
    "            job_id = job_data.get(\"job_id\")\n",
    "            print(f\"✅ Job iniciado com sucesso. ID: {job_id}\")\n",
    "            print(f\"📊 Status: {job_data.get('status', 'INICIADO')}\")\n",
    "            return job_id\n",
    "        else:\n",
    "            print(f\"❌ Erro ao iniciar processamento: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Erro de conexão: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def check_job_status(job_id, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Consulta o status de um job de processamento.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): ID do job para consulta\n",
    "        client_id (str): ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Informações sobre o status do job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async/status/{job_id}\"\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"❌ Erro ao consultar status: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Erro de conexão: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso\n",
    "# gcs_uri_example = \"gs://seu-bucket/exemplo.pdf\"\n",
    "# job_id = process_pdf_with_docai(gcs_uri_example, \"12345\")\n",
    "# if job_id:\n",
    "#     status = check_job_status(job_id, \"12345\")\n",
    "#     print(f\"Status atual: {status}\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "upload-section",
   "metadata": {},
   "source": [
    "## 📄 Upload e Processamento de PDF\n",
    "\n",
    "Interface para upload de arquivos PDF e início do processamento assíncrono."
=======
   "id": "389c2281",
   "metadata": {},
   "source": [
    "## 4. Validação e Mapeamento de Dados\n",
    "\n",
    "Os dados extraídos do Document AI são validados e mapeados para a estrutura interna do sistema antes do armazenamento."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "pdf-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_process_pdf(pdf_file_path, client_id=CLIENT_ID):\n",
    "    \"\"\"\n",
    "    Faz upload do arquivo PDF e inicia o processamento assíncrono.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file_path (str): Caminho para o arquivo PDF\n",
    "        client_id (str): ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resposta da API com job_id se bem-sucedido\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar se o arquivo existe\n",
    "        if not os.path.exists(pdf_file_path):\n",
    "            print(f\"❌ Arquivo não encontrado: {pdf_file_path}\")\n",
    "            print(\"📝 Criando arquivo de exemplo para demonstração...\")\n",
    "            \n",
    "            # Criar dados de exemplo em formato JSON para simular PDF processado\n",
    "            sample_data = {\n",
    "                \"funcionarios\": [\n",
    "                    {\n",
    "                        \"nome\": \"João Silva Santos\",\n",
    "                        \"cpf\": \"123.456.789-00\",\n",
    "                        \"cargo\": \"Analista\",\n",
    "                        \"salario_base\": 5000.00,\n",
    "                        \"horas_extras\": 300.00,\n",
    "                        \"descontos\": 850.00,\n",
    "                        \"salario_liquido\": 4450.00\n",
    "                    },\n",
    "                    {\n",
    "                        \"nome\": \"Maria Oliveira Costa\",\n",
    "                        \"cpf\": \"987.654.321-00\",\n",
    "                        \"cargo\": \"Coordenadora\",\n",
    "                        \"salario_base\": 7500.00,\n",
    "                        \"horas_extras\": 500.00,\n",
    "                        \"descontos\": 1200.00,\n",
    "                        \"salario_liquido\": 6800.00\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Simular processamento bem-sucedido\n",
    "            job_id = f\"job_{int(time.time())}\"\n",
    "            print(f\"✅ Processamento simulado iniciado com sucesso!\")\n",
    "            print(f\"🆔 Job ID: {job_id}\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"job_id\": job_id,\n",
    "                \"message\": \"Processamento iniciado com sucesso\",\n",
    "                \"data\": sample_data\n",
    "            }\n",
    "        \n",
    "        # Processar arquivo real\n",
    "        with open(pdf_file_path, 'rb') as pdf_file:\n",
    "            response = requests.post(\n",
    "                f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async\",\n",
    "                files={\"file\": pdf_file},\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            job_id = result.get(\"job_id\")\n",
    "            print(f\"✅ Job iniciado com sucesso!\")\n",
    "            print(f\"🆔 Job ID: {job_id}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"❌ Erro ao iniciar o job: {response.text}\")\n",
    "            return {\"status\": \"error\", \"message\": response.text}\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Erro de conexão: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Exemplo de uso\n",
    "PDF_FILE_PATH = \"sample_extrato.pdf\"\n",
    "result = upload_and_process_pdf(PDF_FILE_PATH)\n",
    "\n",
    "# Armazenar job_id para uso posterior\n",
    "if result.get(\"status\") == \"success\":\n",
    "    CURRENT_JOB_ID = result.get(\"job_id\")\n",
    "    print(f\"\\n📋 Job ID armazenado: {CURRENT_JOB_ID}\")\n",
    "else:\n",
    "    CURRENT_JOB_ID = None"
=======
   "id": "4d427a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def validate_cpf(cpf: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida formato e dígitos verificadores do CPF.\n",
    "    \"\"\"\n",
    "    if not cpf:\n",
    "        return False\n",
    "    \n",
    "    # Remove caracteres não numéricos\n",
    "    cpf_numbers = re.sub(r'\\D', '', cpf)\n",
    "    \n",
    "    # Verifica se tem 11 dígitos\n",
    "    if len(cpf_numbers) != 11:\n",
    "        return False\n",
    "    \n",
    "    # Verifica se não são todos os dígitos iguais\n",
    "    if cpf_numbers == cpf_numbers[0] * 11:\n",
    "        return False\n",
    "    \n",
    "    # Cálculo dos dígitos verificadores (simplificado para exemplo)\n",
    "    return True\n",
    "\n",
    "def validate_folha_data(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Valida e mapeia dados extraídos da folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de registros extraídos do Document AI\n",
    "    \n",
    "    Returns:\n",
    "        Lista de registros validados com indicadores de erro\n",
    "    \"\"\"\n",
    "    validated_data = []\n",
    "    \n",
    "    for i, record in enumerate(data):\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Validação do CPF\n",
    "        if not validate_cpf(record.get('cpf', '')):\n",
    "            errors.append(\"CPF inválido ou ausente\")\n",
    "        \n",
    "        # Validação do nome do funcionário\n",
    "        if not record.get('funcionario_nome', '').strip():\n",
    "            errors.append(\"Nome do funcionário ausente\")\n",
    "        \n",
    "        # Validação da rubrica\n",
    "        if not record.get('rubrica', '').strip():\n",
    "            warnings.append(\"Rubrica não identificada\")\n",
    "        \n",
    "        # Validação do valor\n",
    "        try:\n",
    "            valor = float(record.get('valor', 0))\n",
    "            if valor == 0:\n",
    "                warnings.append(\"Valor zero detectado\")\n",
    "        except (ValueError, TypeError):\n",
    "            errors.append(\"Valor inválido\")\n",
    "            valor = 0.0\n",
    "        \n",
    "        # Mapeamento para estrutura interna\n",
    "        validated_record = {\n",
    "            'id_linha': i + 1,\n",
    "            'funcionario_nome': record.get('funcionario_nome', '').strip(),\n",
    "            'cpf': re.sub(r'\\D', '', record.get('cpf', '')),\n",
    "            'rubrica': record.get('rubrica', '').strip(),\n",
    "            'valor': valor,\n",
    "            'data_processamento': pd.Timestamp.now().isoformat(),\n",
    "            'status_validacao': 'ERRO' if errors else ('AVISO' if warnings else 'OK'),\n",
    "            'erros': errors,\n",
    "            'avisos': warnings\n",
    "        }\n",
    "        \n",
    "        validated_data.append(validated_record)\n",
    "    \n",
    "    return validated_data\n",
    "\n",
    "# Exemplo de dados extraídos (simulação)\n",
    "dados_exemplo = [\n",
    "    {\"funcionario_nome\": \"João Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"Salário Base\", \"valor\": 5000.00},\n",
    "    {\"funcionario_nome\": \"Maria Oliveira\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale Transporte\", \"valor\": 150.00},\n",
    "    {\"funcionario_nome\": \"Pedro Santos\", \"cpf\": \"111.111.111-11\", \"rubrica\": \"FGTS\", \"valor\": 400.00},  # CPF inválido\n",
    "    {\"funcionario_nome\": \"\", \"cpf\": \"456.789.123-00\", \"rubrica\": \"Desconto INSS\", \"valor\": -300.00},  # Nome ausente\n",
    "]\n",
    "\n",
    "# Processar validação\n",
    "dados_validados = validate_folha_data(dados_exemplo)\n",
    "\n",
    "# Converter para DataFrame para melhor visualização\n",
    "df_validado = pd.DataFrame(dados_validados)\n",
    "print(\"📊 Resultados da Validação:\")\n",
    "print(df_validado[['funcionario_nome', 'cpf', 'rubrica', 'valor', 'status_validacao']].head())\n",
    "\n",
    "# Estatísticas de validação\n",
    "stats = df_validado['status_validacao'].value_counts()\n",
    "print(f\"\\n📈 Estatísticas de Validação:\")\n",
    "for status, count in stats.items():\n",
    "    print(f\"  {status}: {count} registros\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "## 🔍 Monitoramento do Processamento\n",
    "\n",
    "Funções para consultar o status do job e monitorar o progresso do processamento."
=======
   "id": "4091248a",
   "metadata": {},
   "source": [
    "## 5. Armazenamento no BigQuery\n",
    "\n",
    "Dados validados são armazenados no BigQuery com estrutura otimizada para consultas de auditoria."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "job-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_job_status(job_id, client_id=CLIENT_ID, max_attempts=20):\n",
    "    \"\"\"\n",
    "    Consulta o status de um job de processamento.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): ID do job\n",
    "        client_id (str): ID do cliente\n",
    "        max_attempts (int): Número máximo de tentativas\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status final do job\n",
    "    \"\"\"\n",
    "    if not job_id:\n",
    "        print(\"⚠️  Job ID não fornecido. Simulando status...\")\n",
    "        return {\n",
    "            \"status\": \"CONCLUIDO_SUCESSO\",\n",
    "            \"message\": \"Processamento simulado concluído com sucesso\",\n",
    "            \"progress\": 100\n",
    "        }\n",
    "    \n",
    "    print(f\"🔍 Monitorando job: {job_id}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async/status/{job_id}\",\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                job_data = response.json()\n",
    "                status = job_data.get(\"status\")\n",
    "                progress = job_data.get(\"progress\", 0)\n",
    "                \n",
    "                print(f\"📊 Tentativa {attempt + 1}: Status = {status} ({progress}%)\")\n",
    "                \n",
    "                # Status finais\n",
    "                if status in [\"CONCLUIDO_SUCESSO\", \"CONCLUIDO_COM_PENDENCIAS\", \"FALHA_DOCAI\", \"FALHA_MAPEAMENTO\"]:\n",
    "                    if status == \"CONCLUIDO_SUCESSO\":\n",
    "                        print(\"✅ Processamento concluído com sucesso!\")\n",
    "                    elif status == \"CONCLUIDO_COM_PENDENCIAS\":\n",
    "                        print(\"⚠️  Processamento concluído com pendências\")\n",
    "                    else:\n",
    "                        print(f\"❌ Falha no processamento: {status}\")\n",
    "                    \n",
    "                    return job_data\n",
    "                \n",
    "            else:\n",
    "                print(f\"❌ Erro na consulta (tentativa {attempt + 1}): {response.text}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"⚠️  Erro de conexão (tentativa {attempt + 1}): {str(e)}\")\n",
    "        \n",
    "        attempt += 1\n",
    "        if attempt < max_attempts:\n",
    "            print(\"⏳ Aguardando 5 segundos...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    print(f\"⏰ Timeout: Máximo de tentativas ({max_attempts}) atingido\")\n",
    "    return {\"status\": \"TIMEOUT\", \"message\": \"Timeout na consulta do status\"}\n",
    "\n",
    "# Monitorar o job atual\n",
    "if CURRENT_JOB_ID:\n",
    "    final_status = check_job_status(CURRENT_JOB_ID)\n",
    "    print(f\"\\n🎯 Status final: {final_status}\")\n",
    "else:\n",
    "    print(\"⚠️  Nenhum job ativo para monitorar\")\n",
    "    final_status = check_job_status(None)  # Simular status"
=======
   "id": "c18a1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_validated_data_to_bigquery(validated_data: List[Dict], dataset_id: str = \"auditoria_folha_dataset\", table_id: str = \"LinhasFolhaFuncionario\"):\n",
    "    \"\"\"\n",
    "    Insere dados validados no BigQuery com tratamento de erros.\n",
    "    \n",
    "    Args:\n",
    "        validated_data: Lista de dados validados\n",
    "        dataset_id: ID do dataset no BigQuery\n",
    "        table_id: ID da tabela no BigQuery\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado da operação com estatísticas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filtrar apenas dados sem erros críticos\n",
    "        dados_para_insercao = [\n",
    "            record for record in validated_data \n",
    "            if record['status_validacao'] != 'ERRO'\n",
    "        ]\n",
    "        \n",
    "        if not dados_para_insercao:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'Nenhum dado válido para inserção',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(validated_data)\n",
    "            }\n",
    "        \n",
    "        # Preparar dados para BigQuery (remover campos de validação)\n",
    "        bq_data = []\n",
    "        for record in dados_para_insercao:\n",
    "            bq_record = {\n",
    "                'id_linha': record['id_linha'],\n",
    "                'funcionario_nome': record['funcionario_nome'],\n",
    "                'cpf': record['cpf'],\n",
    "                'rubrica': record['rubrica'],\n",
    "                'valor': record['valor'],\n",
    "                'data_processamento': record['data_processamento'],\n",
    "                'status_validacao': record['status_validacao']\n",
    "            }\n",
    "            bq_data.append(bq_record)\n",
    "        \n",
    "        # Inserir no BigQuery\n",
    "        table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
    "        errors = bigquery_client.insert_rows_json(table_ref, bq_data)\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"❌ Erros ao inserir no BigQuery: {errors}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f'Erros na inserção: {errors}',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(bq_data)\n",
    "            }\n",
    "        else:\n",
    "            print(f\"✅ {len(bq_data)} registros inseridos com sucesso no BigQuery!\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'message': 'Dados inseridos com sucesso',\n",
    "                'inserted_count': len(bq_data),\n",
    "                'error_count': len(validated_data) - len(bq_data)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado: {str(e)}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': f'Erro inesperado: {str(e)}',\n",
    "            'inserted_count': 0,\n",
    "            'error_count': len(validated_data)\n",
    "        }\n",
    "\n",
    "# Exemplo de uso com dados validados\n",
    "if 'dados_validados' in locals():\n",
    "    resultado_insercao = insert_validated_data_to_bigquery(dados_validados)\n",
    "    print(f\"\\n📊 Resultado da Inserção:\")\n",
    "    print(f\"  Sucesso: {resultado_insercao['success']}\")\n",
    "    print(f\"  Registros inseridos: {resultado_insercao['inserted_count']}\")\n",
    "    print(f\"  Registros com erro: {resultado_insercao['error_count']}\")\n",
    "    print(f\"  Mensagem: {resultado_insercao['message']}\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## ✅ Validação e Mapeamento de Dados\n",
    "\n",
    "Processamento dos dados extraídos, validação e mapeamento para a estrutura interna do sistema."
=======
   "id": "847aa03a",
   "metadata": {},
   "source": [
    "## 6. Monitoramento e Visualização\n",
    "\n",
    "Acompanhamento do processamento e visualização dos resultados para análise e auditoria."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "data-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_map_data(extracted_data):\n",
    "    \"\"\"\n",
    "    Valida e mapeia os dados extraídos do PDF.\n",
    "    \n",
    "    Args:\n",
    "        extracted_data (dict): Dados extraídos do processamento\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dados validados e mapeados\n",
    "    \"\"\"\n",
    "    print(\"🔍 VALIDAÇÃO E MAPEAMENTO DE DADOS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Obter dados de exemplo ou dados reais\n",
    "    if not extracted_data or not extracted_data.get(\"funcionarios\"):\n",
    "        print(\"📝 Usando dados de exemplo para demonstração...\")\n",
    "        funcionarios_data = [\n",
    "            {\n",
    "                \"nome\": \"João Silva Santos\",\n",
    "                \"cpf\": \"123.456.789-00\",\n",
    "                \"cargo\": \"Analista\",\n",
    "                \"salario_base\": 5000.00,\n",
    "                \"horas_extras\": 300.00,\n",
    "                \"descontos\": 850.00,\n",
    "                \"salario_liquido\": 4450.00\n",
    "            },\n",
    "            {\n",
    "                \"nome\": \"Maria Oliveira Costa\",\n",
    "                \"cpf\": \"987.654.321-00\",\n",
    "                \"cargo\": \"Coordenadora\",\n",
    "                \"salario_base\": 7500.00,\n",
    "                \"horas_extras\": 500.00,\n",
    "                \"descontos\": 1200.00,\n",
    "                \"salario_liquido\": 6800.00\n",
    "            },\n",
    "            {\n",
    "                \"nome\": \"Pedro Santos Lima\",\n",
    "                \"cpf\": \"456.789.123-00\",\n",
    "                \"cargo\": \"Gerente\",\n",
    "                \"salario_base\": 12000.00,\n",
    "                \"horas_extras\": 0.00,\n",
    "                \"descontos\": 2100.00,\n",
    "                \"salario_liquido\": 9900.00\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        funcionarios_data = extracted_data[\"funcionarios\"]\n",
    "    \n",
    "    # Criar DataFrame para validação\n",
    "    df = pd.DataFrame(funcionarios_data)\n",
    "    \n",
    "    print(f\"📊 Total de registros para validar: {len(df)}\")\n",
    "    \n",
    "    # Função de validação\n",
    "    def validate_record(row):\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Validar CPF\n",
    "        if not row.get('cpf') or len(str(row['cpf']).replace('.', '').replace('-', '')) != 11:\n",
    "            errors.append(\"CPF inválido ou ausente\")\n",
    "        \n",
    "        # Validar nome\n",
    "        if not row.get('nome') or len(str(row['nome']).strip()) < 3:\n",
    "            errors.append(\"Nome inválido ou muito curto\")\n",
    "        \n",
    "        # Validar valores financeiros\n",
    "        if row.get('salario_base', 0) <= 0:\n",
    "            errors.append(\"Salário base deve ser maior que zero\")\n",
    "        \n",
    "        if row.get('salario_base', 0) < 1320:  # Salário mínimo aproximado\n",
    "            warnings.append(\"Salário base abaixo do salário mínimo\")\n",
    "        \n",
    "        if row.get('horas_extras', 0) < 0:\n",
    "            errors.append(\"Horas extras não podem ser negativas\")\n",
    "        \n",
    "        if row.get('descontos', 0) < 0:\n",
    "            errors.append(\"Descontos não podem ser negativos\")\n",
    "        \n",
    "        # Validar consistência do salário líquido\n",
    "        calculated_liquido = row.get('salario_base', 0) + row.get('horas_extras', 0) - row.get('descontos', 0)\n",
    "        actual_liquido = row.get('salario_liquido', 0)\n",
    "        \n",
    "        if abs(calculated_liquido - actual_liquido) > 0.01:  # Tolerância de 1 centavo\n",
    "            warnings.append(f\"Inconsistência no salário líquido (calculado: {calculated_liquido:.2f}, informado: {actual_liquido:.2f})\")\n",
    "        \n",
    "        return {\n",
    "            'errors': errors,\n",
    "            'warnings': warnings,\n",
    "            'is_valid': len(errors) == 0\n",
    "        }\n",
    "    \n",
    "    # Aplicar validação\n",
    "    validation_results = df.apply(validate_record, axis=1)\n",
    "    \n",
    "    # Adicionar resultados ao DataFrame\n",
    "    df['validation_errors'] = [result['errors'] for result in validation_results]\n",
    "    df['validation_warnings'] = [result['warnings'] for result in validation_results]\n",
    "    df['is_valid'] = [result['is_valid'] for result in validation_results]\n",
    "    \n",
    "    # Estatísticas de validação\n",
    "    total_records = len(df)\n",
    "    valid_records = df['is_valid'].sum()\n",
    "    invalid_records = total_records - valid_records\n",
    "    \n",
    "    print(f\"\\n📈 Resultados da Validação:\")\n",
    "    print(f\"   ✅ Registros válidos: {valid_records} ({valid_records/total_records*100:.1f}%)\")\n",
    "    print(f\"   ❌ Registros inválidos: {invalid_records} ({invalid_records/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar detalhes dos erros\n",
    "    if invalid_records > 0:\n",
    "        print(f\"\\n⚠️  Detalhes dos Erros:\")\n",
    "        invalid_df = df[~df['is_valid']]\n",
    "        for idx, row in invalid_df.iterrows():\n",
    "            print(f\"   • {row['nome']}: {', '.join(row['validation_errors'])}\")\n",
    "    \n",
    "    # Mostrar warnings\n",
    "    warnings_count = sum(len(w) for w in df['validation_warnings'])\n",
    "    if warnings_count > 0:\n",
    "        print(f\"\\n⚠️  Avisos encontrados: {warnings_count}\")\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['validation_warnings']:\n",
    "                print(f\"   • {row['nome']}: {', '.join(row['validation_warnings'])}\")\n",
    "    \n",
    "    # Retornar dados validados\n",
    "    valid_data = df[df['is_valid']].drop(['validation_errors', 'validation_warnings', 'is_valid'], axis=1)\n",
    "    \n",
    "    return {\n",
    "        'all_data': df,\n",
    "        'valid_data': valid_data,\n",
    "        'validation_summary': {\n",
    "            'total_records': total_records,\n",
    "            'valid_records': valid_records,\n",
    "            'invalid_records': invalid_records,\n",
    "            'warnings_count': warnings_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Validar dados do resultado do processamento\n",
    "if 'result' in locals() and result.get('data'):\n",
    "    validation_result = validate_and_map_data(result['data'])\n",
    "else:\n",
    "    validation_result = validate_and_map_data(None)\n",
    "\n",
    "# Exibir dados válidos\n",
    "print(f\"\\n📋 Dados Válidos para Armazenamento:\")\n",
    "if not validation_result['valid_data'].empty:\n",
    "    display(validation_result['valid_data'])\n",
    "else:\n",
    "    print(\"   ❌ Nenhum registro válido encontrado\")"
=======
   "id": "d934db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_processing_pipeline(job_id: str, client_id: str = \"default\", max_wait_time: int = 300):\n",
    "    \"\"\"\n",
    "    Monitora o pipeline de processamento com timeout e relatórios de progresso.\n",
    "    \n",
    "    Args:\n",
    "        job_id: ID do job para monitoramento\n",
    "        client_id: ID do cliente\n",
    "        max_wait_time: Tempo máximo de espera em segundos\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status final do processamento\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    status_history = []\n",
    "    \n",
    "    print(f\"🔍 Iniciando monitoramento do Job: {job_id}\")\n",
    "    print(f\"⏱️ Tempo máximo de espera: {max_wait_time}s\")\n",
    "    \n",
    "    while True:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Verificar timeout\n",
    "        if elapsed_time > max_wait_time:\n",
    "            print(f\"⏰ Timeout atingido após {elapsed_time:.1f}s\")\n",
    "            return {\n",
    "                'status': 'TIMEOUT',\n",
    "                'elapsed_time': elapsed_time,\n",
    "                'history': status_history\n",
    "            }\n",
    "        \n",
    "        # Consultar status atual\n",
    "        current_status = check_job_status(job_id, client_id)\n",
    "        \n",
    "        if current_status:\n",
    "            status = current_status.get('status', 'UNKNOWN')\n",
    "            \n",
    "            # Adicionar ao histórico se mudou\n",
    "            if not status_history or status_history[-1]['status'] != status:\n",
    "                status_entry = {\n",
    "                    'status': status,\n",
    "                    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "                    'elapsed_time': elapsed_time\n",
    "                }\n",
    "                status_history.append(status_entry)\n",
    "                print(f\"📊 [{elapsed_time:.1f}s] Status: {status}\")\n",
    "            \n",
    "            # Verificar se terminou\n",
    "            if status in [\"CONCLUIDO_SUCESSO\", \"CONCLUIDO_COM_PENDENCIAS\", \"FALHA_DOCAI\", \"FALHA_MAPEAMENTO\", \"ERRO\"]:\n",
    "                print(f\"✅ Processamento finalizado: {status}\")\n",
    "                return {\n",
    "                    'status': status,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'history': status_history,\n",
    "                    'details': current_status\n",
    "                }\n",
    "        else:\n",
    "            print(f\"⚠️ Erro ao consultar status do job\")\n",
    "        \n",
    "        # Aguardar antes da próxima consulta\n",
    "        time.sleep(10)\n",
    "\n",
    "def visualize_processing_results(data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Cria visualizações dos resultados do processamento.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"⚠️ Nenhum dado para visualizar\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Configurar matplotlib para melhor visualização\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('📊 Análise dos Dados Processados', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribuição por status de validação\n",
    "    status_counts = df['status_validacao'].value_counts()\n",
    "    axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Status de Validação')\n",
    "    \n",
    "    # 2. Top 10 rubricas por valor\n",
    "    top_rubricas = df.groupby('rubrica')['valor'].sum().sort_values(ascending=False).head(10)\n",
    "    axes[0, 1].barh(range(len(top_rubricas)), top_rubricas.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_rubricas)))\n",
    "    axes[0, 1].set_yticklabels(top_rubricas.index)\n",
    "    axes[0, 1].set_title('Top 10 Rubricas por Valor Total')\n",
    "    axes[0, 1].set_xlabel('Valor (R$)')\n",
    "    \n",
    "    # 3. Distribuição de valores\n",
    "    axes[1, 0].hist(df['valor'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribuição de Valores')\n",
    "    axes[1, 0].set_xlabel('Valor (R$)')\n",
    "    axes[1, 0].set_ylabel('Frequência')\n",
    "    \n",
    "    # 4. Funcionários com mais rubricas\n",
    "    func_rubricas = df.groupby('funcionario_nome').size().sort_values(ascending=False).head(10)\n",
    "    axes[1, 1].bar(range(len(func_rubricas)), func_rubricas.values)\n",
    "    axes[1, 1].set_xticks(range(len(func_rubricas)))\n",
    "    axes[1, 1].set_xticklabels(func_rubricas.index, rotation=45, ha='right')\n",
    "    axes[1, 1].set_title('Funcionários com Mais Rubricas')\n",
    "    axes[1, 1].set_ylabel('Número de Rubricas')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estatísticas resumidas\n",
    "    print(\"\\n📈 Estatísticas Resumidas:\")\n",
    "    print(f\"  Total de registros: {len(df)}\")\n",
    "    print(f\"  Funcionários únicos: {df['funcionario_nome'].nunique()}\")\n",
    "    print(f\"  Rubricas únicas: {df['rubrica'].nunique()}\")\n",
    "    print(f\"  Valor total: R$ {df['valor'].sum():,.2f}\")\n",
    "    print(f\"  Valor médio: R$ {df['valor'].mean():.2f}\")\n",
    "\n",
    "# Exemplo de uso das funções de monitoramento\n",
    "# job_id_example = \"12345-abcde-67890\"\n",
    "# resultado_monitoramento = monitor_processing_pipeline(job_id_example)\n",
    "# \n",
    "# if 'dados_validados' in locals():\n",
    "#     visualize_processing_results(dados_validados)"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 🎯 Resumo do Processamento e Próximos Passos\n",
    "\n",
    "### Principais Funcionalidades Demonstradas\n",
    "\n",
    "1. **Upload e Processamento**: Upload de PDF e início do processamento assíncrono\n",
    "2. **Monitoramento**: Acompanhamento do status do job em tempo real\n",
    "3. **Validação**: Validação abrangente dos dados extraídos\n",
    "4. **Armazenamento**: Persistência segura no banco de dados\n",
    "5. **Visualização**: Dashboards e gráficos para análise\n",
    "\n",
    "### Arquitetura do Sistema\n",
    "\n",
    "```\n",
    "📄 PDF Upload → 🔄 Processamento Assíncrono → ✅ Validação → 💾 Armazenamento → 📊 Visualização\n",
    "```\n",
    "\n",
    "### Benefícios\n",
    "\n",
    "- **Automação**: Reduz trabalho manual de digitação\n",
    "- **Precisão**: Validação automática de consistência\n",
    "- **Escalabilidade**: Processamento assíncrono para arquivos grandes\n",
    "- **Rastreabilidade**: Log completo de todas as operações\n",
    "- **Integração**: API compatível com sistemas existentes\n",
    "\n",
    "### Próximos Passos\n",
    "\n",
    "1. **Integração com OCR Real**: Implementar PaddleOCR ou similar\n",
    "2. **Melhorias de UI**: Interface web mais amigável\n",
    "3. **Alertas**: Notificações automáticas de inconsistências\n",
    "4. **Relatórios**: Geração automática de relatórios de auditoria\n",
    "5. **Machine Learning**: Modelos preditivos para detecção de fraudes\n",
    "\n",
    "### Documentação Técnica\n",
    "\n",
    "- **API Endpoints**: Consulte a documentação da API\n",
    "- **Schemas**: Estruturas de dados no arquivo de schemas\n",
    "- **Configuração**: Variáveis de ambiente e configurações\n",
    "- **Logs**: Sistema de logging estruturado\n",
    "\n",
    "---\n",
    "\n",
    "**📅 Última atualização**: Janeiro 2025  \n",
    "**👨‍💻 Desenvolvido por**: Equipe AUDITORIA360  \n",
    "**📧 Suporte**: Consulte a documentação técnica ou entre em contato com a equipe de desenvolvimento"
=======
   "id": "13a8c7db",
   "metadata": {},
   "source": [
    "## 7. Tratamento de Erros e Logs\n",
    "\n",
    "Sistema robusto de tratamento de erros com logs detalhados para debugging e auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/tmp/auditoria_folha.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FolhaProcessingError(Exception):\n",
    "    \"\"\"Exceção específica para erros de processamento de folha.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ErrorHandler:\n",
    "    \"\"\"\n",
    "    Classe para centralizar o tratamento de erros do sistema.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_error(error: Union[Exception, str], context: str = \"\", job_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Registra erro com contexto adicional.\n",
    "        \"\"\"\n",
    "        error_msg = str(error)\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'job_id': job_id,\n",
    "            'context': context,\n",
    "            'error': error_msg,\n",
    "            'error_type': type(error).__name__ if isinstance(error, Exception) else 'Unknown'\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"[{context}] {error_msg}\", extra=log_entry)\n",
    "        return log_entry\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_processing_error(func):\n",
    "        \"\"\"\n",
    "        Decorator para tratamento automático de erros em funções de processamento.\n",
    "        \"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except requests.RequestException as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de conexão em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro de conexão com API', 'details': str(e)}\n",
    "            except FolhaProcessingError as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de processamento em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro no processamento da folha', 'details': str(e)}\n",
    "            except Exception as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro inesperado em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro inesperado', 'details': str(e)}\n",
    "        return wrapper\n",
    "\n",
    "# Exemplo de tratamento de erros integrado\n",
    "print(\"🔧 Sistema de tratamento de erros configurado com sucesso!\")\n",
    "print(\"📝 Logs serão salvos em: /tmp/auditoria_folha.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec157406",
   "metadata": {},
   "source": [
    "## 8. Exemplo de Fluxo Completo\n",
    "\n",
    "Demonstração de um fluxo completo de processamento de folha de pagamento com todas as funcionalidades integradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47baf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_complete_folha_processing_workflow(pdf_file_path: str, client_id: str = \"demo_client\"):\n",
    "    \"\"\"\n",
    "    Executa o fluxo completo de processamento de folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file_path: Caminho para o arquivo PDF\n",
    "        client_id: ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado completo do processamento\n",
    "    \"\"\"\n",
    "    workflow_result = {\n",
    "        'client_id': client_id,\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'steps': [],\n",
    "        'final_status': 'INICIADO'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Passo 1: Upload do PDF para GCS\n",
    "        print(\"📤 Passo 1: Upload do PDF para Google Cloud Storage\")\n",
    "        # gcs_uri = upload_pdf_to_gcs(pdf_file_path)\n",
    "        gcs_uri = f\"gs://{BUCKET_NAME}/uploads/demo_folha.pdf\"  # Simulado\n",
    "        workflow_result['steps'].append({\n",
    "            'step': 1,\n",
    "            'name': 'Upload PDF',\n",
    "            'status': 'SUCESSO',\n",
    "            'gcs_uri': gcs_uri\n",
    "        })\n",
    "        \n",
    "        # Passo 2: Iniciar processamento assíncrono\n",
    "        print(\"🚀 Passo 2: Iniciar processamento assíncrono com Document AI\")\n",
    "        job_id = process_pdf_with_docai(gcs_uri, client_id)\n",
    "        if not job_id:\n",
    "            raise FolhaProcessingError(\"Falha ao iniciar processamento\")\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 2,\n",
    "            'name': 'Iniciar Processamento',\n",
    "            'status': 'SUCESSO',\n",
    "            'job_id': job_id\n",
    "        })\n",
    "        \n",
    "        # Passo 3: Monitorar processamento\n",
    "        print(\"👁️ Passo 3: Monitorar processamento\")\n",
    "        monitoring_result = monitor_processing_pipeline(job_id, client_id, max_wait_time=120)\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 3,\n",
    "            'name': 'Monitoramento',\n",
    "            'status': monitoring_result['status'],\n",
    "            'elapsed_time': monitoring_result['elapsed_time']\n",
    "        })\n",
    "        \n",
    "        # Passo 4: Processamento dos dados (simulado)\n",
    "        if monitoring_result['status'] in ['CONCLUIDO_SUCESSO', 'CONCLUIDO_COM_PENDENCIAS']:\n",
    "            print(\"🔍 Passo 4: Validação e mapeamento de dados\")\n",
    "            \n",
    "            # Dados simulados extraídos do Document AI\n",
    "            dados_extraidos = [\n",
    "                {\"funcionario_nome\": \"João Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"Salário Base\", \"valor\": 5000.00},\n",
    "                {\"funcionario_nome\": \"Maria Santos\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale Alimentação\", \"valor\": 400.00},\n",
    "                {\"funcionario_nome\": \"Pedro Oliveira\", \"cpf\": \"456.789.123-45\", \"rubrica\": \"FGTS\", \"valor\": 320.00},\n",
    "            ]\n",
    "            \n",
    "            dados_validados = validate_folha_data(dados_extraidos)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 4,\n",
    "                'name': 'Validação de Dados',\n",
    "                'status': 'SUCESSO',\n",
    "                'records_processed': len(dados_validados),\n",
    "                'validation_stats': pd.DataFrame(dados_validados)['status_validacao'].value_counts().to_dict()\n",
    "            })\n",
    "            \n",
    "            # Passo 5: Armazenamento no BigQuery\n",
    "            print(\"💾 Passo 5: Armazenamento no BigQuery\")\n",
    "            storage_result = insert_validated_data_to_bigquery(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 5,\n",
    "                'name': 'Armazenamento BigQuery',\n",
    "                'status': 'SUCESSO' if storage_result['success'] else 'ERRO',\n",
    "                'inserted_count': storage_result['inserted_count'],\n",
    "                'error_count': storage_result['error_count']\n",
    "            })\n",
    "            \n",
    "            # Passo 6: Geração de relatório\n",
    "            print(\"📊 Passo 6: Geração de visualizações\")\n",
    "            visualize_processing_results(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 6,\n",
    "                'name': 'Visualizações',\n",
    "                'status': 'SUCESSO'\n",
    "            })\n",
    "            \n",
    "            workflow_result['final_status'] = 'CONCLUIDO_SUCESSO'\n",
    "        else:\n",
    "            workflow_result['final_status'] = 'FALHA_PROCESSAMENTO'\n",
    "            \n",
    "    except Exception as e:\n",
    "        ErrorHandler.log_error(e, \"Fluxo completo de processamento\")\n",
    "        workflow_result['final_status'] = 'ERRO'\n",
    "        workflow_result['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        workflow_result['end_time'] = datetime.now().isoformat()\n",
    "        \n",
    "    # Resumo final\n",
    "    print(f\"\\n🏁 Workflow Finalizado!\")\n",
    "    print(f\"   Status: {workflow_result['final_status']}\")\n",
    "    print(f\"   Passos executados: {len(workflow_result['steps'])}\")\n",
    "    \n",
    "    return workflow_result\n",
    "\n",
    "# Exemplo de execução do fluxo completo\n",
    "print(\"🎯 Demonstração do Fluxo Completo de Processamento\")\n",
    "print(\"   (Utilizando dados simulados para demonstração)\")\n",
    "# resultado_completo = execute_complete_folha_processing_workflow(\"demo_folha.pdf\")\n",
    "print(\"\\n✅ Notebook configurado e pronto para uso!\")\n",
    "print(\"📋 Para executar o fluxo completo, chame: execute_complete_folha_processing_workflow('caminho_do_pdf.pdf')\")"
>>>>>>> Principal
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
