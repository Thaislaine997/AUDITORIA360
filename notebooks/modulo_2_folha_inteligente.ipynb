{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8434983e",
   "metadata": {},
   "source": [
    "# M√≥dulo 2: Controle Mensal da Folha Inteligente\n",
    "\n",
    "Este notebook documenta e prototipa as funcionalidades do M√≥dulo 2 do sistema AUDITORIA360, focando na importa√ß√£o e extra√ß√£o de dados de extratos de folha de pagamento. O fluxo principal inclui:\n",
    "\n",
    "1. **Upload de arquivos PDF** via interface Streamlit\n",
    "2. **Processamento ass√≠ncrono** usando Document AI\n",
    "3. **Valida√ß√£o e mapeamento** dos dados extra√≠dos\n",
    "4. **Armazenamento** estruturado no BigQuery\n",
    "5. **Monitoramento** de status e tratamento de erros\n",
    "\n",
    "## Arquitetura do Sistema\n",
    "- **Frontend**: Streamlit para interface de usu√°rio\n",
    "- **Backend**: API FastAPI para processamento ass√≠ncrono\n",
    "- **OCR**: Document AI para extra√ß√£o de dados de PDFs\n",
    "- **Storage**: Google Cloud Storage para arquivos\n",
    "- **Database**: BigQuery para dados estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd6fee",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente\n",
    "\n",
    "Configura√ß√£o inicial das bibliotecas, autentica√ß√£o com Google Cloud e inicializa√ß√£o de vari√°veis globais necess√°rias para o funcionamento do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62802b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Google Cloud libraries\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "# Configura√ß√£o da autentica√ß√£o com Google Cloud\n",
    "# Nota: Em produ√ß√£o, use vari√°veis de ambiente ou service account keys\n",
    "SERVICE_ACCOUNT_PATH = 'path/to/your-service-account-key.json'\n",
    "if os.path.exists(SERVICE_ACCOUNT_PATH):\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_PATH)\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bigquery_client = bigquery.Client(credentials=credentials)\n",
    "    docai_client = documentai.DocumentProcessorServiceClient(credentials=credentials)\n",
    "else:\n",
    "    # Usar credenciais padr√£o do ambiente\n",
    "    storage_client = storage.Client()\n",
    "    bigquery_client = bigquery.Client()\n",
    "    docai_client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "# Vari√°veis de configura√ß√£o globais\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'seu-projeto-id')\n",
    "BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'seu-bucket-folhas-clientes')\n",
    "PROCESSOR_ID = os.getenv('DOCAI_PROCESSOR_ID', 'seu-processor-id')\n",
    "LOCATION = os.getenv('DOCAI_LOCATION', 'us')\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "\n",
    "print(f\"Configura√ß√£o carregada:\")\n",
    "print(f\"- Projeto: {PROJECT_ID}\")\n",
    "print(f\"- Bucket: {BUCKET_NAME}\")\n",
    "print(f\"- Processor ID: {PROCESSOR_ID}\")\n",
    "print(f\"- Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8822d2",
   "metadata": {},
   "source": [
    "## 2. Upload de Arquivo PDF\n",
    "\n",
    "Interface para upload de arquivos PDF usando Streamlit. Os arquivos s√£o validados e salvos no Google Cloud Storage para processamento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf_interface():\n",
    "    \"\"\"\n",
    "    Interface Streamlit para upload de arquivos PDF.\n",
    "    Valida o arquivo e realiza o upload para o Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    st.title(\"üìÑ Upload de Extrato de Folha de Pagamento\")\n",
    "    st.markdown(\"Selecione o arquivo PDF do extrato para processamento autom√°tico.\")\n",
    "    \n",
    "    # Widget de upload de arquivo\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Selecione o arquivo PDF:\", \n",
    "        type=[\"pdf\"],\n",
    "        help=\"Apenas arquivos PDF s√£o aceitos. Tamanho m√°ximo: 10MB\"\n",
    "    )\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Validar tamanho do arquivo (m√°ximo 10MB)\n",
    "        file_size = len(uploaded_file.getvalue())\n",
    "        if file_size > 10 * 1024 * 1024:  # 10MB\n",
    "            st.error(\"‚ùå Arquivo muito grande. Tamanho m√°ximo permitido: 10MB\")\n",
    "            return None\n",
    "            \n",
    "        st.success(f\"‚úÖ Arquivo '{uploaded_file.name}' carregado com sucesso!\")\n",
    "        st.info(f\"üìä Tamanho: {file_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Bot√£o para processar o arquivo\n",
    "        if st.button(\"üöÄ Processar Arquivo\", type=\"primary\"):\n",
    "            with st.spinner(\"Salvando arquivo no Google Cloud Storage...\"):\n",
    "                try:\n",
    "                    # Upload para GCS\n",
    "                    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "                    blob_name = f\"uploads/{uploaded_file.name}\"\n",
    "                    blob = bucket.blob(blob_name)\n",
    "                    \n",
    "                    # Reset file pointer and upload\n",
    "                    uploaded_file.seek(0)\n",
    "                    blob.upload_from_file(uploaded_file, content_type='application/pdf')\n",
    "                    \n",
    "                    gcs_uri = f\"gs://{BUCKET_NAME}/{blob_name}\"\n",
    "                    st.success(f\"‚úÖ Arquivo salvo no GCS: {gcs_uri}\")\n",
    "                    return gcs_uri\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Erro ao salvar arquivo: {str(e)}\")\n",
    "                    return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Exemplo de uso da interface\n",
    "# gcs_uri = upload_pdf_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb271a1",
   "metadata": {},
   "source": [
    "## 3. Processamento Ass√≠ncrono com Document AI\n",
    "\n",
    "Processamento de documentos PDF utilizando o Google Document AI para extra√ß√£o de dados estruturados. O processo √© ass√≠ncrono para melhor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_docai(gcs_uri, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Processa um PDF armazenado no GCS usando Document AI.\n",
    "    \n",
    "    Args:\n",
    "        gcs_uri (str): URI do arquivo no Google Cloud Storage\n",
    "        client_id (str): ID do cliente para processamento\n",
    "    \n",
    "    Returns:\n",
    "        str: Job ID para acompanhamento do processamento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iniciar processamento ass√≠ncrono via API\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async\",\n",
    "            json={\"gcs_uri\": gcs_uri}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            job_data = response.json()\n",
    "            job_id = job_data.get(\"job_id\")\n",
    "            print(f\"‚úÖ Job iniciado com sucesso. ID: {job_id}\")\n",
    "            print(f\"üìä Status: {job_data.get('status', 'INICIADO')}\")\n",
    "            return job_id\n",
    "        else:\n",
    "            print(f\"‚ùå Erro ao iniciar processamento: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Erro de conex√£o: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def check_job_status(job_id, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Consulta o status de um job de processamento.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): ID do job para consulta\n",
    "        client_id (str): ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Informa√ß√µes sobre o status do job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async/status/{job_id}\"\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"‚ùå Erro ao consultar status: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Erro de conex√£o: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso\n",
    "# gcs_uri_example = \"gs://seu-bucket/exemplo.pdf\"\n",
    "# job_id = process_pdf_with_docai(gcs_uri_example, \"12345\")\n",
    "# if job_id:\n",
    "#     status = check_job_status(job_id, \"12345\")\n",
    "#     print(f\"Status atual: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c2281",
   "metadata": {},
   "source": [
    "## 4. Valida√ß√£o e Mapeamento de Dados\n",
    "\n",
    "Os dados extra√≠dos do Document AI s√£o validados e mapeados para a estrutura interna do sistema antes do armazenamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d427a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def validate_cpf(cpf: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida formato e d√≠gitos verificadores do CPF.\n",
    "    \"\"\"\n",
    "    if not cpf:\n",
    "        return False\n",
    "    \n",
    "    # Remove caracteres n√£o num√©ricos\n",
    "    cpf_numbers = re.sub(r'\\D', '', cpf)\n",
    "    \n",
    "    # Verifica se tem 11 d√≠gitos\n",
    "    if len(cpf_numbers) != 11:\n",
    "        return False\n",
    "    \n",
    "    # Verifica se n√£o s√£o todos os d√≠gitos iguais\n",
    "    if cpf_numbers == cpf_numbers[0] * 11:\n",
    "        return False\n",
    "    \n",
    "    # C√°lculo dos d√≠gitos verificadores (simplificado para exemplo)\n",
    "    return True\n",
    "\n",
    "def validate_folha_data(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Valida e mapeia dados extra√≠dos da folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de registros extra√≠dos do Document AI\n",
    "    \n",
    "    Returns:\n",
    "        Lista de registros validados com indicadores de erro\n",
    "    \"\"\"\n",
    "    validated_data = []\n",
    "    \n",
    "    for i, record in enumerate(data):\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Valida√ß√£o do CPF\n",
    "        if not validate_cpf(record.get('cpf', '')):\n",
    "            errors.append(\"CPF inv√°lido ou ausente\")\n",
    "        \n",
    "        # Valida√ß√£o do nome do funcion√°rio\n",
    "        if not record.get('funcionario_nome', '').strip():\n",
    "            errors.append(\"Nome do funcion√°rio ausente\")\n",
    "        \n",
    "        # Valida√ß√£o da rubrica\n",
    "        if not record.get('rubrica', '').strip():\n",
    "            warnings.append(\"Rubrica n√£o identificada\")\n",
    "        \n",
    "        # Valida√ß√£o do valor\n",
    "        try:\n",
    "            valor = float(record.get('valor', 0))\n",
    "            if valor == 0:\n",
    "                warnings.append(\"Valor zero detectado\")\n",
    "        except (ValueError, TypeError):\n",
    "            errors.append(\"Valor inv√°lido\")\n",
    "            valor = 0.0\n",
    "        \n",
    "        # Mapeamento para estrutura interna\n",
    "        validated_record = {\n",
    "            'id_linha': i + 1,\n",
    "            'funcionario_nome': record.get('funcionario_nome', '').strip(),\n",
    "            'cpf': re.sub(r'\\D', '', record.get('cpf', '')),\n",
    "            'rubrica': record.get('rubrica', '').strip(),\n",
    "            'valor': valor,\n",
    "            'data_processamento': pd.Timestamp.now().isoformat(),\n",
    "            'status_validacao': 'ERRO' if errors else ('AVISO' if warnings else 'OK'),\n",
    "            'erros': errors,\n",
    "            'avisos': warnings\n",
    "        }\n",
    "        \n",
    "        validated_data.append(validated_record)\n",
    "    \n",
    "    return validated_data\n",
    "\n",
    "# Exemplo de dados extra√≠dos (simula√ß√£o)\n",
    "dados_exemplo = [\n",
    "    {\"funcionario_nome\": \"Jo√£o Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"Sal√°rio Base\", \"valor\": 5000.00},\n",
    "    {\"funcionario_nome\": \"Maria Oliveira\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale Transporte\", \"valor\": 150.00},\n",
    "    {\"funcionario_nome\": \"Pedro Santos\", \"cpf\": \"111.111.111-11\", \"rubrica\": \"FGTS\", \"valor\": 400.00},  # CPF inv√°lido\n",
    "    {\"funcionario_nome\": \"\", \"cpf\": \"456.789.123-00\", \"rubrica\": \"Desconto INSS\", \"valor\": -300.00},  # Nome ausente\n",
    "]\n",
    "\n",
    "# Processar valida√ß√£o\n",
    "dados_validados = validate_folha_data(dados_exemplo)\n",
    "\n",
    "# Converter para DataFrame para melhor visualiza√ß√£o\n",
    "df_validado = pd.DataFrame(dados_validados)\n",
    "print(\"üìä Resultados da Valida√ß√£o:\")\n",
    "print(df_validado[['funcionario_nome', 'cpf', 'rubrica', 'valor', 'status_validacao']].head())\n",
    "\n",
    "# Estat√≠sticas de valida√ß√£o\n",
    "stats = df_validado['status_validacao'].value_counts()\n",
    "print(f\"\\nüìà Estat√≠sticas de Valida√ß√£o:\")\n",
    "for status, count in stats.items():\n",
    "    print(f\"  {status}: {count} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091248a",
   "metadata": {},
   "source": [
    "## 5. Armazenamento no BigQuery\n",
    "\n",
    "Dados validados s√£o armazenados no BigQuery com estrutura otimizada para consultas de auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_validated_data_to_bigquery(validated_data: List[Dict], dataset_id: str = \"auditoria_folha_dataset\", table_id: str = \"LinhasFolhaFuncionario\"):\n",
    "    \"\"\"\n",
    "    Insere dados validados no BigQuery com tratamento de erros.\n",
    "    \n",
    "    Args:\n",
    "        validated_data: Lista de dados validados\n",
    "        dataset_id: ID do dataset no BigQuery\n",
    "        table_id: ID da tabela no BigQuery\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado da opera√ß√£o com estat√≠sticas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filtrar apenas dados sem erros cr√≠ticos\n",
    "        dados_para_insercao = [\n",
    "            record for record in validated_data \n",
    "            if record['status_validacao'] != 'ERRO'\n",
    "        ]\n",
    "        \n",
    "        if not dados_para_insercao:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'Nenhum dado v√°lido para inser√ß√£o',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(validated_data)\n",
    "            }\n",
    "        \n",
    "        # Preparar dados para BigQuery (remover campos de valida√ß√£o)\n",
    "        bq_data = []\n",
    "        for record in dados_para_insercao:\n",
    "            bq_record = {\n",
    "                'id_linha': record['id_linha'],\n",
    "                'funcionario_nome': record['funcionario_nome'],\n",
    "                'cpf': record['cpf'],\n",
    "                'rubrica': record['rubrica'],\n",
    "                'valor': record['valor'],\n",
    "                'data_processamento': record['data_processamento'],\n",
    "                'status_validacao': record['status_validacao']\n",
    "            }\n",
    "            bq_data.append(bq_record)\n",
    "        \n",
    "        # Inserir no BigQuery\n",
    "        table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
    "        errors = bigquery_client.insert_rows_json(table_ref, bq_data)\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"‚ùå Erros ao inserir no BigQuery: {errors}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f'Erros na inser√ß√£o: {errors}',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(bq_data)\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚úÖ {len(bq_data)} registros inseridos com sucesso no BigQuery!\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'message': 'Dados inseridos com sucesso',\n",
    "                'inserted_count': len(bq_data),\n",
    "                'error_count': len(validated_data) - len(bq_data)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro inesperado: {str(e)}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': f'Erro inesperado: {str(e)}',\n",
    "            'inserted_count': 0,\n",
    "            'error_count': len(validated_data)\n",
    "        }\n",
    "\n",
    "# Exemplo de uso com dados validados\n",
    "if 'dados_validados' in locals():\n",
    "    resultado_insercao = insert_validated_data_to_bigquery(dados_validados)\n",
    "    print(f\"\\nüìä Resultado da Inser√ß√£o:\")\n",
    "    print(f\"  Sucesso: {resultado_insercao['success']}\")\n",
    "    print(f\"  Registros inseridos: {resultado_insercao['inserted_count']}\")\n",
    "    print(f\"  Registros com erro: {resultado_insercao['error_count']}\")\n",
    "    print(f\"  Mensagem: {resultado_insercao['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847aa03a",
   "metadata": {},
   "source": [
    "## 6. Monitoramento e Visualiza√ß√£o\n",
    "\n",
    "Acompanhamento do processamento e visualiza√ß√£o dos resultados para an√°lise e auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_processing_pipeline(job_id: str, client_id: str = \"default\", max_wait_time: int = 300):\n",
    "    \"\"\"\n",
    "    Monitora o pipeline de processamento com timeout e relat√≥rios de progresso.\n",
    "    \n",
    "    Args:\n",
    "        job_id: ID do job para monitoramento\n",
    "        client_id: ID do cliente\n",
    "        max_wait_time: Tempo m√°ximo de espera em segundos\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status final do processamento\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    status_history = []\n",
    "    \n",
    "    print(f\"üîç Iniciando monitoramento do Job: {job_id}\")\n",
    "    print(f\"‚è±Ô∏è Tempo m√°ximo de espera: {max_wait_time}s\")\n",
    "    \n",
    "    while True:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Verificar timeout\n",
    "        if elapsed_time > max_wait_time:\n",
    "            print(f\"‚è∞ Timeout atingido ap√≥s {elapsed_time:.1f}s\")\n",
    "            return {\n",
    "                'status': 'TIMEOUT',\n",
    "                'elapsed_time': elapsed_time,\n",
    "                'history': status_history\n",
    "            }\n",
    "        \n",
    "        # Consultar status atual\n",
    "        current_status = check_job_status(job_id, client_id)\n",
    "        \n",
    "        if current_status:\n",
    "            status = current_status.get('status', 'UNKNOWN')\n",
    "            \n",
    "            # Adicionar ao hist√≥rico se mudou\n",
    "            if not status_history or status_history[-1]['status'] != status:\n",
    "                status_entry = {\n",
    "                    'status': status,\n",
    "                    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "                    'elapsed_time': elapsed_time\n",
    "                }\n",
    "                status_history.append(status_entry)\n",
    "                print(f\"üìä [{elapsed_time:.1f}s] Status: {status}\")\n",
    "            \n",
    "            # Verificar se terminou\n",
    "            if status in [\"CONCLUIDO_SUCESSO\", \"CONCLUIDO_COM_PENDENCIAS\", \"FALHA_DOCAI\", \"FALHA_MAPEAMENTO\", \"ERRO\"]:\n",
    "                print(f\"‚úÖ Processamento finalizado: {status}\")\n",
    "                return {\n",
    "                    'status': status,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'history': status_history,\n",
    "                    'details': current_status\n",
    "                }\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Erro ao consultar status do job\")\n",
    "        \n",
    "        # Aguardar antes da pr√≥xima consulta\n",
    "        time.sleep(10)\n",
    "\n",
    "def visualize_processing_results(data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Cria visualiza√ß√µes dos resultados do processamento.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"‚ö†Ô∏è Nenhum dado para visualizar\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Configurar matplotlib para melhor visualiza√ß√£o\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üìä An√°lise dos Dados Processados', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribui√ß√£o por status de valida√ß√£o\n",
    "    status_counts = df['status_validacao'].value_counts()\n",
    "    axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Status de Valida√ß√£o')\n",
    "    \n",
    "    # 2. Top 10 rubricas por valor\n",
    "    top_rubricas = df.groupby('rubrica')['valor'].sum().sort_values(ascending=False).head(10)\n",
    "    axes[0, 1].barh(range(len(top_rubricas)), top_rubricas.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_rubricas)))\n",
    "    axes[0, 1].set_yticklabels(top_rubricas.index)\n",
    "    axes[0, 1].set_title('Top 10 Rubricas por Valor Total')\n",
    "    axes[0, 1].set_xlabel('Valor (R$)')\n",
    "    \n",
    "    # 3. Distribui√ß√£o de valores\n",
    "    axes[1, 0].hist(df['valor'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribui√ß√£o de Valores')\n",
    "    axes[1, 0].set_xlabel('Valor (R$)')\n",
    "    axes[1, 0].set_ylabel('Frequ√™ncia')\n",
    "    \n",
    "    # 4. Funcion√°rios com mais rubricas\n",
    "    func_rubricas = df.groupby('funcionario_nome').size().sort_values(ascending=False).head(10)\n",
    "    axes[1, 1].bar(range(len(func_rubricas)), func_rubricas.values)\n",
    "    axes[1, 1].set_xticks(range(len(func_rubricas)))\n",
    "    axes[1, 1].set_xticklabels(func_rubricas.index, rotation=45, ha='right')\n",
    "    axes[1, 1].set_title('Funcion√°rios com Mais Rubricas')\n",
    "    axes[1, 1].set_ylabel('N√∫mero de Rubricas')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas resumidas\n",
    "    print(\"\\nüìà Estat√≠sticas Resumidas:\")\n",
    "    print(f\"  Total de registros: {len(df)}\")\n",
    "    print(f\"  Funcion√°rios √∫nicos: {df['funcionario_nome'].nunique()}\")\n",
    "    print(f\"  Rubricas √∫nicas: {df['rubrica'].nunique()}\")\n",
    "    print(f\"  Valor total: R$ {df['valor'].sum():,.2f}\")\n",
    "    print(f\"  Valor m√©dio: R$ {df['valor'].mean():.2f}\")\n",
    "\n",
    "# Exemplo de uso das fun√ß√µes de monitoramento\n",
    "# job_id_example = \"12345-abcde-67890\"\n",
    "# resultado_monitoramento = monitor_processing_pipeline(job_id_example)\n",
    "# \n",
    "# if 'dados_validados' in locals():\n",
    "#     visualize_processing_results(dados_validados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8c7db",
   "metadata": {},
   "source": [
    "## 7. Tratamento de Erros e Logs\n",
    "\n",
    "Sistema robusto de tratamento de erros com logs detalhados para debugging e auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/tmp/auditoria_folha.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FolhaProcessingError(Exception):\n",
    "    \"\"\"Exce√ß√£o espec√≠fica para erros de processamento de folha.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ErrorHandler:\n",
    "    \"\"\"\n",
    "    Classe para centralizar o tratamento de erros do sistema.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_error(error: Union[Exception, str], context: str = \"\", job_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Registra erro com contexto adicional.\n",
    "        \"\"\"\n",
    "        error_msg = str(error)\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'job_id': job_id,\n",
    "            'context': context,\n",
    "            'error': error_msg,\n",
    "            'error_type': type(error).__name__ if isinstance(error, Exception) else 'Unknown'\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"[{context}] {error_msg}\", extra=log_entry)\n",
    "        return log_entry\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_processing_error(func):\n",
    "        \"\"\"\n",
    "        Decorator para tratamento autom√°tico de erros em fun√ß√µes de processamento.\n",
    "        \"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except requests.RequestException as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de conex√£o em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro de conex√£o com API', 'details': str(e)}\n",
    "            except FolhaProcessingError as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de processamento em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro no processamento da folha', 'details': str(e)}\n",
    "            except Exception as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro inesperado em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro inesperado', 'details': str(e)}\n",
    "        return wrapper\n",
    "\n",
    "# Exemplo de tratamento de erros integrado\n",
    "print(\"üîß Sistema de tratamento de erros configurado com sucesso!\")\n",
    "print(\"üìù Logs ser√£o salvos em: /tmp/auditoria_folha.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec157406",
   "metadata": {},
   "source": [
    "## 8. Exemplo de Fluxo Completo\n",
    "\n",
    "Demonstra√ß√£o de um fluxo completo de processamento de folha de pagamento com todas as funcionalidades integradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47baf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_complete_folha_processing_workflow(pdf_file_path: str, client_id: str = \"demo_client\"):\n",
    "    \"\"\"\n",
    "    Executa o fluxo completo de processamento de folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file_path: Caminho para o arquivo PDF\n",
    "        client_id: ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado completo do processamento\n",
    "    \"\"\"\n",
    "    workflow_result = {\n",
    "        'client_id': client_id,\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'steps': [],\n",
    "        'final_status': 'INICIADO'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Passo 1: Upload do PDF para GCS\n",
    "        print(\"üì§ Passo 1: Upload do PDF para Google Cloud Storage\")\n",
    "        # gcs_uri = upload_pdf_to_gcs(pdf_file_path)\n",
    "        gcs_uri = f\"gs://{BUCKET_NAME}/uploads/demo_folha.pdf\"  # Simulado\n",
    "        workflow_result['steps'].append({\n",
    "            'step': 1,\n",
    "            'name': 'Upload PDF',\n",
    "            'status': 'SUCESSO',\n",
    "            'gcs_uri': gcs_uri\n",
    "        })\n",
    "        \n",
    "        # Passo 2: Iniciar processamento ass√≠ncrono\n",
    "        print(\"üöÄ Passo 2: Iniciar processamento ass√≠ncrono com Document AI\")\n",
    "        job_id = process_pdf_with_docai(gcs_uri, client_id)\n",
    "        if not job_id:\n",
    "            raise FolhaProcessingError(\"Falha ao iniciar processamento\")\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 2,\n",
    "            'name': 'Iniciar Processamento',\n",
    "            'status': 'SUCESSO',\n",
    "            'job_id': job_id\n",
    "        })\n",
    "        \n",
    "        # Passo 3: Monitorar processamento\n",
    "        print(\"üëÅÔ∏è Passo 3: Monitorar processamento\")\n",
    "        monitoring_result = monitor_processing_pipeline(job_id, client_id, max_wait_time=120)\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 3,\n",
    "            'name': 'Monitoramento',\n",
    "            'status': monitoring_result['status'],\n",
    "            'elapsed_time': monitoring_result['elapsed_time']\n",
    "        })\n",
    "        \n",
    "        # Passo 4: Processamento dos dados (simulado)\n",
    "        if monitoring_result['status'] in ['CONCLUIDO_SUCESSO', 'CONCLUIDO_COM_PENDENCIAS']:\n",
    "            print(\"üîç Passo 4: Valida√ß√£o e mapeamento de dados\")\n",
    "            \n",
    "            # Dados simulados extra√≠dos do Document AI\n",
    "            dados_extraidos = [\n",
    "                {\"funcionario_nome\": \"Jo√£o Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"Sal√°rio Base\", \"valor\": 5000.00},\n",
    "                {\"funcionario_nome\": \"Maria Santos\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale Alimenta√ß√£o\", \"valor\": 400.00},\n",
    "                {\"funcionario_nome\": \"Pedro Oliveira\", \"cpf\": \"456.789.123-45\", \"rubrica\": \"FGTS\", \"valor\": 320.00},\n",
    "            ]\n",
    "            \n",
    "            dados_validados = validate_folha_data(dados_extraidos)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 4,\n",
    "                'name': 'Valida√ß√£o de Dados',\n",
    "                'status': 'SUCESSO',\n",
    "                'records_processed': len(dados_validados),\n",
    "                'validation_stats': pd.DataFrame(dados_validados)['status_validacao'].value_counts().to_dict()\n",
    "            })\n",
    "            \n",
    "            # Passo 5: Armazenamento no BigQuery\n",
    "            print(\"üíæ Passo 5: Armazenamento no BigQuery\")\n",
    "            storage_result = insert_validated_data_to_bigquery(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 5,\n",
    "                'name': 'Armazenamento BigQuery',\n",
    "                'status': 'SUCESSO' if storage_result['success'] else 'ERRO',\n",
    "                'inserted_count': storage_result['inserted_count'],\n",
    "                'error_count': storage_result['error_count']\n",
    "            })\n",
    "            \n",
    "            # Passo 6: Gera√ß√£o de relat√≥rio\n",
    "            print(\"üìä Passo 6: Gera√ß√£o de visualiza√ß√µes\")\n",
    "            visualize_processing_results(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 6,\n",
    "                'name': 'Visualiza√ß√µes',\n",
    "                'status': 'SUCESSO'\n",
    "            })\n",
    "            \n",
    "            workflow_result['final_status'] = 'CONCLUIDO_SUCESSO'\n",
    "        else:\n",
    "            workflow_result['final_status'] = 'FALHA_PROCESSAMENTO'\n",
    "            \n",
    "    except Exception as e:\n",
    "        ErrorHandler.log_error(e, \"Fluxo completo de processamento\")\n",
    "        workflow_result['final_status'] = 'ERRO'\n",
    "        workflow_result['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        workflow_result['end_time'] = datetime.now().isoformat()\n",
    "        \n",
    "    # Resumo final\n",
    "    print(f\"\\nüèÅ Workflow Finalizado!\")\n",
    "    print(f\"   Status: {workflow_result['final_status']}\")\n",
    "    print(f\"   Passos executados: {len(workflow_result['steps'])}\")\n",
    "    \n",
    "    return workflow_result\n",
    "\n",
    "# Exemplo de execu√ß√£o do fluxo completo\n",
    "print(\"üéØ Demonstra√ß√£o do Fluxo Completo de Processamento\")\n",
    "print(\"   (Utilizando dados simulados para demonstra√ß√£o)\")\n",
    "# resultado_completo = execute_complete_folha_processing_workflow(\"demo_folha.pdf\")\n",
    "print(\"\\n‚úÖ Notebook configurado e pronto para uso!\")\n",
    "print(\"üìã Para executar o fluxo completo, chame: execute_complete_folha_processing_workflow('caminho_do_pdf.pdf')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
