{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "main-header",
   "metadata": {},
   "source": [
    "# MÃ³dulo 2: Controle Mensal da Folha Inteligente - AUDITORIA360\n",
    "\n",
    "## ðŸ“‹ VisÃ£o Geral\n",
    "\n",
    "Este notebook documenta e demonstra as funcionalidades do MÃ³dulo 2, incluindo:\n",
    "- ImportaÃ§Ã£o de extratos de folha de pagamento em formato PDF\n",
    "- Processamento assÃ­ncrono usando OCR (substituto do Document AI)\n",
    "- ValidaÃ§Ã£o e mapeamento de dados\n",
    "- Armazenamento estruturado no banco de dados\n",
    "\n",
    "## ðŸŽ¯ Objetivos\n",
    "\n",
    "- Automatizar a extraÃ§Ã£o de dados de PDFs de folha de pagamento\n",
    "- Implementar processamento assÃ­ncrono para arquivos grandes\n",
    "- Validar e mapear dados extraÃ­dos para estrutura interna\n",
    "- Monitorar e reportar status de processamento\n",
    "\n",
    "## ðŸ“š PrÃ©-requisitos\n",
    "\n",
    "- Python 3.8+\n",
    "- Bibliotecas: requests, pandas, streamlit, paddleocr\n",
    "- API AUDITORIA360 em execuÃ§Ã£o\n",
    "- Arquivos PDF de folha de pagamento\n",
    "\n",
    "## ðŸš€ Como Usar\n",
    "\n",
    "1. Configure as variÃ¡veis de ambiente\n",
    "2. Execute as cÃ©lulas sequencialmente\n",
    "3. FaÃ§a upload do arquivo PDF\n",
    "4. Monitore o processamento\n",
    "5. Visualize os resultados"
=======
   "id": "8434983e",
   "metadata": {},
   "source": [
    "# MÃ³dulo 2: Controle Mensal da Folha Inteligente\n",
    "\n",
    "Este notebook documenta e prototipa as funcionalidades do MÃ³dulo 2 do sistema AUDITORIA360, focando na importaÃ§Ã£o e extraÃ§Ã£o de dados de extratos de folha de pagamento. O fluxo principal inclui:\n",
    "\n",
    "1. **Upload de arquivos PDF** via interface Streamlit\n",
    "2. **Processamento assÃ­ncrono** usando Document AI\n",
    "3. **ValidaÃ§Ã£o e mapeamento** dos dados extraÃ­dos\n",
    "4. **Armazenamento** estruturado no BigQuery\n",
    "5. **Monitoramento** de status e tratamento de erros\n",
    "\n",
    "## Arquitetura do Sistema\n",
    "- **Frontend**: Streamlit para interface de usuÃ¡rio\n",
    "- **Backend**: API FastAPI para processamento assÃ­ncrono\n",
    "- **OCR**: Document AI para extraÃ§Ã£o de dados de PDFs\n",
    "- **Storage**: Google Cloud Storage para arquivos\n",
    "- **Database**: BigQuery para dados estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd6fee",
   "metadata": {},
   "source": [
    "## 1. ConfiguraÃ§Ã£o do Ambiente\n",
    "\n",
    "ConfiguraÃ§Ã£o inicial das bibliotecas, autenticaÃ§Ã£o com Google Cloud e inicializaÃ§Ã£o de variÃ¡veis globais necessÃ¡rias para o funcionamento do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62802b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportaÃ§Ã£o das bibliotecas necessÃ¡rias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Google Cloud libraries\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "# ConfiguraÃ§Ã£o da autenticaÃ§Ã£o com Google Cloud\n",
    "# Nota: Em produÃ§Ã£o, use variÃ¡veis de ambiente ou service account keys\n",
    "SERVICE_ACCOUNT_PATH = 'path/to/your-service-account-key.json'\n",
    "if os.path.exists(SERVICE_ACCOUNT_PATH):\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_PATH)\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bigquery_client = bigquery.Client(credentials=credentials)\n",
    "    docai_client = documentai.DocumentProcessorServiceClient(credentials=credentials)\n",
    "else:\n",
    "    # Usar credenciais padrÃ£o do ambiente\n",
    "    storage_client = storage.Client()\n",
    "    bigquery_client = bigquery.Client()\n",
    "    docai_client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "# VariÃ¡veis de configuraÃ§Ã£o globais\n",
    "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'seu-projeto-id')\n",
    "BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'seu-bucket-folhas-clientes')\n",
    "PROCESSOR_ID = os.getenv('DOCAI_PROCESSOR_ID', 'seu-processor-id')\n",
    "LOCATION = os.getenv('DOCAI_LOCATION', 'us')\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "\n",
    "print(f\"ConfiguraÃ§Ã£o carregada:\")\n",
    "print(f\"- Projeto: {PROJECT_ID}\")\n",
    "print(f\"- Bucket: {BUCKET_NAME}\")\n",
    "print(f\"- Processor ID: {PROCESSOR_ID}\")\n",
    "print(f\"- Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8822d2",
   "metadata": {},
   "source": [
    "## 2. Upload de Arquivo PDF\n",
    "\n",
    "Interface para upload de arquivos PDF usando Streamlit. Os arquivos sÃ£o validados e salvos no Google Cloud Storage para processamento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf_interface():\n",
    "    \"\"\"\n",
    "    Interface Streamlit para upload de arquivos PDF.\n",
    "    Valida o arquivo e realiza o upload para o Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    st.title(\"ðŸ“„ Upload de Extrato de Folha de Pagamento\")\n",
    "    st.markdown(\"Selecione o arquivo PDF do extrato para processamento automÃ¡tico.\")\n",
    "    \n",
    "    # Widget de upload de arquivo\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Selecione o arquivo PDF:\", \n",
    "        type=[\"pdf\"],\n",
    "        help=\"Apenas arquivos PDF sÃ£o aceitos. Tamanho mÃ¡ximo: 10MB\"\n",
    "    )\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Validar tamanho do arquivo (mÃ¡ximo 10MB)\n",
    "        file_size = len(uploaded_file.getvalue())\n",
    "        if file_size > 10 * 1024 * 1024:  # 10MB\n",
    "            st.error(\"âŒ Arquivo muito grande. Tamanho mÃ¡ximo permitido: 10MB\")\n",
    "            return None\n",
    "            \n",
    "        st.success(f\"âœ… Arquivo '{uploaded_file.name}' carregado com sucesso!\")\n",
    "        st.info(f\"ðŸ“Š Tamanho: {file_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # BotÃ£o para processar o arquivo\n",
    "        if st.button(\"ðŸš€ Processar Arquivo\", type=\"primary\"):\n",
    "            with st.spinner(\"Salvando arquivo no Google Cloud Storage...\"):\n",
    "                try:\n",
    "                    # Upload para GCS\n",
    "                    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "                    blob_name = f\"uploads/{uploaded_file.name}\"\n",
    "                    blob = bucket.blob(blob_name)\n",
    "                    \n",
    "                    # Reset file pointer and upload\n",
    "                    uploaded_file.seek(0)\n",
    "                    blob.upload_from_file(uploaded_file, content_type='application/pdf')\n",
    "                    \n",
    "                    gcs_uri = f\"gs://{BUCKET_NAME}/{blob_name}\"\n",
    "                    st.success(f\"âœ… Arquivo salvo no GCS: {gcs_uri}\")\n",
    "                    return gcs_uri\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"âŒ Erro ao salvar arquivo: {str(e)}\")\n",
    "                    return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Exemplo de uso da interface\n",
    "# gcs_uri = upload_pdf_interface()"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "config-section",
   "metadata": {},
   "source": [
    "## ðŸ”§ ConfiguraÃ§Ã£o do Ambiente\n",
    "\n",
    "ConfiguraÃ§Ã£o inicial das bibliotecas, variÃ¡veis de ambiente e autenticaÃ§Ã£o."
=======
   "id": "ffb271a1",
   "metadata": {},
   "source": [
    "## 3. Processamento AssÃ­ncrono com Document AI\n",
    "\n",
    "Processamento de documentos PDF utilizando o Google Document AI para extraÃ§Ã£o de dados estruturados. O processo Ã© assÃ­ncrono para melhor performance."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportaÃ§Ãµes necessÃ¡rias\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suprimir warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraÃ§Ãµes da API\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "CLIENT_ID = os.getenv('CLIENT_ID', '12345')\n",
    "\n",
    "# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… Ambiente configurado com sucesso!\")\n",
    "print(f\"ðŸŒ API Base URL: {API_BASE_URL}\")\n",
    "print(f\"ðŸ†” Client ID: {CLIENT_ID}\")"
=======
   "id": "4b9b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_docai(gcs_uri, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Processa um PDF armazenado no GCS usando Document AI.\n",
    "    \n",
    "    Args:\n",
    "        gcs_uri (str): URI do arquivo no Google Cloud Storage\n",
    "        client_id (str): ID do cliente para processamento\n",
    "    \n",
    "    Returns:\n",
    "        str: Job ID para acompanhamento do processamento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iniciar processamento assÃ­ncrono via API\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async\",\n",
    "            json={\"gcs_uri\": gcs_uri}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            job_data = response.json()\n",
    "            job_id = job_data.get(\"job_id\")\n",
    "            print(f\"âœ… Job iniciado com sucesso. ID: {job_id}\")\n",
    "            print(f\"ðŸ“Š Status: {job_data.get('status', 'INICIADO')}\")\n",
    "            return job_id\n",
    "        else:\n",
    "            print(f\"âŒ Erro ao iniciar processamento: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"âŒ Erro de conexÃ£o: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def check_job_status(job_id, client_id=\"default\"):\n",
    "    \"\"\"\n",
    "    Consulta o status de um job de processamento.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): ID do job para consulta\n",
    "        client_id (str): ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: InformaÃ§Ãµes sobre o status do job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async/status/{job_id}\"\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"âŒ Erro ao consultar status: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"âŒ Erro de conexÃ£o: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso\n",
    "# gcs_uri_example = \"gs://seu-bucket/exemplo.pdf\"\n",
    "# job_id = process_pdf_with_docai(gcs_uri_example, \"12345\")\n",
    "# if job_id:\n",
    "#     status = check_job_status(job_id, \"12345\")\n",
    "#     print(f\"Status atual: {status}\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "upload-section",
   "metadata": {},
   "source": [
    "## ðŸ“„ Upload e Processamento de PDF\n",
    "\n",
    "Interface para upload de arquivos PDF e inÃ­cio do processamento assÃ­ncrono."
=======
   "id": "389c2281",
   "metadata": {},
   "source": [
    "## 4. ValidaÃ§Ã£o e Mapeamento de Dados\n",
    "\n",
    "Os dados extraÃ­dos do Document AI sÃ£o validados e mapeados para a estrutura interna do sistema antes do armazenamento."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "pdf-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_process_pdf(pdf_file_path, client_id=CLIENT_ID):\n",
    "    \"\"\"\n",
    "    Faz upload do arquivo PDF e inicia o processamento assÃ­ncrono.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file_path (str): Caminho para o arquivo PDF\n",
    "        client_id (str): ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resposta da API com job_id se bem-sucedido\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar se o arquivo existe\n",
    "        if not os.path.exists(pdf_file_path):\n",
    "            print(f\"âŒ Arquivo nÃ£o encontrado: {pdf_file_path}\")\n",
    "            print(\"ðŸ“ Criando arquivo de exemplo para demonstraÃ§Ã£o...\")\n",
    "            \n",
    "            # Criar dados de exemplo em formato JSON para simular PDF processado\n",
    "            sample_data = {\n",
    "                \"funcionarios\": [\n",
    "                    {\n",
    "                        \"nome\": \"JoÃ£o Silva Santos\",\n",
    "                        \"cpf\": \"123.456.789-00\",\n",
    "                        \"cargo\": \"Analista\",\n",
    "                        \"salario_base\": 5000.00,\n",
    "                        \"horas_extras\": 300.00,\n",
    "                        \"descontos\": 850.00,\n",
    "                        \"salario_liquido\": 4450.00\n",
    "                    },\n",
    "                    {\n",
    "                        \"nome\": \"Maria Oliveira Costa\",\n",
    "                        \"cpf\": \"987.654.321-00\",\n",
    "                        \"cargo\": \"Coordenadora\",\n",
    "                        \"salario_base\": 7500.00,\n",
    "                        \"horas_extras\": 500.00,\n",
    "                        \"descontos\": 1200.00,\n",
    "                        \"salario_liquido\": 6800.00\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Simular processamento bem-sucedido\n",
    "            job_id = f\"job_{int(time.time())}\"\n",
    "            print(f\"âœ… Processamento simulado iniciado com sucesso!\")\n",
    "            print(f\"ðŸ†” Job ID: {job_id}\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"job_id\": job_id,\n",
    "                \"message\": \"Processamento iniciado com sucesso\",\n",
    "                \"data\": sample_data\n",
    "            }\n",
    "        \n",
    "        # Processar arquivo real\n",
    "        with open(pdf_file_path, 'rb') as pdf_file:\n",
    "            response = requests.post(\n",
    "                f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async\",\n",
    "                files={\"file\": pdf_file},\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            job_id = result.get(\"job_id\")\n",
    "            print(f\"âœ… Job iniciado com sucesso!\")\n",
    "            print(f\"ðŸ†” Job ID: {job_id}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"âŒ Erro ao iniciar o job: {response.text}\")\n",
    "            return {\"status\": \"error\", \"message\": response.text}\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Erro de conexÃ£o: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro inesperado: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Exemplo de uso\n",
    "PDF_FILE_PATH = \"sample_extrato.pdf\"\n",
    "result = upload_and_process_pdf(PDF_FILE_PATH)\n",
    "\n",
    "# Armazenar job_id para uso posterior\n",
    "if result.get(\"status\") == \"success\":\n",
    "    CURRENT_JOB_ID = result.get(\"job_id\")\n",
    "    print(f\"\\nðŸ“‹ Job ID armazenado: {CURRENT_JOB_ID}\")\n",
    "else:\n",
    "    CURRENT_JOB_ID = None"
=======
   "id": "4d427a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def validate_cpf(cpf: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida formato e dÃ­gitos verificadores do CPF.\n",
    "    \"\"\"\n",
    "    if not cpf:\n",
    "        return False\n",
    "    \n",
    "    # Remove caracteres nÃ£o numÃ©ricos\n",
    "    cpf_numbers = re.sub(r'\\D', '', cpf)\n",
    "    \n",
    "    # Verifica se tem 11 dÃ­gitos\n",
    "    if len(cpf_numbers) != 11:\n",
    "        return False\n",
    "    \n",
    "    # Verifica se nÃ£o sÃ£o todos os dÃ­gitos iguais\n",
    "    if cpf_numbers == cpf_numbers[0] * 11:\n",
    "        return False\n",
    "    \n",
    "    # CÃ¡lculo dos dÃ­gitos verificadores (simplificado para exemplo)\n",
    "    return True\n",
    "\n",
    "def validate_folha_data(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Valida e mapeia dados extraÃ­dos da folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        data: Lista de registros extraÃ­dos do Document AI\n",
    "    \n",
    "    Returns:\n",
    "        Lista de registros validados com indicadores de erro\n",
    "    \"\"\"\n",
    "    validated_data = []\n",
    "    \n",
    "    for i, record in enumerate(data):\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # ValidaÃ§Ã£o do CPF\n",
    "        if not validate_cpf(record.get('cpf', '')):\n",
    "            errors.append(\"CPF invÃ¡lido ou ausente\")\n",
    "        \n",
    "        # ValidaÃ§Ã£o do nome do funcionÃ¡rio\n",
    "        if not record.get('funcionario_nome', '').strip():\n",
    "            errors.append(\"Nome do funcionÃ¡rio ausente\")\n",
    "        \n",
    "        # ValidaÃ§Ã£o da rubrica\n",
    "        if not record.get('rubrica', '').strip():\n",
    "            warnings.append(\"Rubrica nÃ£o identificada\")\n",
    "        \n",
    "        # ValidaÃ§Ã£o do valor\n",
    "        try:\n",
    "            valor = float(record.get('valor', 0))\n",
    "            if valor == 0:\n",
    "                warnings.append(\"Valor zero detectado\")\n",
    "        except (ValueError, TypeError):\n",
    "            errors.append(\"Valor invÃ¡lido\")\n",
    "            valor = 0.0\n",
    "        \n",
    "        # Mapeamento para estrutura interna\n",
    "        validated_record = {\n",
    "            'id_linha': i + 1,\n",
    "            'funcionario_nome': record.get('funcionario_nome', '').strip(),\n",
    "            'cpf': re.sub(r'\\D', '', record.get('cpf', '')),\n",
    "            'rubrica': record.get('rubrica', '').strip(),\n",
    "            'valor': valor,\n",
    "            'data_processamento': pd.Timestamp.now().isoformat(),\n",
    "            'status_validacao': 'ERRO' if errors else ('AVISO' if warnings else 'OK'),\n",
    "            'erros': errors,\n",
    "            'avisos': warnings\n",
    "        }\n",
    "        \n",
    "        validated_data.append(validated_record)\n",
    "    \n",
    "    return validated_data\n",
    "\n",
    "# Exemplo de dados extraÃ­dos (simulaÃ§Ã£o)\n",
    "dados_exemplo = [\n",
    "    {\"funcionario_nome\": \"JoÃ£o Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"SalÃ¡rio Base\", \"valor\": 5000.00},\n",
    "    {\"funcionario_nome\": \"Maria Oliveira\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale Transporte\", \"valor\": 150.00},\n",
    "    {\"funcionario_nome\": \"Pedro Santos\", \"cpf\": \"111.111.111-11\", \"rubrica\": \"FGTS\", \"valor\": 400.00},  # CPF invÃ¡lido\n",
    "    {\"funcionario_nome\": \"\", \"cpf\": \"456.789.123-00\", \"rubrica\": \"Desconto INSS\", \"valor\": -300.00},  # Nome ausente\n",
    "]\n",
    "\n",
    "# Processar validaÃ§Ã£o\n",
    "dados_validados = validate_folha_data(dados_exemplo)\n",
    "\n",
    "# Converter para DataFrame para melhor visualizaÃ§Ã£o\n",
    "df_validado = pd.DataFrame(dados_validados)\n",
    "print(\"ðŸ“Š Resultados da ValidaÃ§Ã£o:\")\n",
    "print(df_validado[['funcionario_nome', 'cpf', 'rubrica', 'valor', 'status_validacao']].head())\n",
    "\n",
    "# EstatÃ­sticas de validaÃ§Ã£o\n",
    "stats = df_validado['status_validacao'].value_counts()\n",
    "print(f\"\\nðŸ“ˆ EstatÃ­sticas de ValidaÃ§Ã£o:\")\n",
    "for status, count in stats.items():\n",
    "    print(f\"  {status}: {count} registros\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "## ðŸ” Monitoramento do Processamento\n",
    "\n",
    "FunÃ§Ãµes para consultar o status do job e monitorar o progresso do processamento."
=======
   "id": "4091248a",
   "metadata": {},
   "source": [
    "## 5. Armazenamento no BigQuery\n",
    "\n",
    "Dados validados sÃ£o armazenados no BigQuery com estrutura otimizada para consultas de auditoria."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "job-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_job_status(job_id, client_id=CLIENT_ID, max_attempts=20):\n",
    "    \"\"\"\n",
    "    Consulta o status de um job de processamento.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): ID do job\n",
    "        client_id (str): ID do cliente\n",
    "        max_attempts (int): NÃºmero mÃ¡ximo de tentativas\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status final do job\n",
    "    \"\"\"\n",
    "    if not job_id:\n",
    "        print(\"âš ï¸  Job ID nÃ£o fornecido. Simulando status...\")\n",
    "        return {\n",
    "            \"status\": \"CONCLUIDO_SUCESSO\",\n",
    "            \"message\": \"Processamento simulado concluÃ­do com sucesso\",\n",
    "            \"progress\": 100\n",
    "        }\n",
    "    \n",
    "    print(f\"ðŸ” Monitorando job: {job_id}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{API_BASE_URL}/api/v1/clientes/{client_id}/folhas/importar-pdf-async/status/{job_id}\",\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                job_data = response.json()\n",
    "                status = job_data.get(\"status\")\n",
    "                progress = job_data.get(\"progress\", 0)\n",
    "                \n",
    "                print(f\"ðŸ“Š Tentativa {attempt + 1}: Status = {status} ({progress}%)\")\n",
    "                \n",
    "                # Status finais\n",
    "                if status in [\"CONCLUIDO_SUCESSO\", \"CONCLUIDO_COM_PENDENCIAS\", \"FALHA_DOCAI\", \"FALHA_MAPEAMENTO\"]:\n",
    "                    if status == \"CONCLUIDO_SUCESSO\":\n",
    "                        print(\"âœ… Processamento concluÃ­do com sucesso!\")\n",
    "                    elif status == \"CONCLUIDO_COM_PENDENCIAS\":\n",
    "                        print(\"âš ï¸  Processamento concluÃ­do com pendÃªncias\")\n",
    "                    else:\n",
    "                        print(f\"âŒ Falha no processamento: {status}\")\n",
    "                    \n",
    "                    return job_data\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ Erro na consulta (tentativa {attempt + 1}): {response.text}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âš ï¸  Erro de conexÃ£o (tentativa {attempt + 1}): {str(e)}\")\n",
    "        \n",
    "        attempt += 1\n",
    "        if attempt < max_attempts:\n",
    "            print(\"â³ Aguardando 5 segundos...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    print(f\"â° Timeout: MÃ¡ximo de tentativas ({max_attempts}) atingido\")\n",
    "    return {\"status\": \"TIMEOUT\", \"message\": \"Timeout na consulta do status\"}\n",
    "\n",
    "# Monitorar o job atual\n",
    "if CURRENT_JOB_ID:\n",
    "    final_status = check_job_status(CURRENT_JOB_ID)\n",
    "    print(f\"\\nðŸŽ¯ Status final: {final_status}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Nenhum job ativo para monitorar\")\n",
    "    final_status = check_job_status(None)  # Simular status"
=======
   "id": "c18a1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_validated_data_to_bigquery(validated_data: List[Dict], dataset_id: str = \"auditoria_folha_dataset\", table_id: str = \"LinhasFolhaFuncionario\"):\n",
    "    \"\"\"\n",
    "    Insere dados validados no BigQuery com tratamento de erros.\n",
    "    \n",
    "    Args:\n",
    "        validated_data: Lista de dados validados\n",
    "        dataset_id: ID do dataset no BigQuery\n",
    "        table_id: ID da tabela no BigQuery\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado da operaÃ§Ã£o com estatÃ­sticas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filtrar apenas dados sem erros crÃ­ticos\n",
    "        dados_para_insercao = [\n",
    "            record for record in validated_data \n",
    "            if record['status_validacao'] != 'ERRO'\n",
    "        ]\n",
    "        \n",
    "        if not dados_para_insercao:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'Nenhum dado vÃ¡lido para inserÃ§Ã£o',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(validated_data)\n",
    "            }\n",
    "        \n",
    "        # Preparar dados para BigQuery (remover campos de validaÃ§Ã£o)\n",
    "        bq_data = []\n",
    "        for record in dados_para_insercao:\n",
    "            bq_record = {\n",
    "                'id_linha': record['id_linha'],\n",
    "                'funcionario_nome': record['funcionario_nome'],\n",
    "                'cpf': record['cpf'],\n",
    "                'rubrica': record['rubrica'],\n",
    "                'valor': record['valor'],\n",
    "                'data_processamento': record['data_processamento'],\n",
    "                'status_validacao': record['status_validacao']\n",
    "            }\n",
    "            bq_data.append(bq_record)\n",
    "        \n",
    "        # Inserir no BigQuery\n",
    "        table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
    "        errors = bigquery_client.insert_rows_json(table_ref, bq_data)\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"âŒ Erros ao inserir no BigQuery: {errors}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f'Erros na inserÃ§Ã£o: {errors}',\n",
    "                'inserted_count': 0,\n",
    "                'error_count': len(bq_data)\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âœ… {len(bq_data)} registros inseridos com sucesso no BigQuery!\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'message': 'Dados inseridos com sucesso',\n",
    "                'inserted_count': len(bq_data),\n",
    "                'error_count': len(validated_data) - len(bq_data)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro inesperado: {str(e)}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': f'Erro inesperado: {str(e)}',\n",
    "            'inserted_count': 0,\n",
    "            'error_count': len(validated_data)\n",
    "        }\n",
    "\n",
    "# Exemplo de uso com dados validados\n",
    "if 'dados_validados' in locals():\n",
    "    resultado_insercao = insert_validated_data_to_bigquery(dados_validados)\n",
    "    print(f\"\\nðŸ“Š Resultado da InserÃ§Ã£o:\")\n",
    "    print(f\"  Sucesso: {resultado_insercao['success']}\")\n",
    "    print(f\"  Registros inseridos: {resultado_insercao['inserted_count']}\")\n",
    "    print(f\"  Registros com erro: {resultado_insercao['error_count']}\")\n",
    "    print(f\"  Mensagem: {resultado_insercao['message']}\")"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "validation-section",
   "metadata": {},
   "source": [
    "## âœ… ValidaÃ§Ã£o e Mapeamento de Dados\n",
    "\n",
    "Processamento dos dados extraÃ­dos, validaÃ§Ã£o e mapeamento para a estrutura interna do sistema."
=======
   "id": "847aa03a",
   "metadata": {},
   "source": [
    "## 6. Monitoramento e VisualizaÃ§Ã£o\n",
    "\n",
    "Acompanhamento do processamento e visualizaÃ§Ã£o dos resultados para anÃ¡lise e auditoria."
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "data-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_map_data(extracted_data):\n",
    "    \"\"\"\n",
    "    Valida e mapeia os dados extraÃ­dos do PDF.\n",
    "    \n",
    "    Args:\n",
    "        extracted_data (dict): Dados extraÃ­dos do processamento\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dados validados e mapeados\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” VALIDAÃ‡ÃƒO E MAPEAMENTO DE DADOS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Obter dados de exemplo ou dados reais\n",
    "    if not extracted_data or not extracted_data.get(\"funcionarios\"):\n",
    "        print(\"ðŸ“ Usando dados de exemplo para demonstraÃ§Ã£o...\")\n",
    "        funcionarios_data = [\n",
    "            {\n",
    "                \"nome\": \"JoÃ£o Silva Santos\",\n",
    "                \"cpf\": \"123.456.789-00\",\n",
    "                \"cargo\": \"Analista\",\n",
    "                \"salario_base\": 5000.00,\n",
    "                \"horas_extras\": 300.00,\n",
    "                \"descontos\": 850.00,\n",
    "                \"salario_liquido\": 4450.00\n",
    "            },\n",
    "            {\n",
    "                \"nome\": \"Maria Oliveira Costa\",\n",
    "                \"cpf\": \"987.654.321-00\",\n",
    "                \"cargo\": \"Coordenadora\",\n",
    "                \"salario_base\": 7500.00,\n",
    "                \"horas_extras\": 500.00,\n",
    "                \"descontos\": 1200.00,\n",
    "                \"salario_liquido\": 6800.00\n",
    "            },\n",
    "            {\n",
    "                \"nome\": \"Pedro Santos Lima\",\n",
    "                \"cpf\": \"456.789.123-00\",\n",
    "                \"cargo\": \"Gerente\",\n",
    "                \"salario_base\": 12000.00,\n",
    "                \"horas_extras\": 0.00,\n",
    "                \"descontos\": 2100.00,\n",
    "                \"salario_liquido\": 9900.00\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        funcionarios_data = extracted_data[\"funcionarios\"]\n",
    "    \n",
    "    # Criar DataFrame para validaÃ§Ã£o\n",
    "    df = pd.DataFrame(funcionarios_data)\n",
    "    \n",
    "    print(f\"ðŸ“Š Total de registros para validar: {len(df)}\")\n",
    "    \n",
    "    # FunÃ§Ã£o de validaÃ§Ã£o\n",
    "    def validate_record(row):\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Validar CPF\n",
    "        if not row.get('cpf') or len(str(row['cpf']).replace('.', '').replace('-', '')) != 11:\n",
    "            errors.append(\"CPF invÃ¡lido ou ausente\")\n",
    "        \n",
    "        # Validar nome\n",
    "        if not row.get('nome') or len(str(row['nome']).strip()) < 3:\n",
    "            errors.append(\"Nome invÃ¡lido ou muito curto\")\n",
    "        \n",
    "        # Validar valores financeiros\n",
    "        if row.get('salario_base', 0) <= 0:\n",
    "            errors.append(\"SalÃ¡rio base deve ser maior que zero\")\n",
    "        \n",
    "        if row.get('salario_base', 0) < 1320:  # SalÃ¡rio mÃ­nimo aproximado\n",
    "            warnings.append(\"SalÃ¡rio base abaixo do salÃ¡rio mÃ­nimo\")\n",
    "        \n",
    "        if row.get('horas_extras', 0) < 0:\n",
    "            errors.append(\"Horas extras nÃ£o podem ser negativas\")\n",
    "        \n",
    "        if row.get('descontos', 0) < 0:\n",
    "            errors.append(\"Descontos nÃ£o podem ser negativos\")\n",
    "        \n",
    "        # Validar consistÃªncia do salÃ¡rio lÃ­quido\n",
    "        calculated_liquido = row.get('salario_base', 0) + row.get('horas_extras', 0) - row.get('descontos', 0)\n",
    "        actual_liquido = row.get('salario_liquido', 0)\n",
    "        \n",
    "        if abs(calculated_liquido - actual_liquido) > 0.01:  # TolerÃ¢ncia de 1 centavo\n",
    "            warnings.append(f\"InconsistÃªncia no salÃ¡rio lÃ­quido (calculado: {calculated_liquido:.2f}, informado: {actual_liquido:.2f})\")\n",
    "        \n",
    "        return {\n",
    "            'errors': errors,\n",
    "            'warnings': warnings,\n",
    "            'is_valid': len(errors) == 0\n",
    "        }\n",
    "    \n",
    "    # Aplicar validaÃ§Ã£o\n",
    "    validation_results = df.apply(validate_record, axis=1)\n",
    "    \n",
    "    # Adicionar resultados ao DataFrame\n",
    "    df['validation_errors'] = [result['errors'] for result in validation_results]\n",
    "    df['validation_warnings'] = [result['warnings'] for result in validation_results]\n",
    "    df['is_valid'] = [result['is_valid'] for result in validation_results]\n",
    "    \n",
    "    # EstatÃ­sticas de validaÃ§Ã£o\n",
    "    total_records = len(df)\n",
    "    valid_records = df['is_valid'].sum()\n",
    "    invalid_records = total_records - valid_records\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Resultados da ValidaÃ§Ã£o:\")\n",
    "    print(f\"   âœ… Registros vÃ¡lidos: {valid_records} ({valid_records/total_records*100:.1f}%)\")\n",
    "    print(f\"   âŒ Registros invÃ¡lidos: {invalid_records} ({invalid_records/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar detalhes dos erros\n",
    "    if invalid_records > 0:\n",
    "        print(f\"\\nâš ï¸  Detalhes dos Erros:\")\n",
    "        invalid_df = df[~df['is_valid']]\n",
    "        for idx, row in invalid_df.iterrows():\n",
    "            print(f\"   â€¢ {row['nome']}: {', '.join(row['validation_errors'])}\")\n",
    "    \n",
    "    # Mostrar warnings\n",
    "    warnings_count = sum(len(w) for w in df['validation_warnings'])\n",
    "    if warnings_count > 0:\n",
    "        print(f\"\\nâš ï¸  Avisos encontrados: {warnings_count}\")\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['validation_warnings']:\n",
    "                print(f\"   â€¢ {row['nome']}: {', '.join(row['validation_warnings'])}\")\n",
    "    \n",
    "    # Retornar dados validados\n",
    "    valid_data = df[df['is_valid']].drop(['validation_errors', 'validation_warnings', 'is_valid'], axis=1)\n",
    "    \n",
    "    return {\n",
    "        'all_data': df,\n",
    "        'valid_data': valid_data,\n",
    "        'validation_summary': {\n",
    "            'total_records': total_records,\n",
    "            'valid_records': valid_records,\n",
    "            'invalid_records': invalid_records,\n",
    "            'warnings_count': warnings_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Validar dados do resultado do processamento\n",
    "if 'result' in locals() and result.get('data'):\n",
    "    validation_result = validate_and_map_data(result['data'])\n",
    "else:\n",
    "    validation_result = validate_and_map_data(None)\n",
    "\n",
    "# Exibir dados vÃ¡lidos\n",
    "print(f\"\\nðŸ“‹ Dados VÃ¡lidos para Armazenamento:\")\n",
    "if not validation_result['valid_data'].empty:\n",
    "    display(validation_result['valid_data'])\n",
    "else:\n",
    "    print(\"   âŒ Nenhum registro vÃ¡lido encontrado\")"
=======
   "id": "d934db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_processing_pipeline(job_id: str, client_id: str = \"default\", max_wait_time: int = 300):\n",
    "    \"\"\"\n",
    "    Monitora o pipeline de processamento com timeout e relatÃ³rios de progresso.\n",
    "    \n",
    "    Args:\n",
    "        job_id: ID do job para monitoramento\n",
    "        client_id: ID do cliente\n",
    "        max_wait_time: Tempo mÃ¡ximo de espera em segundos\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status final do processamento\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    status_history = []\n",
    "    \n",
    "    print(f\"ðŸ” Iniciando monitoramento do Job: {job_id}\")\n",
    "    print(f\"â±ï¸ Tempo mÃ¡ximo de espera: {max_wait_time}s\")\n",
    "    \n",
    "    while True:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Verificar timeout\n",
    "        if elapsed_time > max_wait_time:\n",
    "            print(f\"â° Timeout atingido apÃ³s {elapsed_time:.1f}s\")\n",
    "            return {\n",
    "                'status': 'TIMEOUT',\n",
    "                'elapsed_time': elapsed_time,\n",
    "                'history': status_history\n",
    "            }\n",
    "        \n",
    "        # Consultar status atual\n",
    "        current_status = check_job_status(job_id, client_id)\n",
    "        \n",
    "        if current_status:\n",
    "            status = current_status.get('status', 'UNKNOWN')\n",
    "            \n",
    "            # Adicionar ao histÃ³rico se mudou\n",
    "            if not status_history or status_history[-1]['status'] != status:\n",
    "                status_entry = {\n",
    "                    'status': status,\n",
    "                    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "                    'elapsed_time': elapsed_time\n",
    "                }\n",
    "                status_history.append(status_entry)\n",
    "                print(f\"ðŸ“Š [{elapsed_time:.1f}s] Status: {status}\")\n",
    "            \n",
    "            # Verificar se terminou\n",
    "            if status in [\"CONCLUIDO_SUCESSO\", \"CONCLUIDO_COM_PENDENCIAS\", \"FALHA_DOCAI\", \"FALHA_MAPEAMENTO\", \"ERRO\"]:\n",
    "                print(f\"âœ… Processamento finalizado: {status}\")\n",
    "                return {\n",
    "                    'status': status,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'history': status_history,\n",
    "                    'details': current_status\n",
    "                }\n",
    "        else:\n",
    "            print(f\"âš ï¸ Erro ao consultar status do job\")\n",
    "        \n",
    "        # Aguardar antes da prÃ³xima consulta\n",
    "        time.sleep(10)\n",
    "\n",
    "def visualize_processing_results(data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Cria visualizaÃ§Ãµes dos resultados do processamento.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"âš ï¸ Nenhum dado para visualizar\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Configurar matplotlib para melhor visualizaÃ§Ã£o\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ðŸ“Š AnÃ¡lise dos Dados Processados', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. DistribuiÃ§Ã£o por status de validaÃ§Ã£o\n",
    "    status_counts = df['status_validacao'].value_counts()\n",
    "    axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Status de ValidaÃ§Ã£o')\n",
    "    \n",
    "    # 2. Top 10 rubricas por valor\n",
    "    top_rubricas = df.groupby('rubrica')['valor'].sum().sort_values(ascending=False).head(10)\n",
    "    axes[0, 1].barh(range(len(top_rubricas)), top_rubricas.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_rubricas)))\n",
    "    axes[0, 1].set_yticklabels(top_rubricas.index)\n",
    "    axes[0, 1].set_title('Top 10 Rubricas por Valor Total')\n",
    "    axes[0, 1].set_xlabel('Valor (R$)')\n",
    "    \n",
    "    # 3. DistribuiÃ§Ã£o de valores\n",
    "    axes[1, 0].hist(df['valor'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('DistribuiÃ§Ã£o de Valores')\n",
    "    axes[1, 0].set_xlabel('Valor (R$)')\n",
    "    axes[1, 0].set_ylabel('FrequÃªncia')\n",
    "    \n",
    "    # 4. FuncionÃ¡rios com mais rubricas\n",
    "    func_rubricas = df.groupby('funcionario_nome').size().sort_values(ascending=False).head(10)\n",
    "    axes[1, 1].bar(range(len(func_rubricas)), func_rubricas.values)\n",
    "    axes[1, 1].set_xticks(range(len(func_rubricas)))\n",
    "    axes[1, 1].set_xticklabels(func_rubricas.index, rotation=45, ha='right')\n",
    "    axes[1, 1].set_title('FuncionÃ¡rios com Mais Rubricas')\n",
    "    axes[1, 1].set_ylabel('NÃºmero de Rubricas')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # EstatÃ­sticas resumidas\n",
    "    print(\"\\nðŸ“ˆ EstatÃ­sticas Resumidas:\")\n",
    "    print(f\"  Total de registros: {len(df)}\")\n",
    "    print(f\"  FuncionÃ¡rios Ãºnicos: {df['funcionario_nome'].nunique()}\")\n",
    "    print(f\"  Rubricas Ãºnicas: {df['rubrica'].nunique()}\")\n",
    "    print(f\"  Valor total: R$ {df['valor'].sum():,.2f}\")\n",
    "    print(f\"  Valor mÃ©dio: R$ {df['valor'].mean():.2f}\")\n",
    "\n",
    "# Exemplo de uso das funÃ§Ãµes de monitoramento\n",
    "# job_id_example = \"12345-abcde-67890\"\n",
    "# resultado_monitoramento = monitor_processing_pipeline(job_id_example)\n",
    "# \n",
    "# if 'dados_validados' in locals():\n",
    "#     visualize_processing_results(dados_validados)"
>>>>>>> Principal
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< copilot/fix-d10311ee-183b-4a07-8f7a-f3e2c3d8e909
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Resumo do Processamento e PrÃ³ximos Passos\n",
    "\n",
    "### Principais Funcionalidades Demonstradas\n",
    "\n",
    "1. **Upload e Processamento**: Upload de PDF e inÃ­cio do processamento assÃ­ncrono\n",
    "2. **Monitoramento**: Acompanhamento do status do job em tempo real\n",
    "3. **ValidaÃ§Ã£o**: ValidaÃ§Ã£o abrangente dos dados extraÃ­dos\n",
    "4. **Armazenamento**: PersistÃªncia segura no banco de dados\n",
    "5. **VisualizaÃ§Ã£o**: Dashboards e grÃ¡ficos para anÃ¡lise\n",
    "\n",
    "### Arquitetura do Sistema\n",
    "\n",
    "```\n",
    "ðŸ“„ PDF Upload â†’ ðŸ”„ Processamento AssÃ­ncrono â†’ âœ… ValidaÃ§Ã£o â†’ ðŸ’¾ Armazenamento â†’ ðŸ“Š VisualizaÃ§Ã£o\n",
    "```\n",
    "\n",
    "### BenefÃ­cios\n",
    "\n",
    "- **AutomaÃ§Ã£o**: Reduz trabalho manual de digitaÃ§Ã£o\n",
    "- **PrecisÃ£o**: ValidaÃ§Ã£o automÃ¡tica de consistÃªncia\n",
    "- **Escalabilidade**: Processamento assÃ­ncrono para arquivos grandes\n",
    "- **Rastreabilidade**: Log completo de todas as operaÃ§Ãµes\n",
    "- **IntegraÃ§Ã£o**: API compatÃ­vel com sistemas existentes\n",
    "\n",
    "### PrÃ³ximos Passos\n",
    "\n",
    "1. **IntegraÃ§Ã£o com OCR Real**: Implementar PaddleOCR ou similar\n",
    "2. **Melhorias de UI**: Interface web mais amigÃ¡vel\n",
    "3. **Alertas**: NotificaÃ§Ãµes automÃ¡ticas de inconsistÃªncias\n",
    "4. **RelatÃ³rios**: GeraÃ§Ã£o automÃ¡tica de relatÃ³rios de auditoria\n",
    "5. **Machine Learning**: Modelos preditivos para detecÃ§Ã£o de fraudes\n",
    "\n",
    "### DocumentaÃ§Ã£o TÃ©cnica\n",
    "\n",
    "- **API Endpoints**: Consulte a documentaÃ§Ã£o da API\n",
    "- **Schemas**: Estruturas de dados no arquivo de schemas\n",
    "- **ConfiguraÃ§Ã£o**: VariÃ¡veis de ambiente e configuraÃ§Ãµes\n",
    "- **Logs**: Sistema de logging estruturado\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“… Ãšltima atualizaÃ§Ã£o**: Janeiro 2025  \n",
    "**ðŸ‘¨â€ðŸ’» Desenvolvido por**: Equipe AUDITORIA360  \n",
    "**ðŸ“§ Suporte**: Consulte a documentaÃ§Ã£o tÃ©cnica ou entre em contato com a equipe de desenvolvimento"
=======
   "id": "13a8c7db",
   "metadata": {},
   "source": [
    "## 7. Tratamento de Erros e Logs\n",
    "\n",
    "Sistema robusto de tratamento de erros com logs detalhados para debugging e auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/tmp/auditoria_folha.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FolhaProcessingError(Exception):\n",
    "    \"\"\"ExceÃ§Ã£o especÃ­fica para erros de processamento de folha.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ErrorHandler:\n",
    "    \"\"\"\n",
    "    Classe para centralizar o tratamento de erros do sistema.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_error(error: Union[Exception, str], context: str = \"\", job_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Registra erro com contexto adicional.\n",
    "        \"\"\"\n",
    "        error_msg = str(error)\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'job_id': job_id,\n",
    "            'context': context,\n",
    "            'error': error_msg,\n",
    "            'error_type': type(error).__name__ if isinstance(error, Exception) else 'Unknown'\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"[{context}] {error_msg}\", extra=log_entry)\n",
    "        return log_entry\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_processing_error(func):\n",
    "        \"\"\"\n",
    "        Decorator para tratamento automÃ¡tico de erros em funÃ§Ãµes de processamento.\n",
    "        \"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except requests.RequestException as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de conexÃ£o em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro de conexÃ£o com API', 'details': str(e)}\n",
    "            except FolhaProcessingError as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro de processamento em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro no processamento da folha', 'details': str(e)}\n",
    "            except Exception as e:\n",
    "                ErrorHandler.log_error(e, f\"Erro inesperado em {func.__name__}\")\n",
    "                return {'success': False, 'error': 'Erro inesperado', 'details': str(e)}\n",
    "        return wrapper\n",
    "\n",
    "# Exemplo de tratamento de erros integrado\n",
    "print(\"ðŸ”§ Sistema de tratamento de erros configurado com sucesso!\")\n",
    "print(\"ðŸ“ Logs serÃ£o salvos em: /tmp/auditoria_folha.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec157406",
   "metadata": {},
   "source": [
    "## 8. Exemplo de Fluxo Completo\n",
    "\n",
    "DemonstraÃ§Ã£o de um fluxo completo de processamento de folha de pagamento com todas as funcionalidades integradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47baf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_complete_folha_processing_workflow(pdf_file_path: str, client_id: str = \"demo_client\"):\n",
    "    \"\"\"\n",
    "    Executa o fluxo completo de processamento de folha de pagamento.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file_path: Caminho para o arquivo PDF\n",
    "        client_id: ID do cliente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado completo do processamento\n",
    "    \"\"\"\n",
    "    workflow_result = {\n",
    "        'client_id': client_id,\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'steps': [],\n",
    "        'final_status': 'INICIADO'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Passo 1: Upload do PDF para GCS\n",
    "        print(\"ðŸ“¤ Passo 1: Upload do PDF para Google Cloud Storage\")\n",
    "        # gcs_uri = upload_pdf_to_gcs(pdf_file_path)\n",
    "        gcs_uri = f\"gs://{BUCKET_NAME}/uploads/demo_folha.pdf\"  # Simulado\n",
    "        workflow_result['steps'].append({\n",
    "            'step': 1,\n",
    "            'name': 'Upload PDF',\n",
    "            'status': 'SUCESSO',\n",
    "            'gcs_uri': gcs_uri\n",
    "        })\n",
    "        \n",
    "        # Passo 2: Iniciar processamento assÃ­ncrono\n",
    "        print(\"ðŸš€ Passo 2: Iniciar processamento assÃ­ncrono com Document AI\")\n",
    "        job_id = process_pdf_with_docai(gcs_uri, client_id)\n",
    "        if not job_id:\n",
    "            raise FolhaProcessingError(\"Falha ao iniciar processamento\")\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 2,\n",
    "            'name': 'Iniciar Processamento',\n",
    "            'status': 'SUCESSO',\n",
    "            'job_id': job_id\n",
    "        })\n",
    "        \n",
    "        # Passo 3: Monitorar processamento\n",
    "        print(\"ðŸ‘ï¸ Passo 3: Monitorar processamento\")\n",
    "        monitoring_result = monitor_processing_pipeline(job_id, client_id, max_wait_time=120)\n",
    "        \n",
    "        workflow_result['steps'].append({\n",
    "            'step': 3,\n",
    "            'name': 'Monitoramento',\n",
    "            'status': monitoring_result['status'],\n",
    "            'elapsed_time': monitoring_result['elapsed_time']\n",
    "        })\n",
    "        \n",
    "        # Passo 4: Processamento dos dados (simulado)\n",
    "        if monitoring_result['status'] in ['CONCLUIDO_SUCESSO', 'CONCLUIDO_COM_PENDENCIAS']:\n",
    "            print(\"ðŸ” Passo 4: ValidaÃ§Ã£o e mapeamento de dados\")\n",
    "            \n",
    "            # Dados simulados extraÃ­dos do Document AI\n",
    "            dados_extraidos = [\n",
    "                {\"funcionario_nome\": \"JoÃ£o Silva\", \"cpf\": \"123.456.789-00\", \"rubrica\": \"SalÃ¡rio Base\", \"valor\": 5000.00},\n",
    "                {\"funcionario_nome\": \"Maria Santos\", \"cpf\": \"987.654.321-00\", \"rubrica\": \"Vale AlimentaÃ§Ã£o\", \"valor\": 400.00},\n",
    "                {\"funcionario_nome\": \"Pedro Oliveira\", \"cpf\": \"456.789.123-45\", \"rubrica\": \"FGTS\", \"valor\": 320.00},\n",
    "            ]\n",
    "            \n",
    "            dados_validados = validate_folha_data(dados_extraidos)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 4,\n",
    "                'name': 'ValidaÃ§Ã£o de Dados',\n",
    "                'status': 'SUCESSO',\n",
    "                'records_processed': len(dados_validados),\n",
    "                'validation_stats': pd.DataFrame(dados_validados)['status_validacao'].value_counts().to_dict()\n",
    "            })\n",
    "            \n",
    "            # Passo 5: Armazenamento no BigQuery\n",
    "            print(\"ðŸ’¾ Passo 5: Armazenamento no BigQuery\")\n",
    "            storage_result = insert_validated_data_to_bigquery(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 5,\n",
    "                'name': 'Armazenamento BigQuery',\n",
    "                'status': 'SUCESSO' if storage_result['success'] else 'ERRO',\n",
    "                'inserted_count': storage_result['inserted_count'],\n",
    "                'error_count': storage_result['error_count']\n",
    "            })\n",
    "            \n",
    "            # Passo 6: GeraÃ§Ã£o de relatÃ³rio\n",
    "            print(\"ðŸ“Š Passo 6: GeraÃ§Ã£o de visualizaÃ§Ãµes\")\n",
    "            visualize_processing_results(dados_validados)\n",
    "            \n",
    "            workflow_result['steps'].append({\n",
    "                'step': 6,\n",
    "                'name': 'VisualizaÃ§Ãµes',\n",
    "                'status': 'SUCESSO'\n",
    "            })\n",
    "            \n",
    "            workflow_result['final_status'] = 'CONCLUIDO_SUCESSO'\n",
    "        else:\n",
    "            workflow_result['final_status'] = 'FALHA_PROCESSAMENTO'\n",
    "            \n",
    "    except Exception as e:\n",
    "        ErrorHandler.log_error(e, \"Fluxo completo de processamento\")\n",
    "        workflow_result['final_status'] = 'ERRO'\n",
    "        workflow_result['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        workflow_result['end_time'] = datetime.now().isoformat()\n",
    "        \n",
    "    # Resumo final\n",
    "    print(f\"\\nðŸ Workflow Finalizado!\")\n",
    "    print(f\"   Status: {workflow_result['final_status']}\")\n",
    "    print(f\"   Passos executados: {len(workflow_result['steps'])}\")\n",
    "    \n",
    "    return workflow_result\n",
    "\n",
    "# Exemplo de execuÃ§Ã£o do fluxo completo\n",
    "print(\"ðŸŽ¯ DemonstraÃ§Ã£o do Fluxo Completo de Processamento\")\n",
    "print(\"   (Utilizando dados simulados para demonstraÃ§Ã£o)\")\n",
    "# resultado_completo = execute_complete_folha_processing_workflow(\"demo_folha.pdf\")\n",
    "print(\"\\nâœ… Notebook configurado e pronto para uso!\")\n",
    "print(\"ðŸ“‹ Para executar o fluxo completo, chame: execute_complete_folha_processing_workflow('caminho_do_pdf.pdf')\")"
>>>>>>> Principal
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
