"""
Portal Demandas FastAPI Application
Comprehensive API for managing demands/tickets with SQLAlchemy + Neon PostgreSQL
"""

import logging
import json
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from typing import List, Optional

from fastapi import Depends, FastAPI, HTTPException, Query, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import asc, desc, or_
from sqlalchemy.orm import Session

from portal_demandas.db import TicketComment as TicketCommentDB
from portal_demandas.db import (
    TicketDB,
    ContabilidadeDB,
    EmpresaDB, 
    SindicatoDB,
    ConvencaoColetivaCCTDB,
    LegislacaoDocumentoDB,
    ControleMensalDB,
    TarefaControleDB,
    TemplateControleDB,
    TemplateControleTarefaDB,
    ProcessamentosFolhaDB,
    HistoricoAnalisesRiscoDB,
    get_db,
    init_portal_db,
)
from portal_demandas.models import (
    Ticket,
    TicketCategoria,
    TicketComment,
    TicketCreate,
    TicketListResponse,
    TicketPrioridade,
    TicketStats,
    TicketStatus,
    TicketUpdate,
    # Controle Mensal models
    ControleMensalDetalhado,
    ControleMensalResponse,
    ControleMensalSumario,
    Tarefa,
    TemplateControle,
    TemplateControleCreate,
    TemplateAplicacao,
    # Payroll Audit models
    FuncionarioDivergencia,
    ProcessamentoFolhaResponse,
    AuditoriaFolhaRequest,
    # CCT models
    Sindicato,
    SindicatoCreate,
    ConvencaoColetivaCCT,
    ConvencaoColetivaCCTCreate,
    CCTListResponse,
    TipoDocumento,
    StatusProcessamento,
    # Legislation models
    LegislacaoDocumento,
    LegislacaoDocumentoCreate,
    ExtrairPDFResponse,
    # Risk Analysis models
    RiscoDetalhado,
    AnaliseRiscoRequest,
    AnaliseRiscoResponse,
    HistoricoAnaliseRisco,
)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan event handler"""
    # Startup
    try:
        init_portal_db()
        logger.info("Portal demandas database initialized")
    except Exception as e:
        logger.error(f"Failed to initialize database: {e}")

    yield

    # Shutdown (if needed)
    logger.info("Portal demandas API shutting down")


# Create FastAPI app with lifespan
app = FastAPI(
    title="Portal Demandas API",
    description="API para gerenciamento de demandas e tickets do AUDITORIA360",
    version="1.0.0",
    lifespan=lifespan,
    tags_metadata=[
        {"name": "tickets", "description": "Opera√ß√µes com tickets"},
        {"name": "comments", "description": "Coment√°rios dos tickets"},
        {"name": "stats", "description": "Estat√≠sticas e relat√≥rios"},
        {"name": "controle-mensal", "description": "Controles mensais das empresas"},
        {"name": "templates", "description": "Templates para controles recorrentes"},
        {"name": "folha-pagamento", "description": "Auditoria inteligente da folha de pagamento com IA"},
    ],
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure properly for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Health check
@app.get("/health", tags=["health"])
def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "portal_demandas",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": "1.0.0",
    }


# Ticket CRUD operations
@app.post("/tickets/", response_model=Ticket, tags=["tickets"])
def criar_ticket(ticket: TicketCreate, db: Session = Depends(get_db)):
    """
    Criar um novo ticket
    """
    try:
        # Create database ticket
        db_ticket = TicketDB(
            titulo=ticket.titulo,
            descricao=ticket.descricao,
            etapa=ticket.etapa,
            prazo=ticket.prazo,
            responsavel=ticket.responsavel,
            prioridade=ticket.prioridade.value,
            categoria=ticket.categoria.value,
            tags=ticket.tags,
            tempo_estimado=ticket.tempo_estimado,
            criado_em=datetime.now(timezone.utc),
            atualizado_em=datetime.now(timezone.utc),
        )

        db.add(db_ticket)
        db.commit()
        db.refresh(db_ticket)

        # Log creation
        logger.info(f"Ticket created: ID={db_ticket.id}, Titulo='{db_ticket.titulo}'")

        return Ticket.model_validate(db_ticket)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create ticket: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao criar ticket: {str(e)}")


@app.get("/tickets/{ticket_id}", response_model=Ticket, tags=["tickets"])
def obter_ticket(ticket_id: int, db: Session = Depends(get_db)):
    """
    Obter um ticket espec√≠fico por ID
    """
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket n√£o encontrado")

    return Ticket.model_validate(db_ticket)


@app.get("/tickets/", response_model=TicketListResponse, tags=["tickets"])
def listar_tickets(
    page: int = Query(1, ge=1, description="N√∫mero da p√°gina"),
    per_page: int = Query(
        10, ge=1, le=50, description="Itens por p√°gina (m√°ximo 50 para performance)"
    ),
    status: Optional[List[TicketStatus]] = Query(
        None, description="Filtrar por status"
    ),
    prioridade: Optional[List[TicketPrioridade]] = Query(
        None, description="Filtrar por prioridade"
    ),
    categoria: Optional[List[TicketCategoria]] = Query(
        None, description="Filtrar por categoria"
    ),
    responsavel: Optional[str] = Query(None, description="Filtrar por respons√°vel"),
    etapa: Optional[str] = Query(None, description="Filtrar por etapa"),
    search: Optional[str] = Query(None, description="Buscar no t√≠tulo e descri√ß√£o"),
    sort_by: str = Query("criado_em", description="Campo para ordena√ß√£o"),
    sort_order: str = Query("desc", description="Ordem: asc ou desc"),
    db: Session = Depends(get_db),
):
    """
    Listar tickets com filtros e pagina√ß√£o - PERFORMANCE OPTIMIZED
    Reduced maximum per_page from 100 to 50 for better performance
    """
    import time

    start_time = time.time()

    try:
        # Build query
        query = db.query(TicketDB)

        # Apply filters
        if status:
            query = query.filter(TicketDB.status.in_([s.value for s in status]))

        if prioridade:
            query = query.filter(TicketDB.prioridade.in_([p.value for p in prioridade]))

        if categoria:
            query = query.filter(TicketDB.categoria.in_([c.value for c in categoria]))

        if responsavel:
            query = query.filter(TicketDB.responsavel.ilike(f"%{responsavel}%"))

        if etapa:
            query = query.filter(TicketDB.etapa.ilike(f"%{etapa}%"))

        if search:
            search_filter = or_(
                TicketDB.titulo.ilike(f"%{search}%"),
                TicketDB.descricao.ilike(f"%{search}%"),
            )
            query = query.filter(search_filter)

        # Apply sorting
        if hasattr(TicketDB, sort_by):
            sort_column = getattr(TicketDB, sort_by)
            if sort_order.lower() == "desc":
                query = query.order_by(desc(sort_column))
            else:
                query = query.order_by(asc(sort_column))

        # Get total count
        total = query.count()

        # Apply pagination
        offset = (page - 1) * per_page
        tickets = query.offset(offset).limit(per_page).all()

        # Convert to Pydantic models
        ticket_list = [Ticket.model_validate(ticket) for ticket in tickets]

        # Calculate processing time
        processing_time = time.time() - start_time

        logger.info(
            f"Ticket listing completed in {processing_time:.3f}s (page {page}, {len(ticket_list)} items)"
        )

        response = TicketListResponse.create(
            tickets=ticket_list, total=total, page=page, per_page=per_page
        )

        # Add performance metadata if available
        if hasattr(response, "__dict__"):
            response.__dict__["_performance"] = {
                "processing_time_seconds": processing_time,
                "optimization_level": "enhanced",
            }

        return response

    except Exception as e:
        logger.error(f"Failed to list tickets: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao listar tickets: {str(e)}")


@app.patch("/tickets/{ticket_id}", response_model=Ticket, tags=["tickets"])
def atualizar_ticket(
    ticket_id: int, ticket_update: TicketUpdate, db: Session = Depends(get_db)
):
    """
    Atualizar um ticket existente
    """
    # Find ticket
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket n√£o encontrado")

    try:
        # Update fields
        update_data = ticket_update.dict(exclude_unset=True)

        for field, value in update_data.items():
            if hasattr(db_ticket, field) and value is not None:
                # Handle enum fields
                if field in ["status", "prioridade", "categoria"] and hasattr(
                    value, "value"
                ):
                    setattr(db_ticket, field, value.value)
                else:
                    setattr(db_ticket, field, value)

        # Update timestamp
        db_ticket.atualizado_em = datetime.now(timezone.utc)

        db.commit()
        db.refresh(db_ticket)

        logger.info(f"Ticket updated: ID={ticket_id}")

        return Ticket.model_validate(db_ticket)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to update ticket {ticket_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao atualizar ticket: {str(e)}"
        )


@app.delete("/tickets/{ticket_id}", tags=["tickets"])
def deletar_ticket(ticket_id: int, db: Session = Depends(get_db)):
    """
    Deletar um ticket
    """
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket n√£o encontrado")

    try:
        # Delete related comments first
        db.query(TicketCommentDB).filter(
            TicketCommentDB.ticket_id == ticket_id
        ).delete()

        # Delete ticket
        db.delete(db_ticket)
        db.commit()

        logger.info(f"Ticket deleted: ID={ticket_id}")

        return {"message": "Ticket deletado com sucesso"}

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to delete ticket {ticket_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao deletar ticket: {str(e)}")


# Comments endpoints
@app.post(
    "/tickets/{ticket_id}/comments/", response_model=TicketComment, tags=["comments"]
)
def adicionar_comentario(
    ticket_id: int, comment: TicketComment, db: Session = Depends(get_db)
):
    """
    Adicionar coment√°rio a um ticket
    """
    # Verify ticket exists
    ticket_exists = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not ticket_exists:
        raise HTTPException(status_code=404, detail="Ticket n√£o encontrado")

    try:
        db_comment = TicketCommentDB(
            ticket_id=ticket_id,
            autor=comment.autor,
            comentario=comment.comentario,
            tipo=comment.tipo,
            criado_em=datetime.now(timezone.utc),
        )

        db.add(db_comment)
        db.commit()
        db.refresh(db_comment)

        return TicketComment.model_validate(db_comment)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to add comment to ticket {ticket_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao adicionar coment√°rio: {str(e)}"
        )


@app.get(
    "/tickets/{ticket_id}/comments/",
    response_model=List[TicketComment],
    tags=["comments"],
)
def listar_comentarios(ticket_id: int, db: Session = Depends(get_db)):
    """
    Listar coment√°rios de um ticket
    """
    comments = (
        db.query(TicketCommentDB)
        .filter(TicketCommentDB.ticket_id == ticket_id)
        .order_by(TicketCommentDB.criado_em.desc())
        .all()
    )

    return [TicketComment.model_validate(comment) for comment in comments]


# Statistics endpoints with performance optimization
@app.get("/stats/", response_model=TicketStats, tags=["stats"])
def obter_estatisticas(db: Session = Depends(get_db)):
    """
    Obter estat√≠sticas dos tickets - PERFORMANCE OPTIMIZED
    Target: <0.5s response time (was 1.5s)
    """
    import time

    start_time = time.time()

    try:
        # Use single query with aggregations for better performance
        from sqlalchemy import case, func

        # Single query to get all statistics at once
        stats_query = db.query(
            func.count(TicketDB.id).label("total"),
            func.sum(case((TicketDB.status == "pendente", 1), else_=0)).label(
                "pendentes"
            ),
            func.sum(case((TicketDB.status == "em_andamento", 1), else_=0)).label(
                "em_andamento"
            ),
            func.sum(case((TicketDB.status == "concluido", 1), else_=0)).label(
                "concluidos"
            ),
            func.sum(case((TicketDB.status == "cancelado", 1), else_=0)).label(
                "cancelados"
            ),
            # Priority counts
            func.sum(case((TicketDB.prioridade == "baixa", 1), else_=0)).label(
                "prioridade_baixa"
            ),
            func.sum(case((TicketDB.prioridade == "media", 1), else_=0)).label(
                "prioridade_media"
            ),
            func.sum(case((TicketDB.prioridade == "alta", 1), else_=0)).label(
                "prioridade_alta"
            ),
            func.sum(case((TicketDB.prioridade == "critica", 1), else_=0)).label(
                "prioridade_critica"
            ),
            # Category counts
            func.sum(case((TicketDB.categoria == "geral", 1), else_=0)).label(
                "categoria_geral"
            ),
            func.sum(case((TicketDB.categoria == "auditoria", 1), else_=0)).label(
                "categoria_auditoria"
            ),
            func.sum(case((TicketDB.categoria == "folha", 1), else_=0)).label(
                "categoria_folha"
            ),
            func.sum(case((TicketDB.categoria == "documentos", 1), else_=0)).label(
                "categoria_documentos"
            ),
            func.sum(case((TicketDB.categoria == "cct", 1), else_=0)).label(
                "categoria_cct"
            ),
            func.sum(case((TicketDB.categoria == "sistema", 1), else_=0)).label(
                "categoria_sistema"
            ),
            # Average completion time for completed tickets
            func.avg(
                case((TicketDB.status == "concluido", TicketDB.tempo_gasto), else_=None)
            ).label("tempo_medio"),
        ).first()

        # Build response from single query result
        por_prioridade = {
            "baixa": int(stats_query.prioridade_baixa or 0),
            "media": int(stats_query.prioridade_media or 0),
            "alta": int(stats_query.prioridade_alta or 0),
            "critica": int(stats_query.prioridade_critica or 0),
        }

        por_categoria = {
            "geral": int(stats_query.categoria_geral or 0),
            "auditoria": int(stats_query.categoria_auditoria or 0),
            "folha": int(stats_query.categoria_folha or 0),
            "documentos": int(stats_query.categoria_documentos or 0),
            "cct": int(stats_query.categoria_cct or 0),
            "sistema": int(stats_query.categoria_sistema or 0),
        }

        # Calculate processing time
        processing_time = time.time() - start_time

        result = TicketStats(
            total=int(stats_query.total or 0),
            pendentes=int(stats_query.pendentes or 0),
            em_andamento=int(stats_query.em_andamento or 0),
            concluidos=int(stats_query.concluidos or 0),
            cancelados=int(stats_query.cancelados or 0),
            por_prioridade=por_prioridade,
            por_categoria=por_categoria,
            tempo_medio_conclusao=(
                float(stats_query.tempo_medio) if stats_query.tempo_medio else None
            ),
        )

        logger.info(f"Portal stats generated in {processing_time:.3f}s")

        if processing_time > 0.5:
            logger.warning(f"Portal stats took {processing_time:.3f}s (target: <0.5s)")

        return result

    except Exception as e:
        logger.error(f"Failed to get statistics: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao obter estat√≠sticas: {str(e)}"
        )


# Bulk operations
@app.patch("/tickets/bulk/status", tags=["tickets"])
def atualizar_status_bulk(
    ticket_ids: List[int], new_status: TicketStatus, db: Session = Depends(get_db)
):
    """
    Atualizar status de m√∫ltiplos tickets
    """
    try:
        updated_count = (
            db.query(TicketDB)
            .filter(TicketDB.id.in_(ticket_ids))
            .update(
                {
                    "status": new_status.value,
                    "atualizado_em": datetime.now(timezone.utc),
                },
                synchronize_session=False,
            )
        )

        db.commit()

        logger.info(
            f"Bulk status update: {updated_count} tickets updated to {new_status.value}"
        )

        return {
            "message": f"{updated_count} tickets atualizados para status '{new_status.value}'",
            "updated_count": updated_count,
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Failed bulk status update: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro na atualiza√ß√£o em lote: {str(e)}"
        )


# ===== CONTROLE MENSAL ENDPOINTS =====

@app.get("/v1/controles/{ano}/{mes}", response_model=ControleMensalResponse, tags=["controle-mensal"])
def obter_controles_do_mes(
    ano: int, mes: int, db: Session = Depends(get_db)
):
    """
    Obter todos os controles mensais do ano/m√™s especificado
    Endpoint principal que substitui as chamadas diretas ao Supabase
    """
    try:
        # Get user's accounting firm ID (this would come from auth in production)
        # For now, we'll assume contabilidade_id = 1 for testing
        contabilidade_id = 1  # TODO: Get from authentication context
        
        # Query controls with companies and tasks
        controles_query = (
            db.query(ControleMensalDB, EmpresaDB.nome.label('nome_empresa'))
            .join(EmpresaDB, ControleMensalDB.empresa_id == EmpresaDB.id)
            .filter(EmpresaDB.contabilidade_id == contabilidade_id)
            .filter(ControleMensalDB.ano == ano)
            .filter(ControleMensalDB.mes == mes)
        )
        
        controles_raw = controles_query.all()
        
        # Build detailed control objects
        controles = []
        for controle_db, nome_empresa in controles_raw:
            # Get tasks for this control
            tarefas_raw = (
                db.query(TarefaControleDB)
                .filter(TarefaControleDB.controle_mensal_id == controle_db.id)
                .all()
            )
            
            tarefas = [
                Tarefa(
                    id=tarefa.id,
                    nome_tarefa=tarefa.descricao_tarefa,
                    concluido=tarefa.concluida,
                    data_conclusao=tarefa.data_conclusao
                ) for tarefa in tarefas_raw
            ]
            
            controles.append(
                ControleMensalDetalhado(
                    id_controle=controle_db.id,
                    mes=controle_db.mes,
                    ano=controle_db.ano,
                    status_dados=controle_db.status,
                    id_empresa=controle_db.empresa_id,
                    nome_empresa=nome_empresa,
                    tarefas=tarefas
                )
            )
        
        # Calculate summary statistics
        total_empresas = (
            db.query(EmpresaDB)
            .filter(EmpresaDB.contabilidade_id == contabilidade_id)
            .count()
        )
        
        controles_iniciados = len(controles)
        controles_concluidos = sum(1 for c in controles if c.status_dados == "CONCLU√çDO")
        percentual_conclusao = (
            f"{(controles_concluidos / controles_iniciados * 100):.2f}%" 
            if controles_iniciados > 0 else "0.00%"
        )
        
        sumario = ControleMensalSumario(
            total_empresas=total_empresas,
            controles_iniciados=controles_iniciados,
            controles_concluidos=controles_concluidos,
            percentual_conclusao=percentual_conclusao
        )
        
        response = ControleMensalResponse(
            sumario=sumario,
            controles=controles
        )
        
        logger.info(f"Retrieved {len(controles)} controles for {ano}/{mes} - {percentual_conclusao} completed")
        return response

    except Exception as e:
        logger.error(f"Failed to get controles for {ano}/{mes}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao buscar controles: {str(e)}"
        )


@app.patch("/v1/controles-mensais/tarefas/{tarefa_id}/status", response_model=Tarefa, tags=["controle-mensal"])
def atualizar_status_tarefa(
    tarefa_id: int, concluido: bool, db: Session = Depends(get_db)
):
    """
    Atualizar o status de conclus√£o de uma tarefa
    """
    tarefa = db.query(TarefaControleDB).filter(TarefaControleDB.id == tarefa_id).first()
    
    if not tarefa:
        raise HTTPException(status_code=404, detail="Tarefa n√£o encontrada")
    
    try:
        tarefa.concluida = concluido
        tarefa.data_conclusao = datetime.now(timezone.utc) if concluido else None
        
        db.commit()
        db.refresh(tarefa)
        
        logger.info(f"Task {tarefa_id} updated: concluido={concluido}")
        
        return Tarefa(
            id=tarefa.id,
            nome_tarefa=tarefa.descricao_tarefa, 
            concluido=tarefa.concluida,
            data_conclusao=tarefa.data_conclusao
        )

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to update task {tarefa_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao atualizar tarefa: {str(e)}"
        )


# ===== TEMPLATE ENDPOINTS =====

@app.get("/v1/templates", response_model=List[TemplateControle], tags=["templates"])
def listar_templates(db: Session = Depends(get_db)):
    """
    Listar todos os templates da contabilidade do usu√°rio
    """
    try:
        # TODO: Get contabilidade_id from auth context
        contabilidade_id = 1
        
        templates_raw = (
            db.query(TemplateControleDB)
            .filter(TemplateControleDB.contabilidade_id == contabilidade_id)
            .all()
        )
        
        templates = []
        for template_db in templates_raw:
            # Get tasks for this template
            tarefas_raw = (
                db.query(TemplateControleTarefaDB.descricao_tarefa)
                .filter(TemplateControleTarefaDB.template_id == template_db.id)
                .all()
            )
            
            tarefas = [t[0] for t in tarefas_raw]
            
            templates.append(
                TemplateControle(
                    id=template_db.id,
                    contabilidade_id=template_db.contabilidade_id,
                    nome_template=template_db.nome_template,
                    descricao=template_db.descricao,
                    criado_em=template_db.criado_em,
                    tarefas=tarefas
                )
            )
        
        return templates

    except Exception as e:
        logger.error(f"Failed to list templates: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao listar templates: {str(e)}"
        )


@app.post("/v1/templates", response_model=TemplateControle, tags=["templates"])
def criar_template(template: TemplateControleCreate, db: Session = Depends(get_db)):
    """
    Criar um novo template de controle
    """
    try:
        # TODO: Get contabilidade_id from auth context
        contabilidade_id = 1
        
        # Create template
        template_db = TemplateControleDB(
            contabilidade_id=contabilidade_id,
            nome_template=template.nome_template,
            descricao=template.descricao,
            criado_em=datetime.now(timezone.utc)
        )
        
        db.add(template_db)
        db.flush()  # Get the ID without committing
        
        # Create template tasks
        for descricao in template.tarefas:
            tarefa_db = TemplateControleTarefaDB(
                template_id=template_db.id,
                descricao_tarefa=descricao
            )
            db.add(tarefa_db)
        
        db.commit()
        db.refresh(template_db)
        
        logger.info(f"Created template: {template_db.id} - {template_db.nome_template}")
        
        return TemplateControle(
            id=template_db.id,
            contabilidade_id=template_db.contabilidade_id,
            nome_template=template_db.nome_template,
            descricao=template_db.descricao,
            criado_em=template_db.criado_em,
            tarefas=template.tarefas
        )

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create template: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao criar template: {str(e)}"
        )


@app.post("/v1/controles/aplicar-template", tags=["templates"])
def aplicar_template(aplicacao: TemplateAplicacao, db: Session = Depends(get_db)):
    """
    Aplicar um template a empresas para criar controles mensais
    """
    try:
        # TODO: Get contabilidade_id from auth context  
        contabilidade_id = 1
        
        # Verify template exists and belongs to the accounting firm
        template = (
            db.query(TemplateControleDB)
            .filter(TemplateControleDB.id == aplicacao.template_id)
            .filter(TemplateControleDB.contabilidade_id == contabilidade_id)
            .first()
        )
        
        if not template:
            raise HTTPException(status_code=404, detail="Template n√£o encontrado")
        
        # Get template tasks
        template_tarefas = (
            db.query(TemplateControleTarefaDB)
            .filter(TemplateControleTarefaDB.template_id == template.id)
            .all()
        )
        
        # Get companies to apply to
        empresas_query = db.query(EmpresaDB).filter(EmpresaDB.contabilidade_id == contabilidade_id)
        
        if aplicacao.empresas_ids:
            empresas_query = empresas_query.filter(EmpresaDB.id.in_(aplicacao.empresas_ids))
        
        empresas = empresas_query.all()
        
        controles_criados = 0
        tarefas_criadas = 0
        
        for empresa in empresas:
            # Check if control already exists for this month/year
            controle_existente = (
                db.query(ControleMensalDB)
                .filter(ControleMensalDB.empresa_id == empresa.id)
                .filter(ControleMensalDB.mes == aplicacao.mes)
                .filter(ControleMensalDB.ano == aplicacao.ano)
                .first()
            )
            
            if controle_existente:
                continue  # Skip if already exists
            
            # Create monthly control
            controle = ControleMensalDB(
                empresa_id=empresa.id,
                mes=aplicacao.mes,
                ano=aplicacao.ano,
                status="AGUARD. DADOS",
                criado_em=datetime.now(timezone.utc)
            )
            
            db.add(controle)
            db.flush()  # Get ID
            controles_criados += 1
            
            # Create tasks from template
            for template_tarefa in template_tarefas:
                tarefa = TarefaControleDB(
                    controle_mensal_id=controle.id,
                    descricao_tarefa=template_tarefa.descricao_tarefa,
                    concluida=False,
                    criado_em=datetime.now(timezone.utc)
                )
                db.add(tarefa)
                tarefas_criadas += 1
        
        db.commit()
        
        logger.info(f"Applied template {aplicacao.template_id}: {controles_criados} controles, {tarefas_criadas} tarefas created")
        
        return {
            "message": f"Template aplicado com sucesso: {controles_criados} controles criados com {tarefas_criadas} tarefas",
            "controles_criados": controles_criados,
            "tarefas_criadas": tarefas_criadas
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to apply template {aplicacao.template_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao aplicar template: {str(e)}"
        )


# ===== PAYROLL AUDIT ENDPOINTS =====

@app.post("/v1/folha/auditar", response_model=ProcessamentoFolhaResponse, tags=["folha-pagamento"])
async def auditar_folha_pagamento(
    empresa_id: int,
    mes: int,
    ano: int,
    arquivo_pdf: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Motor de Auditoria Inteligente da Folha de Pagamento
    
    Processa PDFs da folha de pagamento usando IA para extrair dados,
    audita contra CCTs e gera relat√≥rio de conformidade e diverg√™ncias.
    """
    try:
        # Validate request parameters
        if mes < 1 or mes > 12:
            raise HTTPException(status_code=400, detail="M√™s deve estar entre 1 e 12")
        if ano < 2020 or ano > 2030:
            raise HTTPException(status_code=400, detail="Ano deve estar entre 2020 e 2030")
        
        # Validate file
        if not arquivo_pdf.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Apenas arquivos PDF s√£o aceitos")
        
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa n√£o encontrada")
        
        # Check if processing already exists for this month/year
        processamento_existente = (
            db.query(ProcessamentosFolhaDB)
            .filter(ProcessamentosFolhaDB.empresa_id == empresa_id)
            .filter(ProcessamentosFolhaDB.mes == mes)
            .filter(ProcessamentosFolhaDB.ano == ano)
            .first()
        )
        
        if processamento_existente:
            raise HTTPException(
                status_code=409, 
                detail=f"J√° existe processamento para {empresa.nome} em {mes:02d}/{ano}"
            )
        
        # Read PDF content
        pdf_content = await arquivo_pdf.read()
        
        # Create processing record
        processamento = ProcessamentosFolhaDB(
            empresa_id=empresa_id,
            mes=mes,
            ano=ano,
            arquivo_pdf=arquivo_pdf.filename,
            status_processamento="PROCESSANDO",
            criado_em=datetime.now(timezone.utc)
        )
        
        db.add(processamento)
        db.flush()  # Get the ID
        
        # TODO: Implement AI processing (for now, simulate with mock data)
        dados_extraidos, divergencias = await processar_pdf_com_ia(
            pdf_content, empresa, mes, ano, db
        )
        
        # Update processing record with results
        processamento.dados_extraidos = json.dumps(dados_extraidos, ensure_ascii=False)
        processamento.relatorio_divergencias = json.dumps(divergencias, ensure_ascii=False)
        processamento.total_funcionarios = len(dados_extraidos.get("funcionarios", []))
        processamento.total_divergencias = len(divergencias)
        processamento.status_processamento = "CONCLUIDO"
        processamento.concluido_em = datetime.now(timezone.utc)
        
        db.commit()
        db.refresh(processamento)
        
        logger.info(
            f"Payroll audit completed: empresa_id={empresa_id}, "
            f"funcionarios={processamento.total_funcionarios}, "
            f"divergencias={processamento.total_divergencias}"
        )
        
        # Convert divergencias to Pydantic models
        divergencias_models = [
            FuncionarioDivergencia(
                nome_funcionario=div["nome_funcionario"],
                tipo_divergencia=div["tipo_divergencia"],
                descricao_divergencia=div["descricao_divergencia"],
                valor_encontrado=div.get("valor_encontrado"),
                valor_esperado=div.get("valor_esperado"),
                campo_afetado=div["campo_afetado"]
            ) for div in divergencias
        ]
        
        return ProcessamentoFolhaResponse(
            id=processamento.id,
            empresa_id=processamento.empresa_id,
            mes=processamento.mes,
            ano=processamento.ano,
            arquivo_pdf=processamento.arquivo_pdf,
            total_funcionarios=processamento.total_funcionarios,
            total_divergencias=processamento.total_divergencias,
            status_processamento=processamento.status_processamento,
            criado_em=processamento.criado_em,
            concluido_em=processamento.concluido_em,
            divergencias=divergencias_models
        )

    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to process payroll audit: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Erro no processamento da auditoria: {str(e)}"
        )


@app.get("/v1/folha/processamentos/{empresa_id}", response_model=List[ProcessamentoFolhaResponse], tags=["folha-pagamento"])
def listar_processamentos_folha(
    empresa_id: int, 
    db: Session = Depends(get_db)
):
    """
    Listar hist√≥rico de processamentos de folha de pagamento de uma empresa
    """
    try:
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa n√£o encontrada")
        
        processamentos = (
            db.query(ProcessamentosFolhaDB)
            .filter(ProcessamentosFolhaDB.empresa_id == empresa_id)
            .order_by(desc(ProcessamentosFolhaDB.criado_em))
            .all()
        )
        
        result = []
        for proc in processamentos:
            # Parse divergencias from JSON
            divergencias_json = json.loads(proc.relatorio_divergencias or "[]")
            divergencias_models = [
                FuncionarioDivergencia(
                    nome_funcionario=div["nome_funcionario"],
                    tipo_divergencia=div["tipo_divergencia"],
                    descricao_divergencia=div["descricao_divergencia"],
                    valor_encontrado=div.get("valor_encontrado"),
                    valor_esperado=div.get("valor_esperado"),
                    campo_afetado=div["campo_afetado"]
                ) for div in divergencias_json
            ]
            
            result.append(ProcessamentoFolhaResponse(
                id=proc.id,
                empresa_id=proc.empresa_id,
                mes=proc.mes,
                ano=proc.ano,
                arquivo_pdf=proc.arquivo_pdf,
                total_funcionarios=proc.total_funcionarios,
                total_divergencias=proc.total_divergencias,
                status_processamento=proc.status_processamento,
                criado_em=proc.criado_em,
                concluido_em=proc.concluido_em,
                divergencias=divergencias_models
            ))
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to list payroll processings: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao listar processamentos: {str(e)}"
        )


# ===== CCT MANAGEMENT ENDPOINTS =====

@app.get("/v1/sindicatos", response_model=List[Sindicato], tags=["cct"])
def listar_sindicatos(db: Session = Depends(get_db)):
    """
    Listar todos os sindicatos cadastrados
    """
    try:
        sindicatos = db.query(SindicatoDB).all()
        return [Sindicato.model_validate(s) for s in sindicatos]
    except Exception as e:
        logger.error(f"Failed to list sindicatos: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao listar sindicatos: {str(e)}"
        )


@app.post("/v1/sindicatos", response_model=Sindicato, tags=["cct"])
def criar_sindicato(sindicato: SindicatoCreate, db: Session = Depends(get_db)):
    """
    Criar um novo sindicato
    """
    try:
        db_sindicato = SindicatoDB(
            nome_sindicato=sindicato.nome_sindicato,
            cnpj=sindicato.cnpj,
            base_territorial=sindicato.base_territorial,
            categoria_representada=sindicato.categoria_representada
        )
        
        db.add(db_sindicato)
        db.commit()
        db.refresh(db_sindicato)
        
        logger.info(f"Sindicato created: {db_sindicato.id} - {db_sindicato.nome_sindicato}")
        return Sindicato.model_validate(db_sindicato)
        
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create sindicato: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao criar sindicato: {str(e)}"
        )


@app.get("/v1/cct", response_model=CCTListResponse, tags=["cct"]) 
def listar_ccts(
    sindicato_id: Optional[int] = Query(None, description="Filtrar por sindicato"),
    vigente: Optional[bool] = Query(None, description="Filtrar por vig√™ncia (true=ativo, false=expirado)"),
    search_text: Optional[str] = Query(None, description="Buscar no nome do sindicato ou n√∫mero de registro"),
    db: Session = Depends(get_db)
):
    """
    Listar CCTs com filtros avan√ßados
    Implementa a funcionalidade de "biblioteca digital" com busca poderosa
    """
    try:
        from datetime import date as current_date_import
        from sqlalchemy import and_, or_
        
        today = current_date_import.today()
        
        # Base query with join to get syndicate info
        query = db.query(ConvencaoColetivaCCTDB).join(SindicatoDB)
        
        # Apply filters
        if sindicato_id:
            query = query.filter(ConvencaoColetivaCCTDB.sindicato_id == sindicato_id)
        
        if vigente is not None:
            if vigente:  # Active CCTs
                query = query.filter(
                    and_(
                        ConvencaoColetivaCCTDB.vigencia_inicio <= today,
                        ConvencaoColetivaCCTDB.vigencia_fim >= today
                    )
                )
            else:  # Expired CCTs
                query = query.filter(ConvencaoColetivaCCTDB.vigencia_fim < today)
        
        if search_text:
            search_filter = or_(
                SindicatoDB.nome_sindicato.ilike(f"%{search_text}%"),
                ConvencaoColetivaCCTDB.numero_registro_mte.ilike(f"%{search_text}%")
            )
            query = query.filter(search_filter)
        
        # Get all CCTs
        all_ccts = query.all()
        ccts = [ConvencaoColetivaCCT.model_validate(cct) for cct in all_ccts]
        
        # Calculate statistics
        total = len(ccts)
        ativas = sum(1 for cct in all_ccts if cct.vigencia_inicio <= today <= cct.vigencia_fim)
        expiradas = sum(1 for cct in all_ccts if cct.vigencia_fim < today)
        
        # CCTs expiring in 30 days
        from datetime import timedelta
        thirty_days_from_now = today + timedelta(days=30)
        expirando_30_dias = sum(
            1 for cct in all_ccts 
            if cct.vigencia_fim >= today and cct.vigencia_fim <= thirty_days_from_now
        )
        
        return CCTListResponse(
            ccts=ccts,
            total=total,
            ativas=ativas,
            expiradas=expiradas,
            expirando_30_dias=expirando_30_dias
        )
        
    except Exception as e:
        logger.error(f"Failed to list CCTs: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao listar CCTs: {str(e)}"
        )


@app.post("/v1/cct", response_model=ConvencaoColetivaCCT, tags=["cct"])
def criar_cct(cct: ConvencaoColetivaCCTCreate, db: Session = Depends(get_db)):
    """
    Criar uma nova CCT (Conven√ß√£o Coletiva de Trabalho)
    """
    try:
        # Verify syndicate exists
        sindicato = db.query(SindicatoDB).filter(SindicatoDB.id == cct.sindicato_id).first()
        if not sindicato:
            raise HTTPException(status_code=404, detail="Sindicato n√£o encontrado")
        
        db_cct = ConvencaoColetivaCCTDB(
            sindicato_id=cct.sindicato_id,
            numero_registro_mte=cct.numero_registro_mte,
            vigencia_inicio=cct.vigencia_inicio,
            vigencia_fim=cct.vigencia_fim,
            link_documento_oficial=cct.link_documento_oficial,
            dados_cct=cct.dados_cct
        )
        
        db.add(db_cct)
        db.commit()
        db.refresh(db_cct)
        
        logger.info(f"CCT created: {db_cct.id} for sindicato {db_cct.sindicato_id}")
        return ConvencaoColetivaCCT.model_validate(db_cct)
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create CCT: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao criar CCT: {str(e)}"
        )


@app.get("/v1/cct/{cct_id}", response_model=ConvencaoColetivaCCT, tags=["cct"])
def obter_cct(cct_id: int, db: Session = Depends(get_db)):
    """
    Obter detalhes de uma CCT espec√≠fica
    """
    cct = db.query(ConvencaoColetivaCCTDB).filter(ConvencaoColetivaCCTDB.id == cct_id).first()
    if not cct:
        raise HTTPException(status_code=404, detail="CCT n√£o encontrada")
    
    return ConvencaoColetivaCCT.model_validate(cct)


# ===== LEGISLATION MANAGEMENT ENDPOINTS =====

@app.get("/v1/legislacao", response_model=List[LegislacaoDocumento], tags=["legislacao"])
def listar_legislacao(
    tipo_documento: Optional[TipoDocumento] = Query(None, description="Filtrar por tipo"),
    status: Optional[StatusProcessamento] = Query(None, description="Filtrar por status"),
    search_text: Optional[str] = Query(None, description="Buscar no t√≠tulo ou n√∫mero"),
    db: Session = Depends(get_db)
):
    """
    Listar documentos de legisla√ß√£o com filtros
    """
    try:
        query = db.query(LegislacaoDocumentoDB)
        
        # Apply filters
        if tipo_documento:
            query = query.filter(LegislacaoDocumentoDB.tipo_documento == tipo_documento.value)
        
        if status:
            query = query.filter(LegislacaoDocumentoDB.status_processamento == status.value)
        
        if search_text:
            search_filter = or_(
                LegislacaoDocumentoDB.titulo.ilike(f"%{search_text}%"),
                LegislacaoDocumentoDB.numero_documento.ilike(f"%{search_text}%")
            )
            query = query.filter(search_filter)
        
        documentos = query.order_by(desc(LegislacaoDocumentoDB.criado_em)).all()
        return [LegislacaoDocumento.model_validate(doc) for doc in documentos]
        
    except Exception as e:
        logger.error(f"Failed to list legislation: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao listar legisla√ß√£o: {str(e)}"
        )


@app.post("/v1/legislacao", response_model=LegislacaoDocumento, tags=["legislacao"])
def criar_documento_legislacao(documento: LegislacaoDocumentoCreate, db: Session = Depends(get_db)):
    """
    Cadastrar um novo documento de legisla√ß√£o manualmente
    """
    try:
        db_documento = LegislacaoDocumentoDB(
            titulo=documento.titulo,
            tipo_documento=documento.tipo_documento.value,
            numero_documento=documento.numero_documento,
            data_publicacao=documento.data_publicacao,
            orgao_emissor=documento.orgao_emissor,
            status_processamento="pendente"
        )
        
        db.add(db_documento)
        db.commit()
        db.refresh(db_documento)
        
        logger.info(f"Legislation document created: {db_documento.id} - {db_documento.titulo}")
        return LegislacaoDocumento.model_validate(db_documento)
        
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create legislation document: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao criar documento: {str(e)}"
        )


@app.post("/v1/legislacao/extrair-pdf", response_model=ExtrairPDFResponse, tags=["legislacao"])
async def extrair_pdf_legislacao(
    arquivo_pdf: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    üß† MOTOR DE EXTRA√á√ÉO INTELIGENTE DE PDFs
    
    O "Leitor Inteligente" que transforma PDFs de legisla√ß√£o em conhecimento estruturado:
    1. Rece√ß√£o e Classifica√ß√£o do documento
    2. Extra√ß√£o Dirigida com IA especializada 
    3. Estrutura√ß√£o dos dados em formato JSON utiliz√°vel
    
    Este √© o cora√ß√£o da transforma√ß√£o de arquivos est√°ticos em base de conhecimento ativa.
    """
    try:
        import time
        start_time = time.time()
        
        # Validate file
        if not arquivo_pdf.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Apenas arquivos PDF s√£o aceitos")
        
        # Read PDF content
        pdf_content = await arquivo_pdf.read()
        if len(pdf_content) == 0:
            raise HTTPException(status_code=400, detail="Arquivo PDF est√° vazio")
        
        # Create document record
        db_documento = LegislacaoDocumentoDB(
            titulo=f"Documento PDF: {arquivo_pdf.filename}",
            tipo_documento="lei",  # Default, will be updated by AI
            status_processamento="processando",
            arquivo_pdf=arquivo_pdf.filename
        )
        
        db.add(db_documento)
        db.flush()  # Get ID without committing
        
        # ü§ñ AI Processing - Extract structured data from PDF
        dados_extraidos, confidence_score, sugestoes = await processar_pdf_legislacao_com_ia(
            pdf_content, arquivo_pdf.filename, db
        )
        
        # Update document with results
        db_documento.dados_extraidos = dados_extraidos
        db_documento.status_processamento = "concluido"
        db_documento.processado_em = datetime.now(timezone.utc)
        
        # Update document type and title if AI could identify them
        if dados_extraidos.get("tipo_documento"):
            db_documento.tipo_documento = dados_extraidos["tipo_documento"]
        
        if dados_extraidos.get("titulo"):
            db_documento.titulo = dados_extraidos["titulo"]
        
        if dados_extraidos.get("numero_documento"):
            db_documento.numero_documento = dados_extraidos["numero_documento"]
        
        if dados_extraidos.get("data_publicacao"):
            try:
                from datetime import datetime as dt
                db_documento.data_publicacao = dt.fromisoformat(dados_extraidos["data_publicacao"]).date()
            except (ValueError, TypeError):
                pass  # Keep original date
        
        db.commit()
        db.refresh(db_documento)
        
        processing_time = time.time() - start_time
        
        logger.info(
            f"PDF extraction completed: documento_id={db_documento.id}, "
            f"confidence={confidence_score:.2f}, time={processing_time:.2f}s"
        )
        
        return ExtrairPDFResponse(
            documento_id=db_documento.id,
            dados_extraidos=dados_extraidos,
            confidence_score=confidence_score,
            processamento_tempo_segundos=processing_time,
            sugestoes_validacao=sugestoes
        )
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to process PDF: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro no processamento do PDF: {str(e)}"
        )


async def processar_pdf_legislacao_com_ia(pdf_content: bytes, filename: str, db: Session):
    """
    üß† AI-Powered PDF Processing for Legislation
    
    This is where the intelligent document processing happens:
    1. Document Classification (CCT, Lei, Decreto, etc.)
    2. Directed Extraction based on document type
    3. Structured Data Generation
    
    For now, returns sophisticated mock data to demonstrate the functionality.
    In production, this would integrate with OCR/AI services.
    """
    import asyncio
    import random
    
    # Simulate AI processing time
    await asyncio.sleep(1.0)
    
    # Analyze filename to guess document type
    filename_lower = filename.lower()
    
    if "cct" in filename_lower or "convencao" in filename_lower or "coletiva" in filename_lower:
        documento_tipo = "cct"
    elif "decreto" in filename_lower:
        documento_tipo = "decreto"
    elif "lei" in filename_lower:
        documento_tipo = "lei"
    elif "portaria" in filename_lower:
        documento_tipo = "portaria"
    else:
        documento_tipo = "lei"  # Default
    
    # Generate structured data based on document type
    if documento_tipo == "cct":
        dados_extraidos = {
            "tipo_documento": "cct",
            "titulo": "Conven√ß√£o Coletiva de Trabalho - Setor Comerci√°rio",
            "numero_documento": "CCT-2024-001",
            "data_publicacao": "2024-01-15",
            "vigencia_inicio": "2024-01-01",
            "vigencia_fim": "2024-12-31",
            "sindicato_empregadores": "Sindicato do Com√©rcio Varejista",
            "sindicato_trabalhadores": "Sindicato dos Comerci√°rios", 
            "principais_clausulas": {
                "piso_salarial": 1850.00,
                "vale_refeicao": 25.00,
                "vale_transporte": "6% do sal√°rio",
                "horas_extras": {
                    "50%": "Duas primeiras horas extras",
                    "100%": "A partir da terceira hora extra"
                },
                "adicional_noturno": "25% sobre a hora normal",
                "ferias": "30 dias + 1/3 constitucional",
                "licenca_paternidade": "15 dias"
            },
            "categorias_abrangidas": [
                "Vendedores",
                "Caixas", 
                "Supervisores",
                "Gerentes de loja"
            ]
        }
        confidence_score = 0.92
        sugestoes = [
            "Verificar se o piso salarial est√° atualizado com o √∫ltimo reajuste",
            "Confirmar se a vig√™ncia est√° correta",
            "Validar percentuais de horas extras"
        ]
        
    elif documento_tipo == "decreto":
        dados_extraidos = {
            "tipo_documento": "decreto",
            "titulo": "Decreto n¬∫ 11.072 - Regulamenta disposi√ß√µes sobre trabalho remoto",
            "numero_documento": "Decreto 11.072/2022",
            "data_publicacao": "2024-02-10",
            "orgao_emissor": "Presid√™ncia da Rep√∫blica",
            "ementa": "Regulamenta as disposi√ß√µes sobre o teletrabalho e trabalho remoto",
            "artigos_principais": [
                {
                    "artigo": "Art. 1¬∫",
                    "conteudo": "Este Decreto regulamenta as disposi√ß√µes sobre teletrabalho"
                },
                {
                    "artigo": "Art. 2¬∫", 
                    "conteudo": "Considera-se teletrabalho a presta√ß√£o de servi√ßos preponderantemente fora das depend√™ncias do empregador"
                }
            ],
            "areas_impactadas": [
                "Direito do Trabalho",
                "Recursos Humanos",
                "Gest√£o de Pessoas"
            ]
        }
        confidence_score = 0.88
        sugestoes = [
            "Confirmar n√∫mero do decreto oficial",
            "Verificar data de publica√ß√£o no DOU",
            "Validar artigos extra√≠dos"
        ]
        
    else:  # lei or other
        dados_extraidos = {
            "tipo_documento": "lei",
            "titulo": "Lei n¬∫ 14.442 - Reforma das normas trabalhistas",
            "numero_documento": "Lei 14.442/2022",
            "data_publicacao": "2024-01-20",
            "orgao_emissor": "Congresso Nacional",
            "ementa": "Altera dispositivos da Consolida√ß√£o das Leis do Trabalho",
            "principais_alteracoes": [
                "Regulamenta√ß√£o do teletrabalho",
                "Novas regras para contratos tempor√°rios",
                "Modifica√ß√µes na jornada de trabalho"
            ],
            "leis_alteradas": [
                "CLT - Consolida√ß√£o das Leis do Trabalho"
            ],
            "areas_impactadas": [
                "Direito do Trabalho",
                "Compliance Trabalhista",
                "Rela√ß√µes Trabalhistas"
            ]
        }
        confidence_score = 0.85
        sugestoes = [
            "Verificar n√∫mero oficial da lei",
            "Confirmar principais altera√ß√µes",
            "Validar impacto nas empresas clientes"
        ]
    
    # Add metadata about processing
    dados_extraidos["metadata_processamento"] = {
        "arquivo_original": filename,
        "tamanho_bytes": len(pdf_content),
        "processado_em": datetime.now(timezone.utc).isoformat(),
        "engine_versao": "AUDITORIA360-AI-v1.0",
        "confidence_geral": confidence_score
    }
    
    return dados_extraidos, confidence_score, sugestoes


async def processar_pdf_com_ia(pdf_content: bytes, empresa: EmpresaDB, mes: int, ano: int, db: Session):
    """
    AI-powered PDF processing and auditing logic
    
    This is where the real magic happens:
    1. Extract data from PDF using OCR/Document AI
    2. Structure the data (employees, earnings, deductions)
    3. Load applicable CCT rules for the company
    4. Audit each employee's data against the rules
    5. Generate divergence reports
    
    For now, this returns mock data to demonstrate the functionality.
    """
    import asyncio
    
    # Simulate AI processing time
    await asyncio.sleep(0.5)
    
    # Mock extracted data (in production this would come from AI/OCR)
    dados_extraidos = {
        "arquivo_processado": f"folha_{mes:02d}_{ano}.pdf",
        "data_processamento": datetime.now(timezone.utc).isoformat(),
        "funcionarios": [
            {
                "nome": "Jo√£o Silva",
                "cargo": "Vendedor",
                "salario_base": 1800.00,
                "horas_extras": {"50%": 120.00, "100%": 0.00},
                "descontos": {"INSS": 198.00, "IRRF": 0.00},
                "liquido": 1722.00
            },
            {
                "nome": "Maria Oliveira", 
                "cargo": "Gerente",
                "salario_base": 3500.00,
                "horas_extras": {"50%": 250.00, "100%": 100.00},
                "descontos": {"INSS": 445.50, "IRRF": 180.25},
                "liquido": 3224.25
            },
            {
                "nome": "Carlos Santos",
                "cargo": "Vendedor",
                "salario_base": 1850.00,
                "horas_extras": {"50%": 0.00, "100%": 0.00},
                "descontos": {"INSS": 203.50, "IRRF": 0.00},
                "liquido": 1646.50
            }
        ]
    }
    
    # Mock CCT rules (in production this would come from database)
    regras_cct = {
        "piso_salarial": 1850.00,
        "percentual_he_50": 60.0,  # 60% instead of 50%
        "percentual_he_100": 100.0,
        "auxilio_creche": 150.00,
        "vale_transporte": 6.0  # 6% do sal√°rio base
    }
    
    # Mock audit logic - compare extracted data against CCT rules
    divergencias = []
    
    for funcionario in dados_extraidos["funcionarios"]:
        nome = funcionario["nome"]
        
        # Check minimum wage
        if funcionario["salario_base"] < regras_cct["piso_salarial"]:
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "ALERTA",
                "descricao_divergencia": f"Sal√°rio base (R$ {funcionario['salario_base']:,.2f}) est√° abaixo do piso da CCT",
                "valor_encontrado": f"R$ {funcionario['salario_base']:,.2f}",
                "valor_esperado": f"R$ {regras_cct['piso_salarial']:,.2f}",
                "campo_afetado": "salario_base"
            })
        
        # Check overtime calculation (mock - just for Jo√£o Silva)
        if nome == "Jo√£o Silva" and funcionario["horas_extras"]["50%"] > 0:
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "AVISO",
                "descricao_divergencia": "Valor da hora extra (50%) pode n√£o corresponder ao valor da CCT (60%)",
                "valor_encontrado": f"R$ {funcionario['horas_extras']['50%']:,.2f}",
                "valor_esperado": "Verificar c√°lculo com 60%",
                "campo_afetado": "horas_extras_50"
            })
        
        # Check missing benefits (mock - for all employees)
        if nome != "Carlos Santos":  # Simulate Carlos has it, others don't
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "INFO",
                "descricao_divergencia": f"Benef√≠cio 'Aux√≠lio Creche' previsto na CCT n√£o foi encontrado na folha",
                "valor_encontrado": None,
                "valor_esperado": f"R$ {regras_cct['auxilio_creche']:,.2f}",
                "campo_afetado": "auxilio_creche"
            })
    
    return dados_extraidos, divergencias


# ===== RISK ANALYSIS ENDPOINTS =====

@app.post("/v1/riscos/analisar", response_model=AnaliseRiscoResponse, tags=["riscos"])
async def analisar_riscos_empresa(
    request: AnaliseRiscoRequest,
    db: Session = Depends(get_db)
):
    """
    üîÆ CONSULTOR DE RISCOS - An√°lise Preditiva Completa
    
    Motor de an√°lise que atua como um "or√°culo estrat√©gico":
    1. Agrega dados hist√≥ricos da empresa (12-24 meses)
    2. Executa an√°lise de conformidade trabalhista, fiscal e operacional
    3. Aplica algoritmos de detec√ß√£o de padr√µes e anomalias
    4. Gera score de risco (0-100) e recomenda√ß√µes espec√≠ficas
    5. Armazena hist√≥rico para an√°lise de evolu√ß√£o temporal
    
    Esta funcionalidade transforma contabilidade reativa em consultoria proativa.
    """
    try:
        # Verify company exists and get details
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == request.empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa n√£o encontrada")
        
        contabilidade = db.query(ContabilidadeDB).filter(ContabilidadeDB.id == empresa.contabilidade_id).first()
        
        logger.info(f"üîç Starting risk analysis for company: {empresa.nome} (ID: {request.empresa_id})")
        
        # Execute comprehensive risk analysis
        resultado_analise = await executar_analise_completa_riscos(
            empresa=empresa, 
            contabilidade=contabilidade,
            db=db
        )
        
        # Get previous analysis for comparison
        analise_anterior = (
            db.query(HistoricoAnalisesRiscoDB)
            .filter(HistoricoAnalisesRiscoDB.empresa_id == request.empresa_id)
            .order_by(desc(HistoricoAnalisesRiscoDB.analisado_em))
            .first()
        )
        
        score_anterior = analise_anterior.score_risco if analise_anterior else None
        variacao_score = (resultado_analise["score_risco"] - score_anterior) if score_anterior else None
        
        # Store analysis in history
        nova_analise = HistoricoAnalisesRiscoDB(
            empresa_id=request.empresa_id,
            contabilidade_id=empresa.contabilidade_id,
            score_risco=resultado_analise["score_risco"],
            relatorio_completo=json.dumps(resultado_analise["relatorio_completo"], ensure_ascii=False),
            analisado_em=datetime.now(timezone.utc),
            analisado_por_user_id="sistema"  # TODO: Get from auth context
        )
        
        db.add(nova_analise)
        db.commit()
        db.refresh(nova_analise)
        
        # Build response
        response = AnaliseRiscoResponse(
            empresa_id=request.empresa_id,
            empresa_nome=empresa.nome,
            score_risco=resultado_analise["score_risco"],
            nivel_risco=resultado_analise["nivel_risco"],
            data_analise=datetime.now(timezone.utc),
            progresso_analise=resultado_analise["progresso_analise"],
            riscos_encontrados=resultado_analise["riscos_encontrados"],
            total_riscos=resultado_analise["total_riscos"],
            riscos_criticos=resultado_analise["riscos_criticos"],
            riscos_altos=resultado_analise["riscos_altos"],
            riscos_medios=resultado_analise["riscos_medios"],
            riscos_baixos=resultado_analise["riscos_baixos"],
            score_anterior=score_anterior,
            variacao_score=variacao_score
        )
        
        logger.info(
            f"‚úÖ Risk analysis completed for {empresa.nome}: "
            f"Score={resultado_analise['score_risco']}/100, "
            f"Level={resultado_analise['nivel_risco']}, "
            f"Total Risks={resultado_analise['total_riscos']}"
        )
        
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to analyze risks for empresa_id {request.empresa_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro na an√°lise de riscos: {str(e)}"
        )


@app.get("/v1/riscos/historico/{empresa_id}", response_model=List[HistoricoAnaliseRisco], tags=["riscos"])
def obter_historico_riscos(
    empresa_id: int,
    limit: int = Query(10, ge=1, le=50, description="N√∫mero m√°ximo de an√°lises"),
    db: Session = Depends(get_db)
):
    """
    üìä Hist√≥rico de An√°lises de Risco
    
    Recupera o hist√≥rico de an√°lises de risco de uma empresa para:
    - Acompanhar evolu√ß√£o do score de risco
    - Identificar tend√™ncias
    - Avaliar efetividade das a√ß√µes implementadas
    """
    try:
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa n√£o encontrada")
        
        # Get historical analyses
        historico = (
            db.query(HistoricoAnalisesRiscoDB)
            .filter(HistoricoAnalisesRiscoDB.empresa_id == empresa_id)
            .order_by(desc(HistoricoAnalisesRiscoDB.analisado_em))
            .limit(limit)
            .all()
        )
        
        result = []
        for analise in historico:
            try:
                relatorio_resumo = json.loads(analise.relatorio_completo or "{}")
            except (json.JSONDecodeError, TypeError):
                relatorio_resumo = {}
            
            result.append(HistoricoAnaliseRisco(
                id=analise.id,
                empresa_id=analise.empresa_id,
                contabilidade_id=analise.contabilidade_id,
                score_risco=analise.score_risco,
                data_analise=analise.analisado_em,
                relatorio_resumo={
                    "total_riscos": relatorio_resumo.get("total_riscos", 0),
                    "nivel_risco": relatorio_resumo.get("nivel_risco", "DESCONHECIDO"),
                    "categorias": relatorio_resumo.get("categorias_resumo", {})
                }
            ))
        
        logger.info(f"Retrieved {len(result)} historical risk analyses for empresa_id {empresa_id}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get risk history for empresa_id {empresa_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao obter hist√≥rico de riscos: {str(e)}"
        )


async def executar_analise_completa_riscos(
    empresa: EmpresaDB, 
    contabilidade: ContabilidadeDB,
    db: Session
) -> dict:
    """
    üß† CORE DA AN√ÅLISE DE RISCOS
    
    Esta fun√ß√£o implementa o "c√©rebro" do Consultor de Riscos:
    1. Agrega√ß√£o massiva de dados hist√≥ricos
    2. Motor de regras de conformidade
    3. An√°lise de padr√µes e anomalias
    4. Quantifica√ß√£o e classifica√ß√£o de riscos
    5. Gera√ß√£o de recomenda√ß√µes espec√≠ficas
    
    Simula processos que na produ√ß√£o usariam:
    - An√°lise de dados dos √∫ltimos 12-24 meses
    - Consulta a bases de CCTs e legisla√ß√£o
    - Algoritmos de ML para detec√ß√£o de anomalias
    - Sistema especialista para recomenda√ß√µes
    """
    import asyncio
    from datetime import datetime, timedelta, timezone
    
    # Simulate analysis progress
    progresso_analise = {}
    
    # Step 1: Data Aggregation (simulate time-consuming process)
    progresso_analise["coleta_dados"] = "CONCLU√çDO"
    await asyncio.sleep(0.2)
    
    # Get historical data (last 12 months)
    data_limite = datetime.now(timezone.utc) - timedelta(days=365)
    
    controles_historicos = (
        db.query(ControleMensalDB)
        .filter(ControleMensalDB.empresa_id == empresa.id)
        .filter(ControleMensalDB.criado_em >= data_limite)
        .count()
    )
    
    processamentos_folha = (
        db.query(ProcessamentosFolhaDB)
        .filter(ProcessamentosFolhaDB.empresa_id == empresa.id)
        .filter(ProcessamentosFolhaDB.criado_em >= data_limite)
        .count()
    )
    
    progresso_analise["analise_conformidade_trabalhista"] = "CONCLU√çDO"
    await asyncio.sleep(0.2)
    
    progresso_analise["analise_conformidade_fiscal"] = "CONCLU√çDO"
    await asyncio.sleep(0.2)
    
    progresso_analise["deteccao_anomalias"] = "CONCLU√çDO"  
    await asyncio.sleep(0.2)
    
    progresso_analise["calculo_score"] = "CONCLU√çDO"
    await asyncio.sleep(0.1)
    
    # Risk Analysis Engine - Simulation with realistic business logic
    riscos_encontrados = []
    
    # 1. TRABALHISTA RISKS
    if controles_historicos < 6:  # Less than 6 months of controls
        riscos_encontrados.append(RiscoDetalhado(
            categoria="TRABALHISTA",
            tipo_risco="Hist√≥rico de Controles Insuficiente",
            descricao="Empresa possui poucos controles mensais registrados no sistema",
            evidencia=f"Apenas {controles_historicos} controles encontrados nos √∫ltimos 12 meses",
            impacto_potencial="Risco de n√£o conformidade com obriga√ß√µes trabalhistas. Poss√≠vel multa de R$ 2.000 a R$ 20.000 por irregularidade n√£o detectada.",
            plano_acao="1. Implementar controles mensais sistem√°ticos. 2. Revisar processos de documenta√ß√£o. 3. Treinar equipe em boas pr√°ticas.",
            severidade=4  # High severity
        ))
    
    # Simulate payroll compliance check
    if processamentos_folha == 0:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="TRABALHISTA", 
            tipo_risco="Auditoria de Folha Pendente",
            descricao="Nenhuma auditoria de folha foi realizada recentemente",
            evidencia="Sistema n√£o registra processamentos de auditoria da folha nos √∫ltimos 12 meses",
            impacto_potencial="Riscos n√£o identificados em c√°lculos trabalhistas. Estimativa de exposi√ß√£o: R$ 5.000 a R$ 50.000 em poss√≠veis corre√ß√µes.",
            plano_acao="1. Realizar auditoria completa da folha de pagamento. 2. Implementar auditorias mensais autom√°ticas. 3. Validar c√°lculos contra CCT vigente.",
            severidade=3  # Medium-high severity
        ))
    
    # 2. FISCAL RISKS  
    # Simulate tax compliance analysis with safe datetime handling
    empresa_criada = getattr(empresa, 'criado_em', None)
    if empresa_criada is None:
        # Fallback: assume company created 400 days ago if no creation date
        dias_funcionamento = 400
    else:
        # Make sure both datetimes have timezone info
        now = datetime.now(timezone.utc)
        if empresa_criada.tzinfo is None:
            # If empresa_criada is timezone-naive, assume it's UTC
            empresa_criada = empresa_criada.replace(tzinfo=timezone.utc)
        dias_funcionamento = (now - empresa_criada).days
    
    if dias_funcionamento > 365 and controles_historicos < 12:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="FISCAL",
            tipo_risco="Controle Fiscal Inconsistente", 
            descricao="Empresa com mais de 1 ano de funcionamento apresenta gaps no controle mensal",
            evidencia=f"Empresa ativa h√° {dias_funcionamento} dias mas apenas {controles_historicos} controles registrados",
            impacto_potencial="Risco de autua√ß√£o fiscal por falta de documenta√ß√£o. Multas podem variar de 20% a 75% do tributo devido.",
            plano_acao="1. Regularizar controles em atraso. 2. Implementar rotina de compliance fiscal mensal. 3. Revisar regime tribut√°rio.",
            severidade=5  # Critical severity
        ))
    
    # 3. OPERATIONAL RISKS
    # Simulate operational efficiency analysis
    if controles_historicos > 0 and controles_historicos < 8:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="OPERACIONAL",
            tipo_risco="Efici√™ncia Operacional Sub√≥tima",
            descricao="Padr√£o irregular nos controles mensais indica poss√≠veis problemas operacionais",
            evidencia=f"Varia√ß√£o significativa na frequ√™ncia de controles: {controles_historicos} em 12 meses",
            impacto_potencial="Inefici√™ncia operacional pode resultar em aumento de 15-30% nos custos de compliance.",
            plano_acao="1. Padronizar processos operacionais. 2. Implementar automa√ß√µes. 3. Treinar equipe em ferramentas do sistema.",
            severidade=2  # Low-medium severity
        ))
    
    # Add some positive findings for companies with good practices
    if controles_historicos >= 10 and processamentos_folha > 0:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="CONFORMIDADE",
            tipo_risco="Pr√°ticas de Compliance Adequadas",
            descricao="Empresa demonstra boas pr√°ticas de controle e auditoria",
            evidencia=f"Registros regulares: {controles_historicos} controles e {processamentos_folha} auditorias de folha",
            impacto_potencial="Redu√ß√£o significativa do risco de n√£o conformidade. Potencial economia de 40-60% em multas evitadas.",
            plano_acao="1. Manter rotina estabelecida. 2. Considerar automa√ß√µes adicionais. 3. Usar como benchmark para outras empresas.",
            severidade=1  # Low risk (positive finding)
        ))
    
    # Calculate risk score based on found risks
    total_riscos = len(riscos_encontrados)
    riscos_criticos = sum(1 for r in riscos_encontrados if r.severidade == 5)
    riscos_altos = sum(1 for r in riscos_encontrados if r.severidade == 4) 
    riscos_medios = sum(1 for r in riscos_encontrados if r.severidade == 3)
    riscos_baixos = sum(1 for r in riscos_encontrados if r.severidade <= 2)
    
    # Scoring algorithm: Base score starts at 100 (perfect), subtract points for risks
    score_risco = 100
    score_risco -= (riscos_criticos * 25)  # Critical risks: -25 points each
    score_risco -= (riscos_altos * 15)     # High risks: -15 points each  
    score_risco -= (riscos_medios * 10)    # Medium risks: -10 points each
    score_risco -= (riscos_baixos * 5)     # Low risks: -5 points each (but positive findings add back)
    
    # Add bonus points for positive compliance findings
    bonus_compliance = sum(5 for r in riscos_encontrados if r.categoria == "CONFORMIDADE")
    score_risco = min(100, max(0, score_risco + bonus_compliance))  # Keep between 0-100
    
    # Determine risk level based on score
    if score_risco >= 80:
        nivel_risco = "BAIXO"
    elif score_risco >= 60:
        nivel_risco = "M√âDIO" 
    elif score_risco >= 40:
        nivel_risco = "ALTO"
    else:
        nivel_risco = "CR√çTICO"
    
    # Build comprehensive report
    relatorio_completo = {
        "analise_id": f"RISK_{empresa.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "empresa_dados": {
            "id": empresa.id,
            "nome": empresa.nome,
            "contabilidade_id": empresa.contabilidade_id,
            "dias_funcionamento": dias_funcionamento
        },
        "dados_historicos": {
            "controles_mensais_12m": controles_historicos,
            "auditorias_folha_12m": processamentos_folha,
            "periodo_analise": "12 meses"
        },
        "score_risco": score_risco,
        "nivel_risco": nivel_risco,
        "total_riscos": total_riscos,
        "riscos_criticos": riscos_criticos,
        "riscos_altos": riscos_altos,
        "riscos_medios": riscos_medios,
        "riscos_baixos": riscos_baixos,
        "categorias_resumo": {
            "TRABALHISTA": sum(1 for r in riscos_encontrados if r.categoria == "TRABALHISTA"),
            "FISCAL": sum(1 for r in riscos_encontrados if r.categoria == "FISCAL"),
            "OPERACIONAL": sum(1 for r in riscos_encontrados if r.categoria == "OPERACIONAL"),
            "CONFORMIDADE": sum(1 for r in riscos_encontrados if r.categoria == "CONFORMIDADE")
        },
        "algoritmo_versao": "v1.0",
        "processado_em": datetime.now(timezone.utc).isoformat()
    }
    
    return {
        "score_risco": score_risco,
        "nivel_risco": nivel_risco,
        "progresso_analise": progresso_analise,
        "riscos_encontrados": riscos_encontrados,
        "total_riscos": total_riscos,
        "riscos_criticos": riscos_criticos,
        "riscos_altos": riscos_altos,
        "riscos_medios": riscos_medios,
        "riscos_baixos": riscos_baixos,
        "relatorio_completo": relatorio_completo
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001)
