"""
Portal Demandas FastAPI Application
Comprehensive API for managing demands/tickets with SQLAlchemy + Neon PostgreSQL
"""

import logging
import json
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from typing import List, Optional

from fastapi import Depends, FastAPI, HTTPException, Query, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import asc, desc, or_
from sqlalchemy.orm import Session

from portal_demandas.db import TicketComment as TicketCommentDB
from portal_demandas.db import (
    TicketDB,
    ContabilidadeDB,
    EmpresaDB, 
    ControleMensalDB,
    TarefaControleDB,
    TemplateControleDB,
    TemplateControleTarefaDB,
    ProcessamentosFolhaDB,
    HistoricoAnalisesRiscoDB,
    get_db,
    init_portal_db,
)
from portal_demandas.models import (
    Ticket,
    TicketCategoria,
    TicketComment,
    TicketCreate,
    TicketListResponse,
    TicketPrioridade,
    TicketStats,
    TicketStatus,
    TicketUpdate,
    # Controle Mensal models
    ControleMensalDetalhado,
    ControleMensalResponse,
    ControleMensalSumario,
    Tarefa,
    TemplateControle,
    TemplateControleCreate,
    TemplateAplicacao,
    # Payroll Audit models
    FuncionarioDivergencia,
    ProcessamentoFolhaResponse,
    AuditoriaFolhaRequest,
    # Risk Analysis models
    RiscoDetalhado,
    AnaliseRiscoResponse,
    AnaliseRiscoRequest,
    HistoricoAnaliseRisco,
)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan event handler"""
    # Startup
    try:
        init_portal_db()
        logger.info("Portal demandas database initialized")
    except Exception as e:
        logger.error(f"Failed to initialize database: {e}")

    yield

    # Shutdown (if needed)
    logger.info("Portal demandas API shutting down")


# Create FastAPI app with lifespan
app = FastAPI(
    title="Portal Demandas API",
    description="API para gerenciamento de demandas e tickets do AUDITORIA360",
    version="1.0.0",
    lifespan=lifespan,
    tags_metadata=[
        {"name": "tickets", "description": "Operações com tickets"},
        {"name": "comments", "description": "Comentários dos tickets"},
        {"name": "stats", "description": "Estatísticas e relatórios"},
        {"name": "controle-mensal", "description": "Controles mensais das empresas"},
        {"name": "templates", "description": "Templates para controles recorrentes"},
        {"name": "folha-pagamento", "description": "Auditoria inteligente da folha de pagamento com IA"},
        {"name": "riscos", "description": "Consultor de Riscos - Análise preditiva e identificação de riscos"},
    ],
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure properly for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Health check
@app.get("/health", tags=["health"])
def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "portal_demandas",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": "1.0.0",
    }


# Ticket CRUD operations
@app.post("/tickets/", response_model=Ticket, tags=["tickets"])
def criar_ticket(ticket: TicketCreate, db: Session = Depends(get_db)):
    """
    Criar um novo ticket
    """
    try:
        # Create database ticket
        db_ticket = TicketDB(
            titulo=ticket.titulo,
            descricao=ticket.descricao,
            etapa=ticket.etapa,
            prazo=ticket.prazo,
            responsavel=ticket.responsavel,
            prioridade=ticket.prioridade.value,
            categoria=ticket.categoria.value,
            tags=ticket.tags,
            tempo_estimado=ticket.tempo_estimado,
            criado_em=datetime.now(timezone.utc),
            atualizado_em=datetime.now(timezone.utc),
        )

        db.add(db_ticket)
        db.commit()
        db.refresh(db_ticket)

        # Log creation
        logger.info(f"Ticket created: ID={db_ticket.id}, Titulo='{db_ticket.titulo}'")

        return Ticket.model_validate(db_ticket)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create ticket: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao criar ticket: {str(e)}")


@app.get("/tickets/{ticket_id}", response_model=Ticket, tags=["tickets"])
def obter_ticket(ticket_id: int, db: Session = Depends(get_db)):
    """
    Obter um ticket específico por ID
    """
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket não encontrado")

    return Ticket.model_validate(db_ticket)


@app.get("/tickets/", response_model=TicketListResponse, tags=["tickets"])
def listar_tickets(
    page: int = Query(1, ge=1, description="Número da página"),
    per_page: int = Query(
        10, ge=1, le=50, description="Itens por página (máximo 50 para performance)"
    ),
    status: Optional[List[TicketStatus]] = Query(
        None, description="Filtrar por status"
    ),
    prioridade: Optional[List[TicketPrioridade]] = Query(
        None, description="Filtrar por prioridade"
    ),
    categoria: Optional[List[TicketCategoria]] = Query(
        None, description="Filtrar por categoria"
    ),
    responsavel: Optional[str] = Query(None, description="Filtrar por responsável"),
    etapa: Optional[str] = Query(None, description="Filtrar por etapa"),
    search: Optional[str] = Query(None, description="Buscar no título e descrição"),
    sort_by: str = Query("criado_em", description="Campo para ordenação"),
    sort_order: str = Query("desc", description="Ordem: asc ou desc"),
    db: Session = Depends(get_db),
):
    """
    Listar tickets com filtros e paginação - PERFORMANCE OPTIMIZED
    Reduced maximum per_page from 100 to 50 for better performance
    """
    import time

    start_time = time.time()

    try:
        # Build query
        query = db.query(TicketDB)

        # Apply filters
        if status:
            query = query.filter(TicketDB.status.in_([s.value for s in status]))

        if prioridade:
            query = query.filter(TicketDB.prioridade.in_([p.value for p in prioridade]))

        if categoria:
            query = query.filter(TicketDB.categoria.in_([c.value for c in categoria]))

        if responsavel:
            query = query.filter(TicketDB.responsavel.ilike(f"%{responsavel}%"))

        if etapa:
            query = query.filter(TicketDB.etapa.ilike(f"%{etapa}%"))

        if search:
            search_filter = or_(
                TicketDB.titulo.ilike(f"%{search}%"),
                TicketDB.descricao.ilike(f"%{search}%"),
            )
            query = query.filter(search_filter)

        # Apply sorting
        if hasattr(TicketDB, sort_by):
            sort_column = getattr(TicketDB, sort_by)
            if sort_order.lower() == "desc":
                query = query.order_by(desc(sort_column))
            else:
                query = query.order_by(asc(sort_column))

        # Get total count
        total = query.count()

        # Apply pagination
        offset = (page - 1) * per_page
        tickets = query.offset(offset).limit(per_page).all()

        # Convert to Pydantic models
        ticket_list = [Ticket.model_validate(ticket) for ticket in tickets]

        # Calculate processing time
        processing_time = time.time() - start_time

        logger.info(
            f"Ticket listing completed in {processing_time:.3f}s (page {page}, {len(ticket_list)} items)"
        )

        response = TicketListResponse.create(
            tickets=ticket_list, total=total, page=page, per_page=per_page
        )

        # Add performance metadata if available
        if hasattr(response, "__dict__"):
            response.__dict__["_performance"] = {
                "processing_time_seconds": processing_time,
                "optimization_level": "enhanced",
            }

        return response

    except Exception as e:
        logger.error(f"Failed to list tickets: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao listar tickets: {str(e)}")


@app.patch("/tickets/{ticket_id}", response_model=Ticket, tags=["tickets"])
def atualizar_ticket(
    ticket_id: int, ticket_update: TicketUpdate, db: Session = Depends(get_db)
):
    """
    Atualizar um ticket existente
    """
    # Find ticket
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket não encontrado")

    try:
        # Update fields
        update_data = ticket_update.dict(exclude_unset=True)

        for field, value in update_data.items():
            if hasattr(db_ticket, field) and value is not None:
                # Handle enum fields
                if field in ["status", "prioridade", "categoria"] and hasattr(
                    value, "value"
                ):
                    setattr(db_ticket, field, value.value)
                else:
                    setattr(db_ticket, field, value)

        # Update timestamp
        db_ticket.atualizado_em = datetime.now(timezone.utc)

        db.commit()
        db.refresh(db_ticket)

        logger.info(f"Ticket updated: ID={ticket_id}")

        return Ticket.model_validate(db_ticket)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to update ticket {ticket_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao atualizar ticket: {str(e)}"
        )


@app.delete("/tickets/{ticket_id}", tags=["tickets"])
def deletar_ticket(ticket_id: int, db: Session = Depends(get_db)):
    """
    Deletar um ticket
    """
    db_ticket = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not db_ticket:
        raise HTTPException(status_code=404, detail="Ticket não encontrado")

    try:
        # Delete related comments first
        db.query(TicketCommentDB).filter(
            TicketCommentDB.ticket_id == ticket_id
        ).delete()

        # Delete ticket
        db.delete(db_ticket)
        db.commit()

        logger.info(f"Ticket deleted: ID={ticket_id}")

        return {"message": "Ticket deletado com sucesso"}

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to delete ticket {ticket_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao deletar ticket: {str(e)}")


# Comments endpoints
@app.post(
    "/tickets/{ticket_id}/comments/", response_model=TicketComment, tags=["comments"]
)
def adicionar_comentario(
    ticket_id: int, comment: TicketComment, db: Session = Depends(get_db)
):
    """
    Adicionar comentário a um ticket
    """
    # Verify ticket exists
    ticket_exists = db.query(TicketDB).filter(TicketDB.id == ticket_id).first()
    if not ticket_exists:
        raise HTTPException(status_code=404, detail="Ticket não encontrado")

    try:
        db_comment = TicketCommentDB(
            ticket_id=ticket_id,
            autor=comment.autor,
            comentario=comment.comentario,
            tipo=comment.tipo,
            criado_em=datetime.now(timezone.utc),
        )

        db.add(db_comment)
        db.commit()
        db.refresh(db_comment)

        return TicketComment.model_validate(db_comment)

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to add comment to ticket {ticket_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao adicionar comentário: {str(e)}"
        )


@app.get(
    "/tickets/{ticket_id}/comments/",
    response_model=List[TicketComment],
    tags=["comments"],
)
def listar_comentarios(ticket_id: int, db: Session = Depends(get_db)):
    """
    Listar comentários de um ticket
    """
    comments = (
        db.query(TicketCommentDB)
        .filter(TicketCommentDB.ticket_id == ticket_id)
        .order_by(TicketCommentDB.criado_em.desc())
        .all()
    )

    return [TicketComment.model_validate(comment) for comment in comments]


# Statistics endpoints with performance optimization
@app.get("/stats/", response_model=TicketStats, tags=["stats"])
def obter_estatisticas(db: Session = Depends(get_db)):
    """
    Obter estatísticas dos tickets - PERFORMANCE OPTIMIZED
    Target: <0.5s response time (was 1.5s)
    """
    import time

    start_time = time.time()

    try:
        # Use single query with aggregations for better performance
        from sqlalchemy import case, func

        # Single query to get all statistics at once
        stats_query = db.query(
            func.count(TicketDB.id).label("total"),
            func.sum(case((TicketDB.status == "pendente", 1), else_=0)).label(
                "pendentes"
            ),
            func.sum(case((TicketDB.status == "em_andamento", 1), else_=0)).label(
                "em_andamento"
            ),
            func.sum(case((TicketDB.status == "concluido", 1), else_=0)).label(
                "concluidos"
            ),
            func.sum(case((TicketDB.status == "cancelado", 1), else_=0)).label(
                "cancelados"
            ),
            # Priority counts
            func.sum(case((TicketDB.prioridade == "baixa", 1), else_=0)).label(
                "prioridade_baixa"
            ),
            func.sum(case((TicketDB.prioridade == "media", 1), else_=0)).label(
                "prioridade_media"
            ),
            func.sum(case((TicketDB.prioridade == "alta", 1), else_=0)).label(
                "prioridade_alta"
            ),
            func.sum(case((TicketDB.prioridade == "critica", 1), else_=0)).label(
                "prioridade_critica"
            ),
            # Category counts
            func.sum(case((TicketDB.categoria == "geral", 1), else_=0)).label(
                "categoria_geral"
            ),
            func.sum(case((TicketDB.categoria == "auditoria", 1), else_=0)).label(
                "categoria_auditoria"
            ),
            func.sum(case((TicketDB.categoria == "folha", 1), else_=0)).label(
                "categoria_folha"
            ),
            func.sum(case((TicketDB.categoria == "documentos", 1), else_=0)).label(
                "categoria_documentos"
            ),
            func.sum(case((TicketDB.categoria == "cct", 1), else_=0)).label(
                "categoria_cct"
            ),
            func.sum(case((TicketDB.categoria == "sistema", 1), else_=0)).label(
                "categoria_sistema"
            ),
            # Average completion time for completed tickets
            func.avg(
                case((TicketDB.status == "concluido", TicketDB.tempo_gasto), else_=None)
            ).label("tempo_medio"),
        ).first()

        # Build response from single query result
        por_prioridade = {
            "baixa": int(stats_query.prioridade_baixa or 0),
            "media": int(stats_query.prioridade_media or 0),
            "alta": int(stats_query.prioridade_alta or 0),
            "critica": int(stats_query.prioridade_critica or 0),
        }

        por_categoria = {
            "geral": int(stats_query.categoria_geral or 0),
            "auditoria": int(stats_query.categoria_auditoria or 0),
            "folha": int(stats_query.categoria_folha or 0),
            "documentos": int(stats_query.categoria_documentos or 0),
            "cct": int(stats_query.categoria_cct or 0),
            "sistema": int(stats_query.categoria_sistema or 0),
        }

        # Calculate processing time
        processing_time = time.time() - start_time

        result = TicketStats(
            total=int(stats_query.total or 0),
            pendentes=int(stats_query.pendentes or 0),
            em_andamento=int(stats_query.em_andamento or 0),
            concluidos=int(stats_query.concluidos or 0),
            cancelados=int(stats_query.cancelados or 0),
            por_prioridade=por_prioridade,
            por_categoria=por_categoria,
            tempo_medio_conclusao=(
                float(stats_query.tempo_medio) if stats_query.tempo_medio else None
            ),
        )

        logger.info(f"Portal stats generated in {processing_time:.3f}s")

        if processing_time > 0.5:
            logger.warning(f"Portal stats took {processing_time:.3f}s (target: <0.5s)")

        return result

    except Exception as e:
        logger.error(f"Failed to get statistics: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao obter estatísticas: {str(e)}"
        )


# Bulk operations
@app.patch("/tickets/bulk/status", tags=["tickets"])
def atualizar_status_bulk(
    ticket_ids: List[int], new_status: TicketStatus, db: Session = Depends(get_db)
):
    """
    Atualizar status de múltiplos tickets
    """
    try:
        updated_count = (
            db.query(TicketDB)
            .filter(TicketDB.id.in_(ticket_ids))
            .update(
                {
                    "status": new_status.value,
                    "atualizado_em": datetime.now(timezone.utc),
                },
                synchronize_session=False,
            )
        )

        db.commit()

        logger.info(
            f"Bulk status update: {updated_count} tickets updated to {new_status.value}"
        )

        return {
            "message": f"{updated_count} tickets atualizados para status '{new_status.value}'",
            "updated_count": updated_count,
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Failed bulk status update: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro na atualização em lote: {str(e)}"
        )


# ===== CONTROLE MENSAL ENDPOINTS =====

@app.get("/v1/controles/{ano}/{mes}", response_model=ControleMensalResponse, tags=["controle-mensal"])
def obter_controles_do_mes(
    ano: int, mes: int, db: Session = Depends(get_db)
):
    """
    Obter todos os controles mensais do ano/mês especificado
    Endpoint principal que substitui as chamadas diretas ao Supabase
    """
    try:
        # Get user's accounting firm ID (this would come from auth in production)
        # For now, we'll assume contabilidade_id = 1 for testing
        contabilidade_id = 1  # TODO: Get from authentication context
        
        # Query controls with companies and tasks
        controles_query = (
            db.query(ControleMensalDB, EmpresaDB.nome.label('nome_empresa'))
            .join(EmpresaDB, ControleMensalDB.empresa_id == EmpresaDB.id)
            .filter(EmpresaDB.contabilidade_id == contabilidade_id)
            .filter(ControleMensalDB.ano == ano)
            .filter(ControleMensalDB.mes == mes)
        )
        
        controles_raw = controles_query.all()
        
        # Build detailed control objects
        controles = []
        for controle_db, nome_empresa in controles_raw:
            # Get tasks for this control
            tarefas_raw = (
                db.query(TarefaControleDB)
                .filter(TarefaControleDB.controle_mensal_id == controle_db.id)
                .all()
            )
            
            tarefas = [
                Tarefa(
                    id=tarefa.id,
                    nome_tarefa=tarefa.descricao_tarefa,
                    concluido=tarefa.concluida,
                    data_conclusao=tarefa.data_conclusao
                ) for tarefa in tarefas_raw
            ]
            
            controles.append(
                ControleMensalDetalhado(
                    id_controle=controle_db.id,
                    mes=controle_db.mes,
                    ano=controle_db.ano,
                    status_dados=controle_db.status,
                    id_empresa=controle_db.empresa_id,
                    nome_empresa=nome_empresa,
                    tarefas=tarefas
                )
            )
        
        # Calculate summary statistics
        total_empresas = (
            db.query(EmpresaDB)
            .filter(EmpresaDB.contabilidade_id == contabilidade_id)
            .count()
        )
        
        controles_iniciados = len(controles)
        controles_concluidos = sum(1 for c in controles if c.status_dados == "CONCLUÍDO")
        percentual_conclusao = (
            f"{(controles_concluidos / controles_iniciados * 100):.2f}%" 
            if controles_iniciados > 0 else "0.00%"
        )
        
        sumario = ControleMensalSumario(
            total_empresas=total_empresas,
            controles_iniciados=controles_iniciados,
            controles_concluidos=controles_concluidos,
            percentual_conclusao=percentual_conclusao
        )
        
        response = ControleMensalResponse(
            sumario=sumario,
            controles=controles
        )
        
        logger.info(f"Retrieved {len(controles)} controles for {ano}/{mes} - {percentual_conclusao} completed")
        return response

    except Exception as e:
        logger.error(f"Failed to get controles for {ano}/{mes}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao buscar controles: {str(e)}"
        )


@app.patch("/v1/controles-mensais/tarefas/{tarefa_id}/status", response_model=Tarefa, tags=["controle-mensal"])
def atualizar_status_tarefa(
    tarefa_id: int, concluido: bool, db: Session = Depends(get_db)
):
    """
    Atualizar o status de conclusão de uma tarefa
    """
    tarefa = db.query(TarefaControleDB).filter(TarefaControleDB.id == tarefa_id).first()
    
    if not tarefa:
        raise HTTPException(status_code=404, detail="Tarefa não encontrada")
    
    try:
        tarefa.concluida = concluido
        tarefa.data_conclusao = datetime.now(timezone.utc) if concluido else None
        
        db.commit()
        db.refresh(tarefa)
        
        logger.info(f"Task {tarefa_id} updated: concluido={concluido}")
        
        return Tarefa(
            id=tarefa.id,
            nome_tarefa=tarefa.descricao_tarefa, 
            concluido=tarefa.concluida,
            data_conclusao=tarefa.data_conclusao
        )

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to update task {tarefa_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao atualizar tarefa: {str(e)}"
        )


# ===== TEMPLATE ENDPOINTS =====

@app.get("/v1/templates", response_model=List[TemplateControle], tags=["templates"])
def listar_templates(db: Session = Depends(get_db)):
    """
    Listar todos os templates da contabilidade do usuário
    """
    try:
        # TODO: Get contabilidade_id from auth context
        contabilidade_id = 1
        
        templates_raw = (
            db.query(TemplateControleDB)
            .filter(TemplateControleDB.contabilidade_id == contabilidade_id)
            .all()
        )
        
        templates = []
        for template_db in templates_raw:
            # Get tasks for this template
            tarefas_raw = (
                db.query(TemplateControleTarefaDB.descricao_tarefa)
                .filter(TemplateControleTarefaDB.template_id == template_db.id)
                .all()
            )
            
            tarefas = [t[0] for t in tarefas_raw]
            
            templates.append(
                TemplateControle(
                    id=template_db.id,
                    contabilidade_id=template_db.contabilidade_id,
                    nome_template=template_db.nome_template,
                    descricao=template_db.descricao,
                    criado_em=template_db.criado_em,
                    tarefas=tarefas
                )
            )
        
        return templates

    except Exception as e:
        logger.error(f"Failed to list templates: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao listar templates: {str(e)}"
        )


@app.post("/v1/templates", response_model=TemplateControle, tags=["templates"])
def criar_template(template: TemplateControleCreate, db: Session = Depends(get_db)):
    """
    Criar um novo template de controle
    """
    try:
        # TODO: Get contabilidade_id from auth context
        contabilidade_id = 1
        
        # Create template
        template_db = TemplateControleDB(
            contabilidade_id=contabilidade_id,
            nome_template=template.nome_template,
            descricao=template.descricao,
            criado_em=datetime.now(timezone.utc)
        )
        
        db.add(template_db)
        db.flush()  # Get the ID without committing
        
        # Create template tasks
        for descricao in template.tarefas:
            tarefa_db = TemplateControleTarefaDB(
                template_id=template_db.id,
                descricao_tarefa=descricao
            )
            db.add(tarefa_db)
        
        db.commit()
        db.refresh(template_db)
        
        logger.info(f"Created template: {template_db.id} - {template_db.nome_template}")
        
        return TemplateControle(
            id=template_db.id,
            contabilidade_id=template_db.contabilidade_id,
            nome_template=template_db.nome_template,
            descricao=template_db.descricao,
            criado_em=template_db.criado_em,
            tarefas=template.tarefas
        )

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create template: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao criar template: {str(e)}"
        )


@app.post("/v1/controles/aplicar-template", tags=["templates"])
def aplicar_template(aplicacao: TemplateAplicacao, db: Session = Depends(get_db)):
    """
    Aplicar um template a empresas para criar controles mensais
    """
    try:
        # TODO: Get contabilidade_id from auth context  
        contabilidade_id = 1
        
        # Verify template exists and belongs to the accounting firm
        template = (
            db.query(TemplateControleDB)
            .filter(TemplateControleDB.id == aplicacao.template_id)
            .filter(TemplateControleDB.contabilidade_id == contabilidade_id)
            .first()
        )
        
        if not template:
            raise HTTPException(status_code=404, detail="Template não encontrado")
        
        # Get template tasks
        template_tarefas = (
            db.query(TemplateControleTarefaDB)
            .filter(TemplateControleTarefaDB.template_id == template.id)
            .all()
        )
        
        # Get companies to apply to
        empresas_query = db.query(EmpresaDB).filter(EmpresaDB.contabilidade_id == contabilidade_id)
        
        if aplicacao.empresas_ids:
            empresas_query = empresas_query.filter(EmpresaDB.id.in_(aplicacao.empresas_ids))
        
        empresas = empresas_query.all()
        
        controles_criados = 0
        tarefas_criadas = 0
        
        for empresa in empresas:
            # Check if control already exists for this month/year
            controle_existente = (
                db.query(ControleMensalDB)
                .filter(ControleMensalDB.empresa_id == empresa.id)
                .filter(ControleMensalDB.mes == aplicacao.mes)
                .filter(ControleMensalDB.ano == aplicacao.ano)
                .first()
            )
            
            if controle_existente:
                continue  # Skip if already exists
            
            # Create monthly control
            controle = ControleMensalDB(
                empresa_id=empresa.id,
                mes=aplicacao.mes,
                ano=aplicacao.ano,
                status="AGUARD. DADOS",
                criado_em=datetime.now(timezone.utc)
            )
            
            db.add(controle)
            db.flush()  # Get ID
            controles_criados += 1
            
            # Create tasks from template
            for template_tarefa in template_tarefas:
                tarefa = TarefaControleDB(
                    controle_mensal_id=controle.id,
                    descricao_tarefa=template_tarefa.descricao_tarefa,
                    concluida=False,
                    criado_em=datetime.now(timezone.utc)
                )
                db.add(tarefa)
                tarefas_criadas += 1
        
        db.commit()
        
        logger.info(f"Applied template {aplicacao.template_id}: {controles_criados} controles, {tarefas_criadas} tarefas created")
        
        return {
            "message": f"Template aplicado com sucesso: {controles_criados} controles criados com {tarefas_criadas} tarefas",
            "controles_criados": controles_criados,
            "tarefas_criadas": tarefas_criadas
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to apply template {aplicacao.template_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro ao aplicar template: {str(e)}"
        )


# ===== PAYROLL AUDIT ENDPOINTS =====

@app.post("/v1/folha/auditar", response_model=ProcessamentoFolhaResponse, tags=["folha-pagamento"])
async def auditar_folha_pagamento(
    empresa_id: int,
    mes: int,
    ano: int,
    arquivo_pdf: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Motor de Auditoria Inteligente da Folha de Pagamento
    
    Processa PDFs da folha de pagamento usando IA para extrair dados,
    audita contra CCTs e gera relatório de conformidade e divergências.
    """
    try:
        # Validate request parameters
        if mes < 1 or mes > 12:
            raise HTTPException(status_code=400, detail="Mês deve estar entre 1 e 12")
        if ano < 2020 or ano > 2030:
            raise HTTPException(status_code=400, detail="Ano deve estar entre 2020 e 2030")
        
        # Validate file
        if not arquivo_pdf.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Apenas arquivos PDF são aceitos")
        
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa não encontrada")
        
        # Check if processing already exists for this month/year
        processamento_existente = (
            db.query(ProcessamentosFolhaDB)
            .filter(ProcessamentosFolhaDB.empresa_id == empresa_id)
            .filter(ProcessamentosFolhaDB.mes == mes)
            .filter(ProcessamentosFolhaDB.ano == ano)
            .first()
        )
        
        if processamento_existente:
            raise HTTPException(
                status_code=409, 
                detail=f"Já existe processamento para {empresa.nome} em {mes:02d}/{ano}"
            )
        
        # Read PDF content
        pdf_content = await arquivo_pdf.read()
        
        # Create processing record
        processamento = ProcessamentosFolhaDB(
            empresa_id=empresa_id,
            mes=mes,
            ano=ano,
            arquivo_pdf=arquivo_pdf.filename,
            status_processamento="PROCESSANDO",
            criado_em=datetime.now(timezone.utc)
        )
        
        db.add(processamento)
        db.flush()  # Get the ID
        
        # TODO: Implement AI processing (for now, simulate with mock data)
        dados_extraidos, divergencias = await processar_pdf_com_ia(
            pdf_content, empresa, mes, ano, db
        )
        
        # Update processing record with results
        processamento.dados_extraidos = json.dumps(dados_extraidos, ensure_ascii=False)
        processamento.relatorio_divergencias = json.dumps(divergencias, ensure_ascii=False)
        processamento.total_funcionarios = len(dados_extraidos.get("funcionarios", []))
        processamento.total_divergencias = len(divergencias)
        processamento.status_processamento = "CONCLUIDO"
        processamento.concluido_em = datetime.now(timezone.utc)
        
        db.commit()
        db.refresh(processamento)
        
        logger.info(
            f"Payroll audit completed: empresa_id={empresa_id}, "
            f"funcionarios={processamento.total_funcionarios}, "
            f"divergencias={processamento.total_divergencias}"
        )
        
        # Convert divergencias to Pydantic models
        divergencias_models = [
            FuncionarioDivergencia(
                nome_funcionario=div["nome_funcionario"],
                tipo_divergencia=div["tipo_divergencia"],
                descricao_divergencia=div["descricao_divergencia"],
                valor_encontrado=div.get("valor_encontrado"),
                valor_esperado=div.get("valor_esperado"),
                campo_afetado=div["campo_afetado"]
            ) for div in divergencias
        ]
        
        return ProcessamentoFolhaResponse(
            id=processamento.id,
            empresa_id=processamento.empresa_id,
            mes=processamento.mes,
            ano=processamento.ano,
            arquivo_pdf=processamento.arquivo_pdf,
            total_funcionarios=processamento.total_funcionarios,
            total_divergencias=processamento.total_divergencias,
            status_processamento=processamento.status_processamento,
            criado_em=processamento.criado_em,
            concluido_em=processamento.concluido_em,
            divergencias=divergencias_models
        )

    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to process payroll audit: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Erro no processamento da auditoria: {str(e)}"
        )


@app.get("/v1/folha/processamentos/{empresa_id}", response_model=List[ProcessamentoFolhaResponse], tags=["folha-pagamento"])
def listar_processamentos_folha(
    empresa_id: int, 
    db: Session = Depends(get_db)
):
    """
    Listar histórico de processamentos de folha de pagamento de uma empresa
    """
    try:
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa não encontrada")
        
        processamentos = (
            db.query(ProcessamentosFolhaDB)
            .filter(ProcessamentosFolhaDB.empresa_id == empresa_id)
            .order_by(desc(ProcessamentosFolhaDB.criado_em))
            .all()
        )
        
        result = []
        for proc in processamentos:
            # Parse divergencias from JSON
            divergencias_json = json.loads(proc.relatorio_divergencias or "[]")
            divergencias_models = [
                FuncionarioDivergencia(
                    nome_funcionario=div["nome_funcionario"],
                    tipo_divergencia=div["tipo_divergencia"],
                    descricao_divergencia=div["descricao_divergencia"],
                    valor_encontrado=div.get("valor_encontrado"),
                    valor_esperado=div.get("valor_esperado"),
                    campo_afetado=div["campo_afetado"]
                ) for div in divergencias_json
            ]
            
            result.append(ProcessamentoFolhaResponse(
                id=proc.id,
                empresa_id=proc.empresa_id,
                mes=proc.mes,
                ano=proc.ano,
                arquivo_pdf=proc.arquivo_pdf,
                total_funcionarios=proc.total_funcionarios,
                total_divergencias=proc.total_divergencias,
                status_processamento=proc.status_processamento,
                criado_em=proc.criado_em,
                concluido_em=proc.concluido_em,
                divergencias=divergencias_models
            ))
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to list payroll processings: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao listar processamentos: {str(e)}"
        )


async def processar_pdf_com_ia(pdf_content: bytes, empresa: EmpresaDB, mes: int, ano: int, db: Session):
    """
    AI-powered PDF processing and auditing logic
    
    This is where the real magic happens:
    1. Extract data from PDF using OCR/Document AI
    2. Structure the data (employees, earnings, deductions)
    3. Load applicable CCT rules for the company
    4. Audit each employee's data against the rules
    5. Generate divergence reports
    
    For now, this returns mock data to demonstrate the functionality.
    """
    import asyncio
    
    # Simulate AI processing time
    await asyncio.sleep(0.5)
    
    # Mock extracted data (in production this would come from AI/OCR)
    dados_extraidos = {
        "arquivo_processado": f"folha_{mes:02d}_{ano}.pdf",
        "data_processamento": datetime.now(timezone.utc).isoformat(),
        "funcionarios": [
            {
                "nome": "João Silva",
                "cargo": "Vendedor",
                "salario_base": 1800.00,
                "horas_extras": {"50%": 120.00, "100%": 0.00},
                "descontos": {"INSS": 198.00, "IRRF": 0.00},
                "liquido": 1722.00
            },
            {
                "nome": "Maria Oliveira", 
                "cargo": "Gerente",
                "salario_base": 3500.00,
                "horas_extras": {"50%": 250.00, "100%": 100.00},
                "descontos": {"INSS": 445.50, "IRRF": 180.25},
                "liquido": 3224.25
            },
            {
                "nome": "Carlos Santos",
                "cargo": "Vendedor",
                "salario_base": 1850.00,
                "horas_extras": {"50%": 0.00, "100%": 0.00},
                "descontos": {"INSS": 203.50, "IRRF": 0.00},
                "liquido": 1646.50
            }
        ]
    }
    
    # Mock CCT rules (in production this would come from database)
    regras_cct = {
        "piso_salarial": 1850.00,
        "percentual_he_50": 60.0,  # 60% instead of 50%
        "percentual_he_100": 100.0,
        "auxilio_creche": 150.00,
        "vale_transporte": 6.0  # 6% do salário base
    }
    
    # Mock audit logic - compare extracted data against CCT rules
    divergencias = []
    
    for funcionario in dados_extraidos["funcionarios"]:
        nome = funcionario["nome"]
        
        # Check minimum wage
        if funcionario["salario_base"] < regras_cct["piso_salarial"]:
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "ALERTA",
                "descricao_divergencia": f"Salário base (R$ {funcionario['salario_base']:,.2f}) está abaixo do piso da CCT",
                "valor_encontrado": f"R$ {funcionario['salario_base']:,.2f}",
                "valor_esperado": f"R$ {regras_cct['piso_salarial']:,.2f}",
                "campo_afetado": "salario_base"
            })
        
        # Check overtime calculation (mock - just for João Silva)
        if nome == "João Silva" and funcionario["horas_extras"]["50%"] > 0:
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "AVISO",
                "descricao_divergencia": "Valor da hora extra (50%) pode não corresponder ao valor da CCT (60%)",
                "valor_encontrado": f"R$ {funcionario['horas_extras']['50%']:,.2f}",
                "valor_esperado": "Verificar cálculo com 60%",
                "campo_afetado": "horas_extras_50"
            })
        
        # Check missing benefits (mock - for all employees)
        if nome != "Carlos Santos":  # Simulate Carlos has it, others don't
            divergencias.append({
                "nome_funcionario": nome,
                "tipo_divergencia": "INFO",
                "descricao_divergencia": f"Benefício 'Auxílio Creche' previsto na CCT não foi encontrado na folha",
                "valor_encontrado": None,
                "valor_esperado": f"R$ {regras_cct['auxilio_creche']:,.2f}",
                "campo_afetado": "auxilio_creche"
            })
    
    return dados_extraidos, divergencias


# ===== RISK ANALYSIS ENDPOINTS =====

@app.post("/v1/riscos/analisar", response_model=AnaliseRiscoResponse, tags=["riscos"])
async def analisar_riscos_empresa(
    request: AnaliseRiscoRequest,
    db: Session = Depends(get_db)
):
    """
    🔮 CONSULTOR DE RISCOS - Análise Preditiva Completa
    
    Motor de análise que atua como um "oráculo estratégico":
    1. Agrega dados históricos da empresa (12-24 meses)
    2. Executa análise de conformidade trabalhista, fiscal e operacional
    3. Aplica algoritmos de detecção de padrões e anomalias
    4. Gera score de risco (0-100) e recomendações específicas
    5. Armazena histórico para análise de evolução temporal
    
    Esta funcionalidade transforma contabilidade reativa em consultoria proativa.
    """
    try:
        # Verify company exists and get details
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == request.empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa não encontrada")
        
        contabilidade = db.query(ContabilidadeDB).filter(ContabilidadeDB.id == empresa.contabilidade_id).first()
        
        logger.info(f"🔍 Starting risk analysis for company: {empresa.nome} (ID: {request.empresa_id})")
        
        # Execute comprehensive risk analysis
        resultado_analise = await executar_analise_completa_riscos(
            empresa=empresa, 
            contabilidade=contabilidade,
            db=db
        )
        
        # Get previous analysis for comparison
        analise_anterior = (
            db.query(HistoricoAnalisesRiscoDB)
            .filter(HistoricoAnalisesRiscoDB.empresa_id == request.empresa_id)
            .order_by(desc(HistoricoAnalisesRiscoDB.analisado_em))
            .first()
        )
        
        score_anterior = analise_anterior.score_risco if analise_anterior else None
        variacao_score = (resultado_analise["score_risco"] - score_anterior) if score_anterior else None
        
        # Store analysis in history
        nova_analise = HistoricoAnalisesRiscoDB(
            empresa_id=request.empresa_id,
            contabilidade_id=empresa.contabilidade_id,
            score_risco=resultado_analise["score_risco"],
            relatorio_completo=json.dumps(resultado_analise["relatorio_completo"], ensure_ascii=False),
            analisado_em=datetime.now(timezone.utc),
            analisado_por_user_id="sistema"  # TODO: Get from auth context
        )
        
        db.add(nova_analise)
        db.commit()
        db.refresh(nova_analise)
        
        # Build response
        response = AnaliseRiscoResponse(
            empresa_id=request.empresa_id,
            empresa_nome=empresa.nome,
            score_risco=resultado_analise["score_risco"],
            nivel_risco=resultado_analise["nivel_risco"],
            data_analise=datetime.now(timezone.utc),
            progresso_analise=resultado_analise["progresso_analise"],
            riscos_encontrados=resultado_analise["riscos_encontrados"],
            total_riscos=resultado_analise["total_riscos"],
            riscos_criticos=resultado_analise["riscos_criticos"],
            riscos_altos=resultado_analise["riscos_altos"],
            riscos_medios=resultado_analise["riscos_medios"],
            riscos_baixos=resultado_analise["riscos_baixos"],
            score_anterior=score_anterior,
            variacao_score=variacao_score
        )
        
        logger.info(
            f"✅ Risk analysis completed for {empresa.nome}: "
            f"Score={resultado_analise['score_risco']}/100, "
            f"Level={resultado_analise['nivel_risco']}, "
            f"Total Risks={resultado_analise['total_riscos']}"
        )
        
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to analyze risks for empresa_id {request.empresa_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro na análise de riscos: {str(e)}"
        )


@app.get("/v1/riscos/historico/{empresa_id}", response_model=List[HistoricoAnaliseRisco], tags=["riscos"])
def obter_historico_riscos(
    empresa_id: int,
    limit: int = Query(10, ge=1, le=50, description="Número máximo de análises"),
    db: Session = Depends(get_db)
):
    """
    📊 Histórico de Análises de Risco
    
    Recupera o histórico de análises de risco de uma empresa para:
    - Acompanhar evolução do score de risco
    - Identificar tendências
    - Avaliar efetividade das ações implementadas
    """
    try:
        # Verify company exists
        empresa = db.query(EmpresaDB).filter(EmpresaDB.id == empresa_id).first()
        if not empresa:
            raise HTTPException(status_code=404, detail="Empresa não encontrada")
        
        # Get historical analyses
        historico = (
            db.query(HistoricoAnalisesRiscoDB)
            .filter(HistoricoAnalisesRiscoDB.empresa_id == empresa_id)
            .order_by(desc(HistoricoAnalisesRiscoDB.analisado_em))
            .limit(limit)
            .all()
        )
        
        result = []
        for analise in historico:
            try:
                relatorio_resumo = json.loads(analise.relatorio_completo or "{}")
            except (json.JSONDecodeError, TypeError):
                relatorio_resumo = {}
            
            result.append(HistoricoAnaliseRisco(
                id=analise.id,
                empresa_id=analise.empresa_id,
                contabilidade_id=analise.contabilidade_id,
                score_risco=analise.score_risco,
                data_analise=analise.analisado_em,
                relatorio_resumo={
                    "total_riscos": relatorio_resumo.get("total_riscos", 0),
                    "nivel_risco": relatorio_resumo.get("nivel_risco", "DESCONHECIDO"),
                    "categorias": relatorio_resumo.get("categorias_resumo", {})
                }
            ))
        
        logger.info(f"Retrieved {len(result)} historical risk analyses for empresa_id {empresa_id}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get risk history for empresa_id {empresa_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao obter histórico de riscos: {str(e)}"
        )


async def executar_analise_completa_riscos(
    empresa: EmpresaDB, 
    contabilidade: ContabilidadeDB,
    db: Session
) -> dict:
    """
    🧠 CORE DA ANÁLISE DE RISCOS
    
    Esta função implementa o "cérebro" do Consultor de Riscos:
    1. Agregação massiva de dados históricos
    2. Motor de regras de conformidade
    3. Análise de padrões e anomalias
    4. Quantificação e classificação de riscos
    5. Geração de recomendações específicas
    
    Simula processos que na produção usariam:
    - Análise de dados dos últimos 12-24 meses
    - Consulta a bases de CCTs e legislação
    - Algoritmos de ML para detecção de anomalias
    - Sistema especialista para recomendações
    """
    import asyncio
    from datetime import datetime, timedelta, timezone
    
    # Simulate analysis progress
    progresso_analise = {}
    
    # Step 1: Data Aggregation (simulate time-consuming process)
    progresso_analise["coleta_dados"] = "CONCLUÍDO"
    await asyncio.sleep(0.2)
    
    # Get historical data (last 12 months)
    data_limite = datetime.now(timezone.utc) - timedelta(days=365)
    
    controles_historicos = (
        db.query(ControleMensalDB)
        .filter(ControleMensalDB.empresa_id == empresa.id)
        .filter(ControleMensalDB.criado_em >= data_limite)
        .count()
    )
    
    processamentos_folha = (
        db.query(ProcessamentosFolhaDB)
        .filter(ProcessamentosFolhaDB.empresa_id == empresa.id)
        .filter(ProcessamentosFolhaDB.criado_em >= data_limite)
        .count()
    )
    
    progresso_analise["analise_conformidade_trabalhista"] = "CONCLUÍDO"
    await asyncio.sleep(0.2)
    
    progresso_analise["analise_conformidade_fiscal"] = "CONCLUÍDO"
    await asyncio.sleep(0.2)
    
    progresso_analise["deteccao_anomalias"] = "CONCLUÍDO"  
    await asyncio.sleep(0.2)
    
    progresso_analise["calculo_score"] = "CONCLUÍDO"
    await asyncio.sleep(0.1)
    
    # Risk Analysis Engine - Simulation with realistic business logic
    riscos_encontrados = []
    
    # 1. TRABALHISTA RISKS
    if controles_historicos < 6:  # Less than 6 months of controls
        riscos_encontrados.append(RiscoDetalhado(
            categoria="TRABALHISTA",
            tipo_risco="Histórico de Controles Insuficiente",
            descricao="Empresa possui poucos controles mensais registrados no sistema",
            evidencia=f"Apenas {controles_historicos} controles encontrados nos últimos 12 meses",
            impacto_potencial="Risco de não conformidade com obrigações trabalhistas. Possível multa de R$ 2.000 a R$ 20.000 por irregularidade não detectada.",
            plano_acao="1. Implementar controles mensais sistemáticos. 2. Revisar processos de documentação. 3. Treinar equipe em boas práticas.",
            severidade=4  # High severity
        ))
    
    # Simulate payroll compliance check
    if processamentos_folha == 0:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="TRABALHISTA", 
            tipo_risco="Auditoria de Folha Pendente",
            descricao="Nenhuma auditoria de folha foi realizada recentemente",
            evidencia="Sistema não registra processamentos de auditoria da folha nos últimos 12 meses",
            impacto_potencial="Riscos não identificados em cálculos trabalhistas. Estimativa de exposição: R$ 5.000 a R$ 50.000 em possíveis correções.",
            plano_acao="1. Realizar auditoria completa da folha de pagamento. 2. Implementar auditorias mensais automáticas. 3. Validar cálculos contra CCT vigente.",
            severidade=3  # Medium-high severity
        ))
    
    # 2. FISCAL RISKS  
    # Simulate tax compliance analysis with safe datetime handling
    empresa_criada = getattr(empresa, 'criado_em', None)
    if empresa_criada is None:
        # Fallback: assume company created 400 days ago if no creation date
        dias_funcionamento = 400
    else:
        # Make sure both datetimes have timezone info
        now = datetime.now(timezone.utc)
        if empresa_criada.tzinfo is None:
            # If empresa_criada is timezone-naive, assume it's UTC
            empresa_criada = empresa_criada.replace(tzinfo=timezone.utc)
        dias_funcionamento = (now - empresa_criada).days
    
    if dias_funcionamento > 365 and controles_historicos < 12:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="FISCAL",
            tipo_risco="Controle Fiscal Inconsistente", 
            descricao="Empresa com mais de 1 ano de funcionamento apresenta gaps no controle mensal",
            evidencia=f"Empresa ativa há {dias_funcionamento} dias mas apenas {controles_historicos} controles registrados",
            impacto_potencial="Risco de autuação fiscal por falta de documentação. Multas podem variar de 20% a 75% do tributo devido.",
            plano_acao="1. Regularizar controles em atraso. 2. Implementar rotina de compliance fiscal mensal. 3. Revisar regime tributário.",
            severidade=5  # Critical severity
        ))
    
    # 3. OPERATIONAL RISKS
    # Simulate operational efficiency analysis
    if controles_historicos > 0 and controles_historicos < 8:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="OPERACIONAL",
            tipo_risco="Eficiência Operacional Subótima",
            descricao="Padrão irregular nos controles mensais indica possíveis problemas operacionais",
            evidencia=f"Variação significativa na frequência de controles: {controles_historicos} em 12 meses",
            impacto_potencial="Ineficiência operacional pode resultar em aumento de 15-30% nos custos de compliance.",
            plano_acao="1. Padronizar processos operacionais. 2. Implementar automações. 3. Treinar equipe em ferramentas do sistema.",
            severidade=2  # Low-medium severity
        ))
    
    # Add some positive findings for companies with good practices
    if controles_historicos >= 10 and processamentos_folha > 0:
        riscos_encontrados.append(RiscoDetalhado(
            categoria="CONFORMIDADE",
            tipo_risco="Práticas de Compliance Adequadas",
            descricao="Empresa demonstra boas práticas de controle e auditoria",
            evidencia=f"Registros regulares: {controles_historicos} controles e {processamentos_folha} auditorias de folha",
            impacto_potencial="Redução significativa do risco de não conformidade. Potencial economia de 40-60% em multas evitadas.",
            plano_acao="1. Manter rotina estabelecida. 2. Considerar automações adicionais. 3. Usar como benchmark para outras empresas.",
            severidade=1  # Low risk (positive finding)
        ))
    
    # Calculate risk score based on found risks
    total_riscos = len(riscos_encontrados)
    riscos_criticos = sum(1 for r in riscos_encontrados if r.severidade == 5)
    riscos_altos = sum(1 for r in riscos_encontrados if r.severidade == 4) 
    riscos_medios = sum(1 for r in riscos_encontrados if r.severidade == 3)
    riscos_baixos = sum(1 for r in riscos_encontrados if r.severidade <= 2)
    
    # Scoring algorithm: Base score starts at 100 (perfect), subtract points for risks
    score_risco = 100
    score_risco -= (riscos_criticos * 25)  # Critical risks: -25 points each
    score_risco -= (riscos_altos * 15)     # High risks: -15 points each  
    score_risco -= (riscos_medios * 10)    # Medium risks: -10 points each
    score_risco -= (riscos_baixos * 5)     # Low risks: -5 points each (but positive findings add back)
    
    # Add bonus points for positive compliance findings
    bonus_compliance = sum(5 for r in riscos_encontrados if r.categoria == "CONFORMIDADE")
    score_risco = min(100, max(0, score_risco + bonus_compliance))  # Keep between 0-100
    
    # Determine risk level based on score
    if score_risco >= 80:
        nivel_risco = "BAIXO"
    elif score_risco >= 60:
        nivel_risco = "MÉDIO" 
    elif score_risco >= 40:
        nivel_risco = "ALTO"
    else:
        nivel_risco = "CRÍTICO"
    
    # Build comprehensive report
    relatorio_completo = {
        "analise_id": f"RISK_{empresa.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "empresa_dados": {
            "id": empresa.id,
            "nome": empresa.nome,
            "contabilidade_id": empresa.contabilidade_id,
            "dias_funcionamento": dias_funcionamento
        },
        "dados_historicos": {
            "controles_mensais_12m": controles_historicos,
            "auditorias_folha_12m": processamentos_folha,
            "periodo_analise": "12 meses"
        },
        "score_risco": score_risco,
        "nivel_risco": nivel_risco,
        "total_riscos": total_riscos,
        "riscos_criticos": riscos_criticos,
        "riscos_altos": riscos_altos,
        "riscos_medios": riscos_medios,
        "riscos_baixos": riscos_baixos,
        "categorias_resumo": {
            "TRABALHISTA": sum(1 for r in riscos_encontrados if r.categoria == "TRABALHISTA"),
            "FISCAL": sum(1 for r in riscos_encontrados if r.categoria == "FISCAL"),
            "OPERACIONAL": sum(1 for r in riscos_encontrados if r.categoria == "OPERACIONAL"),
            "CONFORMIDADE": sum(1 for r in riscos_encontrados if r.categoria == "CONFORMIDADE")
        },
        "algoritmo_versao": "v1.0",
        "processado_em": datetime.now(timezone.utc).isoformat()
    }
    
    return {
        "score_risco": score_risco,
        "nivel_risco": nivel_risco,
        "progresso_analise": progresso_analise,
        "riscos_encontrados": riscos_encontrados,
        "total_riscos": total_riscos,
        "riscos_criticos": riscos_criticos,
        "riscos_altos": riscos_altos,
        "riscos_medios": riscos_medios,
        "riscos_baixos": riscos_baixos,
        "relatorio_completo": relatorio_completo
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001)
